{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Apache MXNet and Gluon\n",
    "\n",
    "This tutorial shows how to train and test a Sentiment Analysis (Text Classification) model on Amazon SageMaker using Apache MXNet and the Gluon API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download training and test data\n",
    "\n",
    "In this notebook, we train a Sentiment Analysis model on the [SST-2 (Stanford Sentiment Treebank 2) dataset](https://nlp.stanford.edu/sentiment/index.html). This dataset consists of movie reviews with one sentence per review. The task is to classify the review as either positive or negative.\n",
    "\n",
    "We download the preprocessed version of this dataset from the links below. Each line in the dataset has space separated tokens, with the first token being the label: 1 for positive and 0 for negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "100 4147k  100 4147k    0     0  6209k      0 --:--:-- --:--:-- --:--:-- 6200k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "100  189k  100  189k    0     0   727k      0 --:--:-- --:--:-- --:--:--  727k\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "mkdir data\n",
    "\n",
    "curl https://raw.githubusercontent.com/saurabh3949/Text-Classification-Datasets/master/stsa.binary.phrases.train > data/train\n",
    "curl https://raw.githubusercontent.com/saurabh3949/Text-Classification-Datasets/master/stsa.binary.test > data/test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the data\n",
    "\n",
    "We use the `sagemaker.s3.S3Uploader` to upload our datasets to an Amazon S3 location. The return value `inputs` identifies the location -- we use this later when we start the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import s3, session\n",
    "\n",
    "bucket = session.Session().default_bucket()\n",
    "inputs = s3.S3Uploader.upload('data', 's3://{}/mxnet-gluon-sentiment-example/data'.format(bucket))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement the training function\n",
    "\n",
    "We need to provide a training script that can run on the SageMaker platform. The training scripts are essentially the same as one you would write for local training, but you can also access useful properties about the training environment through various environment variables. In addition, hyperparameters are passed to the script as arguments. For more about writing an MXNet training script for use with SageMaker, see [the SageMaker documentation](https://sagemaker.readthedocs.io/en/stable/using_mxnet.html#prepare-an-mxnet-training-script).\n",
    "\n",
    "The script here is a simplified implementation of [\"Bag of Tricks for Efficient Text Classification\"](https://arxiv.org/abs/1607.01759), as implemented by Facebook's [FastText](https://github.com/facebookresearch/fastText/) for text classification. The model maps each word to a vector and averages vectors of all the words in a sentence to form a hidden representation of the sentence, which is inputted to a softmax classification layer. For more details, please refer to [the paper](https://arxiv.org/abs/1607.01759).\n",
    "\n",
    "At the end of every epoch, our script also checks the validation accuracy, and checkpoints the best model so far, along with the optimizer state, in the folder `/opt/ml/checkpoints`. (If the folder `/opt/ml/checkpoints` does not exist, this checkpointing step is skipped.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m print_function\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mbisect\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrandom\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mcollections\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Counter\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mitertools\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m chain, islice\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mmxnet\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mmx\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmxnet\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m gluon, autograd, nd\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmxnet.io\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DataIter, DataBatch, DataDesc\n",
      "\n",
      "logging.basicConfig(level=logging.DEBUG)\n",
      "\n",
      "\u001b[37m# ------------------------------------------------------------ #\u001b[39;49;00m\n",
      "\u001b[37m# Training methods                                             #\u001b[39;49;00m\n",
      "\u001b[37m# ------------------------------------------------------------ #\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtrain\u001b[39;49;00m(current_host, hosts, num_cpus, num_gpus, training_dir, model_dir,\n",
      "          batch_size, epochs, learning_rate, log_interval, embedding_size):\n",
      "    \u001b[34mif\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(hosts) == \u001b[34m1\u001b[39;49;00m:\n",
      "        kvstore = \u001b[33m'\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m num_gpus > \u001b[34m0\u001b[39;49;00m \u001b[34melse\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mlocal\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "    \u001b[34melse\u001b[39;49;00m:\n",
      "        kvstore = \u001b[33m'\u001b[39;49;00m\u001b[33mdist_device_sync\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m num_gpus > \u001b[34m0\u001b[39;49;00m \u001b[34melse\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mdist_sync\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "\n",
      "    ctx = mx.gpu() \u001b[34mif\u001b[39;49;00m num_gpus > \u001b[34m0\u001b[39;49;00m \u001b[34melse\u001b[39;49;00m mx.cpu()\n",
      "\n",
      "    checkpoints_dir = \u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/checkpoints\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "    checkpoints_enabled = os.path.exists(checkpoints_dir)\n",
      "\n",
      "    train_sentences, train_labels, _ = get_dataset(training_dir + \u001b[33m'\u001b[39;49;00m\u001b[33m/train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    val_sentences, val_labels, _ = get_dataset(training_dir + \u001b[33m'\u001b[39;49;00m\u001b[33m/test\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    num_classes = \u001b[36mlen\u001b[39;49;00m(\u001b[36mset\u001b[39;49;00m(train_labels))\n",
      "    vocab = create_vocab(train_sentences)\n",
      "    vocab_size = \u001b[36mlen\u001b[39;49;00m(vocab)\n",
      "\n",
      "    train_sentences = [[vocab.get(token, \u001b[34m1\u001b[39;49;00m) \u001b[34mfor\u001b[39;49;00m token \u001b[35min\u001b[39;49;00m line \u001b[34mif\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(line) > \u001b[34m0\u001b[39;49;00m] \u001b[34mfor\u001b[39;49;00m line \u001b[35min\u001b[39;49;00m train_sentences]\n",
      "    val_sentences = [[vocab.get(token, \u001b[34m1\u001b[39;49;00m) \u001b[34mfor\u001b[39;49;00m token \u001b[35min\u001b[39;49;00m line \u001b[34mif\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(line) > \u001b[34m0\u001b[39;49;00m] \u001b[34mfor\u001b[39;49;00m line \u001b[35min\u001b[39;49;00m val_sentences]\n",
      "\n",
      "    \u001b[37m# Alternatively to splitting in memory, the data could be pre-split in S3 and use ShardedByS3Key\u001b[39;49;00m\n",
      "    \u001b[37m# to do parallel training.\u001b[39;49;00m\n",
      "    shard_size = \u001b[36mlen\u001b[39;49;00m(train_sentences) // \u001b[36mlen\u001b[39;49;00m(hosts)\n",
      "    \u001b[34mfor\u001b[39;49;00m i, host \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(hosts):\n",
      "        \u001b[34mif\u001b[39;49;00m host == current_host:\n",
      "            start = shard_size * i\n",
      "            end = start + shard_size\n",
      "            \u001b[34mbreak\u001b[39;49;00m\n",
      "\n",
      "    train_iterator = BucketSentenceIter(train_sentences[start:end], train_labels[start:end], batch_size)\n",
      "    val_iterator = BucketSentenceIter(val_sentences, val_labels, batch_size)\n",
      "\n",
      "    \u001b[37m# define the network\u001b[39;49;00m\n",
      "    net = TextClassifier(vocab_size, embedding_size, num_classes)\n",
      "\n",
      "    \u001b[37m# Collect all parameters from net and its children, then initialize them.\u001b[39;49;00m\n",
      "    net.initialize(mx.init.Xavier(magnitude=\u001b[34m2.24\u001b[39;49;00m), ctx=ctx)\n",
      "    \u001b[37m# Trainer is for updating parameters with gradient.\u001b[39;49;00m\n",
      "    trainer = gluon.Trainer(net.collect_params(), \u001b[33m'\u001b[39;49;00m\u001b[33madam\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                            {\u001b[33m'\u001b[39;49;00m\u001b[33mlearning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: learning_rate},\n",
      "                            kvstore=kvstore)\n",
      "    metric = mx.metric.Accuracy()\n",
      "    loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
      "    net.hybridize()\n",
      "\n",
      "    best_acc_score = \u001b[34m0.0\u001b[39;49;00m\n",
      "    \u001b[34mfor\u001b[39;49;00m epoch \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(epochs):\n",
      "        \u001b[37m# reset data iterator and metric at begining of epoch.\u001b[39;49;00m\n",
      "        metric.reset()\n",
      "        btic = time.time()\n",
      "        i = \u001b[34m0\u001b[39;49;00m\n",
      "        \u001b[34mfor\u001b[39;49;00m batch \u001b[35min\u001b[39;49;00m train_iterator:\n",
      "            \u001b[37m# Copy data to ctx if necessary\u001b[39;49;00m\n",
      "            data = batch.data[\u001b[34m0\u001b[39;49;00m].as_in_context(ctx)\n",
      "            label = batch.label[\u001b[34m0\u001b[39;49;00m].as_in_context(ctx)\n",
      "\n",
      "            \u001b[37m# Start recording computation graph with record() section.\u001b[39;49;00m\n",
      "            \u001b[37m# Recorded graphs can then be differentiated with backward.\u001b[39;49;00m\n",
      "            \u001b[34mwith\u001b[39;49;00m autograd.record():\n",
      "                output = net(data)\n",
      "                L = loss(output, label)\n",
      "                L.backward()\n",
      "            \u001b[37m# take a gradient step with batch_size equal to data.shape[0]\u001b[39;49;00m\n",
      "            trainer.step(data.shape[\u001b[34m0\u001b[39;49;00m])\n",
      "            \u001b[37m# update metric at last.\u001b[39;49;00m\n",
      "            metric.update([label], [output])\n",
      "\n",
      "            \u001b[34mif\u001b[39;49;00m i % log_interval == \u001b[34m0\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m i > \u001b[34m0\u001b[39;49;00m:\n",
      "                name, acc = metric.get()\n",
      "                \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m[Epoch \u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m Batch \u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m] Training: \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m=\u001b[39;49;00m\u001b[33m%f\u001b[39;49;00m\u001b[33m, \u001b[39;49;00m\u001b[33m%f\u001b[39;49;00m\u001b[33m samples/s\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m %\n",
      "                      (epoch, i, name, acc, batch_size / (time.time() - btic)))\n",
      "\n",
      "            btic = time.time()\n",
      "            i += \u001b[34m1\u001b[39;49;00m\n",
      "\n",
      "        name, acc = metric.get()\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m[Epoch \u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m] Training: \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m=\u001b[39;49;00m\u001b[33m%f\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m % (epoch, name, acc))\n",
      "\n",
      "        name, val_acc = test(ctx, net, val_iterator)\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m[Epoch \u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m] Validation: \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m=\u001b[39;49;00m\u001b[33m%f\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m % (epoch, name, val_acc))\n",
      "        \u001b[34mif\u001b[39;49;00m checkpoints_enabled \u001b[35mand\u001b[39;49;00m val_acc > best_acc_score:\n",
      "            best_acc_score = val_acc\n",
      "            logging.info(\u001b[33m'\u001b[39;49;00m\u001b[33mSaving the model, params and optimizer state.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "            net.export(checkpoints_dir + \u001b[33m'\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m%.4f\u001b[39;49;00m\u001b[33m-gluon_sentiment\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m % (best_acc_score), epoch)\n",
      "            trainer.save_states(checkpoints_dir + \u001b[33m'\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m%.4f\u001b[39;49;00m\u001b[33m-gluon_sentiment-\u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m.states\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m % (best_acc_score, epoch))\n",
      "        train_iterator.reset()\n",
      "    \u001b[34mreturn\u001b[39;49;00m net, vocab\n",
      "\n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mBucketSentenceIter\u001b[39;49;00m(DataIter):\n",
      "    \u001b[33m\"\"\"Simple bucketing iterator for text classification model.\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m    Args:\u001b[39;49;00m\n",
      "\u001b[33m        sentences (list[list[int]]): Encoded sentences.\u001b[39;49;00m\n",
      "\u001b[33m        labels (list[int]): Corresponding labels.\u001b[39;49;00m\n",
      "\u001b[33m        batch_size (int): Batch size of the data.\u001b[39;49;00m\n",
      "\u001b[33m        buckets (list[int]): Optional. Size of the data buckets. Automatically generated if None.\u001b[39;49;00m\n",
      "\u001b[33m        invalid_label (int): Optional. Key for invalid label, e.g. <unk. The default is 0.\u001b[39;49;00m\n",
      "\u001b[33m        dtype (str): Optional. Data type of the encoding. The default data type is 'float32'.\u001b[39;49;00m\n",
      "\u001b[33m        data_name (str): Optional. Name of the data. The default name is 'data'.\u001b[39;49;00m\n",
      "\u001b[33m        label_name (str): Optional. Name of the label. The default name is 'softmax_label'.\u001b[39;49;00m\n",
      "\u001b[33m        layout (str): Optional. Format of data and label. 'NT' means (batch_size, length)\u001b[39;49;00m\n",
      "\u001b[33m            and 'TN' means (length, batch_size).\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, sentences, labels, batch_size, buckets=\u001b[36mNone\u001b[39;49;00m, invalid_label=\u001b[34m0\u001b[39;49;00m,\n",
      "                 data_name=\u001b[33m'\u001b[39;49;00m\u001b[33mdata\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, label_name=\u001b[33m'\u001b[39;49;00m\u001b[33msoftmax_label\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, dtype=\u001b[33m'\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "                 layout=\u001b[33m'\u001b[39;49;00m\u001b[33mNT\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m):\n",
      "        \u001b[36msuper\u001b[39;49;00m(BucketSentenceIter, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m buckets:\n",
      "            buckets = [i \u001b[34mfor\u001b[39;49;00m i, j \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(np.bincount([\u001b[36mlen\u001b[39;49;00m(s) \u001b[34mfor\u001b[39;49;00m s \u001b[35min\u001b[39;49;00m sentences]))\n",
      "                       \u001b[34mif\u001b[39;49;00m j >= batch_size]\n",
      "        buckets.sort()\n",
      "\n",
      "        ndiscard = \u001b[34m0\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.data = [[] \u001b[34mfor\u001b[39;49;00m _ \u001b[35min\u001b[39;49;00m buckets]\n",
      "        \u001b[36mself\u001b[39;49;00m.labels = [[] \u001b[34mfor\u001b[39;49;00m _ \u001b[35min\u001b[39;49;00m buckets]\n",
      "        \u001b[34mfor\u001b[39;49;00m i, sent \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(sentences):\n",
      "            buck = bisect.bisect_left(buckets, \u001b[36mlen\u001b[39;49;00m(sent))\n",
      "            \u001b[34mif\u001b[39;49;00m buck == \u001b[36mlen\u001b[39;49;00m(buckets):\n",
      "                ndiscard += \u001b[34m1\u001b[39;49;00m\n",
      "                \u001b[34mcontinue\u001b[39;49;00m\n",
      "            buff = np.full((buckets[buck],), invalid_label, dtype=dtype)\n",
      "            buff[:\u001b[36mlen\u001b[39;49;00m(sent)] = sent\n",
      "            \u001b[36mself\u001b[39;49;00m.data[buck].append(buff)\n",
      "            \u001b[36mself\u001b[39;49;00m.labels[buck].append(labels[i])\n",
      "\n",
      "        \u001b[36mself\u001b[39;49;00m.data = [np.asarray(i, dtype=dtype) \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.data]\n",
      "        \u001b[36mself\u001b[39;49;00m.labels = [np.asarray(i, dtype=dtype) \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.labels]\n",
      "\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mWARNING: discarded \u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m sentences longer than the largest bucket.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m % ndiscard)\n",
      "\n",
      "        \u001b[36mself\u001b[39;49;00m.batch_size = batch_size\n",
      "        \u001b[36mself\u001b[39;49;00m.buckets = buckets\n",
      "        \u001b[36mself\u001b[39;49;00m.data_name = data_name\n",
      "        \u001b[36mself\u001b[39;49;00m.label_name = label_name\n",
      "        \u001b[36mself\u001b[39;49;00m.dtype = dtype\n",
      "        \u001b[36mself\u001b[39;49;00m.invalid_label = invalid_label\n",
      "        \u001b[36mself\u001b[39;49;00m.nddata = []\n",
      "        \u001b[36mself\u001b[39;49;00m.ndlabel = []\n",
      "        \u001b[36mself\u001b[39;49;00m.major_axis = layout.find(\u001b[33m'\u001b[39;49;00m\u001b[33mN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        \u001b[36mself\u001b[39;49;00m.layout = layout\n",
      "        \u001b[36mself\u001b[39;49;00m.default_bucket_key = \u001b[36mmax\u001b[39;49;00m(buckets)\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.major_axis == \u001b[34m0\u001b[39;49;00m:\n",
      "            \u001b[36mself\u001b[39;49;00m.provide_data = [DataDesc(\n",
      "                name=\u001b[36mself\u001b[39;49;00m.data_name, shape=(batch_size, \u001b[36mself\u001b[39;49;00m.default_bucket_key),\n",
      "                layout=\u001b[36mself\u001b[39;49;00m.layout)]\n",
      "            \u001b[36mself\u001b[39;49;00m.provide_label = [DataDesc(\n",
      "                name=\u001b[36mself\u001b[39;49;00m.label_name, shape=(batch_size,),\n",
      "                layout=\u001b[36mself\u001b[39;49;00m.layout)]\n",
      "        \u001b[34melif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.major_axis == \u001b[34m1\u001b[39;49;00m:\n",
      "            \u001b[36mself\u001b[39;49;00m.provide_data = [DataDesc(\n",
      "                name=\u001b[36mself\u001b[39;49;00m.data_name, shape=(\u001b[36mself\u001b[39;49;00m.default_bucket_key, batch_size),\n",
      "                layout=\u001b[36mself\u001b[39;49;00m.layout)]\n",
      "            \u001b[36mself\u001b[39;49;00m.provide_label = [DataDesc(\n",
      "                name=\u001b[36mself\u001b[39;49;00m.label_name, shape=(\u001b[36mself\u001b[39;49;00m.default_bucket_key, batch_size),\n",
      "                layout=\u001b[36mself\u001b[39;49;00m.layout)]\n",
      "        \u001b[34melse\u001b[39;49;00m:\n",
      "            \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mInvalid layout \u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m: Must by NT (batch major) or TN (time major)\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "        \u001b[36mself\u001b[39;49;00m.idx = []\n",
      "        \u001b[34mfor\u001b[39;49;00m i, buck \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.data):\n",
      "            \u001b[36mself\u001b[39;49;00m.idx.extend([(i, j) \u001b[34mfor\u001b[39;49;00m j \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[34m0\u001b[39;49;00m, \u001b[36mlen\u001b[39;49;00m(buck) - batch_size + \u001b[34m1\u001b[39;49;00m, batch_size)])\n",
      "        \u001b[36mself\u001b[39;49;00m.curr_idx = \u001b[34m0\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.reset()\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mreset\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Resets the iterator to the beginning of the data.\"\"\"\u001b[39;49;00m\n",
      "        \u001b[36mself\u001b[39;49;00m.curr_idx = \u001b[34m0\u001b[39;49;00m\n",
      "        random.shuffle(\u001b[36mself\u001b[39;49;00m.idx)\n",
      "        \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(\u001b[36mlen\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.data)):\n",
      "            data, labels = \u001b[36mself\u001b[39;49;00m.data[i], \u001b[36mself\u001b[39;49;00m.labels[i]\n",
      "            p = np.random.permutation(\u001b[36mlen\u001b[39;49;00m(data))\n",
      "            \u001b[36mself\u001b[39;49;00m.data[i], \u001b[36mself\u001b[39;49;00m.labels[i] = data[p], labels[p]\n",
      "\n",
      "        \u001b[36mself\u001b[39;49;00m.nddata = []\n",
      "        \u001b[36mself\u001b[39;49;00m.ndlabel = []\n",
      "        \u001b[34mfor\u001b[39;49;00m buck, label_buck \u001b[35min\u001b[39;49;00m \u001b[36mzip\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.data, \u001b[36mself\u001b[39;49;00m.labels):\n",
      "            \u001b[36mself\u001b[39;49;00m.nddata.append(nd.array(buck, dtype=\u001b[36mself\u001b[39;49;00m.dtype))\n",
      "            \u001b[36mself\u001b[39;49;00m.ndlabel.append(nd.array(label_buck, dtype=\u001b[36mself\u001b[39;49;00m.dtype))\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mnext\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m):\n",
      "        \u001b[33m\"\"\"Returns the next batch of data.\"\"\"\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.curr_idx == \u001b[36mlen\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m.idx):\n",
      "            \u001b[34mraise\u001b[39;49;00m \u001b[36mStopIteration\u001b[39;49;00m\n",
      "        i, j = \u001b[36mself\u001b[39;49;00m.idx[\u001b[36mself\u001b[39;49;00m.curr_idx]\n",
      "        \u001b[36mself\u001b[39;49;00m.curr_idx += \u001b[34m1\u001b[39;49;00m\n",
      "\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.major_axis == \u001b[34m1\u001b[39;49;00m:\n",
      "            data = \u001b[36mself\u001b[39;49;00m.nddata[i][j:j+\u001b[36mself\u001b[39;49;00m.batch_size].T\n",
      "            label = \u001b[36mself\u001b[39;49;00m.ndlabel[i][j:j+\u001b[36mself\u001b[39;49;00m.batch_size].T\n",
      "        \u001b[34melse\u001b[39;49;00m:\n",
      "            data = \u001b[36mself\u001b[39;49;00m.nddata[i][j:j+\u001b[36mself\u001b[39;49;00m.batch_size]\n",
      "            label = \u001b[36mself\u001b[39;49;00m.ndlabel[i][j:j+\u001b[36mself\u001b[39;49;00m.batch_size]\n",
      "\n",
      "        \u001b[34mreturn\u001b[39;49;00m DataBatch([data], [label], pad=\u001b[34m0\u001b[39;49;00m,\n",
      "                         bucket_key=\u001b[36mself\u001b[39;49;00m.buckets[i],\n",
      "                         provide_data=[DataDesc(\n",
      "                             name=\u001b[36mself\u001b[39;49;00m.data_name, shape=data.shape,\n",
      "                             layout=\u001b[36mself\u001b[39;49;00m.layout)],\n",
      "                         provide_label=[DataDesc(\n",
      "                             name=\u001b[36mself\u001b[39;49;00m.label_name, shape=label.shape,\n",
      "                             layout=\u001b[36mself\u001b[39;49;00m.layout)])\n",
      "\n",
      "\n",
      "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mTextClassifier\u001b[39;49;00m(gluon.HybridBlock):\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, vocab_size, embedding_size, classes, **kwargs):\n",
      "        \u001b[36msuper\u001b[39;49;00m(TextClassifier, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m(**kwargs)\n",
      "        \u001b[34mwith\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.name_scope():\n",
      "            \u001b[36mself\u001b[39;49;00m.dense = gluon.nn.Dense(classes)\n",
      "            \u001b[36mself\u001b[39;49;00m.embedding = gluon.nn.Embedding(input_dim=vocab_size, output_dim=embedding_size)\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mhybrid_forward\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, F, x):\n",
      "        x = \u001b[36mself\u001b[39;49;00m.embedding(x)\n",
      "        x = F.mean(x, axis=\u001b[34m1\u001b[39;49;00m)\n",
      "        x = \u001b[36mself\u001b[39;49;00m.dense(x)\n",
      "        \u001b[34mreturn\u001b[39;49;00m x\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_dataset\u001b[39;49;00m(filename):\n",
      "    labels = []\n",
      "    sentences = []\n",
      "    max_length = -\u001b[34m1\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(filename) \u001b[34mas\u001b[39;49;00m f:\n",
      "        \u001b[34mfor\u001b[39;49;00m line \u001b[35min\u001b[39;49;00m f:\n",
      "            tokens = line.split()\n",
      "            label = \u001b[36mint\u001b[39;49;00m(tokens[\u001b[34m0\u001b[39;49;00m])\n",
      "            words = tokens[\u001b[34m1\u001b[39;49;00m:]\n",
      "            max_length = \u001b[36mmax\u001b[39;49;00m(max_length, \u001b[36mlen\u001b[39;49;00m(words))\n",
      "            labels.append(label)\n",
      "            sentences.append(words)\n",
      "    \u001b[34mreturn\u001b[39;49;00m sentences, labels, max_length\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mcreate_vocab\u001b[39;49;00m(sentences, min_count=\u001b[34m5\u001b[39;49;00m, num_words=\u001b[34m100000\u001b[39;49;00m):\n",
      "    BOS_SYMBOL = \u001b[33m'\u001b[39;49;00m\u001b[33m<s>\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "    EOS_SYMBOL = \u001b[33m'\u001b[39;49;00m\u001b[33m</s>\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "    UNK_SYMBOL = \u001b[33m'\u001b[39;49;00m\u001b[33m<unk>\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "    PAD_SYMBOL = \u001b[33m'\u001b[39;49;00m\u001b[33m<pad>\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "    VOCAB_SYMBOLS = [PAD_SYMBOL, UNK_SYMBOL, BOS_SYMBOL, EOS_SYMBOL]\n",
      "    raw_vocab = Counter(token \u001b[34mfor\u001b[39;49;00m line \u001b[35min\u001b[39;49;00m sentences \u001b[34mfor\u001b[39;49;00m token \u001b[35min\u001b[39;49;00m line)\n",
      "    pruned_vocab = \u001b[36msorted\u001b[39;49;00m(((c, w) \u001b[34mfor\u001b[39;49;00m w, c \u001b[35min\u001b[39;49;00m raw_vocab.items() \u001b[34mif\u001b[39;49;00m c >= min_count), reverse=\u001b[36mTrue\u001b[39;49;00m)\n",
      "    vocab = islice((w \u001b[34mfor\u001b[39;49;00m c, w \u001b[35min\u001b[39;49;00m pruned_vocab), num_words)\n",
      "    word_to_id = {word: idx \u001b[34mfor\u001b[39;49;00m idx, word \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(chain(VOCAB_SYMBOLS, vocab))}\n",
      "    \u001b[34mreturn\u001b[39;49;00m word_to_id\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mvocab_to_json\u001b[39;49;00m(vocab, path):\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(path, \u001b[33m'\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m out:\n",
      "        json.dump(vocab, out, indent=\u001b[34m4\u001b[39;49;00m, ensure_ascii=\u001b[36mTrue\u001b[39;49;00m)\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mVocabulary saved to \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, path)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mvocab_from_json\u001b[39;49;00m(path):\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(path) \u001b[34mas\u001b[39;49;00m inp:\n",
      "        vocab = json.load(inp)\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mVocabulary (\u001b[39;49;00m\u001b[33m%d\u001b[39;49;00m\u001b[33m words) loaded from \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mlen\u001b[39;49;00m(vocab), path)\n",
      "        \u001b[34mreturn\u001b[39;49;00m vocab\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32msave\u001b[39;49;00m(net, model_dir):\n",
      "    net, vocab = net\n",
      "    y = net(mx.sym.var(\u001b[33m'\u001b[39;49;00m\u001b[33mdata\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    y.save(\u001b[33m'\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m/model.json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m % model_dir)\n",
      "    net.collect_params().save(\u001b[33m'\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m/model.params\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m % model_dir)\n",
      "    vocab_to_json(vocab, \u001b[33m'\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m/vocab.json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m % model_dir)\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtest\u001b[39;49;00m(ctx, net, val_data):\n",
      "    val_data.reset()\n",
      "    metric = mx.metric.Accuracy()\n",
      "    \u001b[34mfor\u001b[39;49;00m batch \u001b[35min\u001b[39;49;00m val_data:\n",
      "        data = batch.data[\u001b[34m0\u001b[39;49;00m].as_in_context(ctx)\n",
      "        label = batch.label[\u001b[34m0\u001b[39;49;00m].as_in_context(ctx)\n",
      "        output = net(data)\n",
      "        metric.update([label], [output])\n",
      "    \u001b[34mreturn\u001b[39;49;00m metric.get()\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mparse_args\u001b[39;49;00m():\n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    \u001b[37m# retrieve the hyperparameters we set in notebook (with some defaults)\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--batch-size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m8\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m2\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--learning-rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.01\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--log-interval\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1000\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--embedding-size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m50\u001b[39;49;00m)\n",
      "\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--training_channel\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAINING\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current-host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, default=json.loads(os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]))\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m parser.parse_args()\n",
      "\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "    args = parse_args()\n",
      "    num_cpus = \u001b[36mint\u001b[39;49;00m(os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_NUM_CPUS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    num_gpus = \u001b[36mint\u001b[39;49;00m(os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\n",
      "    model = train(args.current_host, args.hosts, num_cpus, num_gpus, args.training_channel, args.model_dir,\n",
      "                  args.batch_size, args.epochs, args.learning_rate, args.log_interval, args.embedding_size)\n",
      "\n",
      "    \u001b[34mif\u001b[39;49;00m args.current_host == args.hosts[\u001b[34m0\u001b[39;49;00m]:\n",
      "        save(model, args.model_dir)\n",
      "\n",
      "\n",
      "\u001b[37m# ------------------------------------------------------------ #\u001b[39;49;00m\n",
      "\u001b[37m# Hosting methods                                              #\u001b[39;49;00m\n",
      "\u001b[37m# ------------------------------------------------------------ #\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\n",
      "    \u001b[33m\"\"\"Loads the Gluon model. Called once when hosting service starts.\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m    Args:\u001b[39;49;00m\n",
      "\u001b[33m        model_dir (str): The directory where model files are stored.\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m    Returns:\u001b[39;49;00m\n",
      "\u001b[33m        mxnet.gluon.block.Block: a Gluon network.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    symbol = mx.sym.load(\u001b[33m'\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m/model.json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m % model_dir)\n",
      "    vocab = vocab_from_json(\u001b[33m'\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m/vocab.json\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m % model_dir)\n",
      "    outputs = mx.symbol.softmax(data=symbol, name=\u001b[33m'\u001b[39;49;00m\u001b[33msoftmax_label\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    inputs = mx.sym.var(\u001b[33m'\u001b[39;49;00m\u001b[33mdata\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    param_dict = gluon.ParameterDict(\u001b[33m'\u001b[39;49;00m\u001b[33mmodel_\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    net = gluon.SymbolBlock(outputs, inputs, param_dict)\n",
      "    net.load_params(\u001b[33m'\u001b[39;49;00m\u001b[33m%s\u001b[39;49;00m\u001b[33m/model.params\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m % model_dir, ctx=mx.cpu())\n",
      "    \u001b[34mreturn\u001b[39;49;00m net, vocab\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mtransform_fn\u001b[39;49;00m(net, data, input_content_type, output_content_type):\n",
      "    \u001b[33m\"\"\"Transforms a request using the Gluon model. Called once per request.\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m    Args:\u001b[39;49;00m\n",
      "\u001b[33m        net (mxnet.gluon.block.Block): The Gluon model.\u001b[39;49;00m\n",
      "\u001b[33m        data (obj): The request payload.\u001b[39;49;00m\n",
      "\u001b[33m        input_content_type (str): The request content type.\u001b[39;49;00m\n",
      "\u001b[33m        output_content_type (str): The (desired) response content type.\u001b[39;49;00m\n",
      "\u001b[33m\u001b[39;49;00m\n",
      "\u001b[33m    Returns:\u001b[39;49;00m\n",
      "\u001b[33m        tuple[obj, str]: The response payload and content type.\u001b[39;49;00m\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\n",
      "    \u001b[37m# we can use content types to vary input/output handling, but\u001b[39;49;00m\n",
      "    \u001b[37m# here we just assume json for both\u001b[39;49;00m\n",
      "    net, vocab = net\n",
      "    parsed = json.loads(data)\n",
      "    outputs = []\n",
      "    \u001b[34mfor\u001b[39;49;00m row \u001b[35min\u001b[39;49;00m parsed:\n",
      "        tokens = [vocab.get(token, \u001b[34m1\u001b[39;49;00m) \u001b[34mfor\u001b[39;49;00m token \u001b[35min\u001b[39;49;00m row.split()]\n",
      "        nda = mx.nd.array([tokens])\n",
      "        output = net(nda)\n",
      "        prediction = mx.nd.argmax(output, axis=\u001b[34m1\u001b[39;49;00m)\n",
      "        outputs.append(\u001b[36mint\u001b[39;49;00m(prediction.asscalar()))\n",
      "    response_body = json.dumps(outputs)\n",
      "    \u001b[34mreturn\u001b[39;49;00m response_body, output_content_type\n"
     ]
    }
   ],
   "source": [
    "!pygmentize 'sentiment.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a SageMaker training job\n",
    "\n",
    "The `MXNet` class allows us to run our training function on SageMaker infrastructure. We need to configure it with our training script, an IAM role, the number of training instances, and the training instance type. In this case we run our training job on a single `c4.2xlarge` instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker.mxnet import MXNet\n",
    "\n",
    "m2 = MXNet('sentiment.py',\n",
    "          role=get_execution_role(),\n",
    "          train_instance_count=1,\n",
    "          train_instance_type='ml.c4.xlarge',\n",
    "          framework_version='1.6.0',\n",
    "          py_version='py3',\n",
    "          distributions={'parameter_server': {'enabled': True}},\n",
    "          hyperparameters={'batch-size': 8,\n",
    "                           'epochs': 2,\n",
    "                           'learning-rate': 0.01,\n",
    "                           'embedding-size': 50, \n",
    "                           'log-interval': 1000})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've constructed our `MXNet` estimator, we can fit it using the data we uploaded to S3. SageMaker makes sure our data is available in the local filesystem, so our training script can simply read the data from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-25 23:12:42 Starting - Starting the training job...\n",
      "2020-05-25 23:12:44 Starting - Launching requested ML instances......\n",
      "2020-05-25 23:13:44 Starting - Preparing the instances for training...\n",
      "2020-05-25 23:14:29 Downloading - Downloading input data...\n",
      "2020-05-25 23:15:06 Training - Training image download completed. Training in progress..\u001b[34m2020-05-25 23:15:08,173 sagemaker-containers INFO     Imported framework sagemaker_mxnet_container.training\u001b[0m\n",
      "\u001b[34m2020-05-25 23:15:08,176 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-05-25 23:15:08,190 sagemaker_mxnet_container.training INFO     MXNet training environment: {'SM_HOSTS': '[\"algo-1\"]', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'SM_HPS': '{\"batch-size\":8,\"embedding-size\":50,\"epochs\":2,\"learning-rate\":0.01,\"log-interval\":1000}', 'SM_USER_ENTRY_POINT': 'sentiment.py', 'SM_FRAMEWORK_PARAMS': '{\"sagemaker_parameter_server_enabled\":true}', 'SM_RESOURCE_CONFIG': '{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}', 'SM_INPUT_DATA_CONFIG': '{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'SM_CHANNELS': '[\"training\"]', 'SM_CURRENT_HOST': 'algo-1', 'SM_MODULE_NAME': 'sentiment', 'SM_LOG_LEVEL': '20', 'SM_FRAMEWORK_MODULE': 'sagemaker_mxnet_container.training:main', 'SM_INPUT_DIR': '/opt/ml/input', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_NUM_CPUS': '4', 'SM_NUM_GPUS': '0', 'SM_MODEL_DIR': '/opt/ml/model', 'SM_MODULE_DIR': 's3://sagemaker-us-east-2-722530644018/mxnet-training-2020-05-25-23-12-41-684/source/sourcedir.tar.gz', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{\"sagemaker_parameter_server_enabled\":true},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":8,\"embedding-size\":50,\"epochs\":2,\"learning-rate\":0.01,\"log-interval\":1000},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"mxnet-training-2020-05-25-23-12-41-684\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-722530644018/mxnet-training-2020-05-25-23-12-41-684/source/sourcedir.tar.gz\",\"module_name\":\"sentiment\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"sentiment.py\"}', 'SM_USER_ARGS': '[\"--batch-size\",\"8\",\"--embedding-size\",\"50\",\"--epochs\",\"2\",\"--learning-rate\",\"0.01\",\"--log-interval\",\"1000\"]', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'SM_CHANNEL_TRAINING': '/opt/ml/input/data/training', 'SM_HP_BATCH-SIZE': '8', 'SM_HP_LOG-INTERVAL': '1000', 'SM_HP_LEARNING-RATE': '0.01', 'SM_HP_EMBEDDING-SIZE': '50', 'SM_HP_EPOCHS': '2'}\u001b[0m\n",
      "\u001b[34m2020-05-25 23:15:09,624 sagemaker_mxnet_container.training INFO     Starting distributed training task\u001b[0m\n",
      "\u001b[34m2020-05-25 23:15:09,982 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-05-25 23:15:09,982 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-05-25 23:15:09,983 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-05-25 23:15:09,983 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/local/bin/python3.6 -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmp4qn40h4l/module_dir\u001b[0m\n",
      "\u001b[34mInstalling collected packages: default-user-module-name\n",
      "    Running setup.py install for default-user-module-name: started\n",
      "    Running setup.py install for default-user-module-name: finished with status 'done'\u001b[0m\n",
      "\u001b[34mSuccessfully installed default-user-module-name-1.0.0\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 19.3.1; however, version 20.1.1 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-05-25 23:15:12,638 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-05-25 23:15:12,654 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-05-25 23:15:12,668 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-05-25 23:15:12,681 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_parameter_server_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_mxnet_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 8,\n",
      "        \"log-interval\": 1000,\n",
      "        \"learning-rate\": 0.01,\n",
      "        \"embedding-size\": 50,\n",
      "        \"epochs\": 2\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"mxnet-training-2020-05-25-23-12-41-684\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-2-722530644018/mxnet-training-2020-05-25-23-12-41-684/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"sentiment\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"sentiment.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":8,\"embedding-size\":50,\"epochs\":2,\"learning-rate\":0.01,\"log-interval\":1000}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=sentiment.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_parameter_server_enabled\":true}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=sentiment\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_mxnet_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-2-722530644018/mxnet-training-2020-05-25-23-12-41-684/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_parameter_server_enabled\":true},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":8,\"embedding-size\":50,\"epochs\":2,\"learning-rate\":0.01,\"log-interval\":1000},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"mxnet-training-2020-05-25-23-12-41-684\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-2-722530644018/mxnet-training-2020-05-25-23-12-41-684/source/sourcedir.tar.gz\",\"module_name\":\"sentiment\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"sentiment.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"8\",\"--embedding-size\",\"50\",\"--epochs\",\"2\",\"--learning-rate\",\"0.01\",\"--log-interval\",\"1000\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_LOG-INTERVAL=1000\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING-RATE=0.01\u001b[0m\n",
      "\u001b[34mSM_HP_EMBEDDING-SIZE=50\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python36.zip:/usr/local/lib/python3.6:/usr/local/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/local/bin/python3.6 sentiment.py --batch-size 8 --embedding-size 50 --epochs 2 --learning-rate 0.01 --log-interval 1000\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mWARNING: discarded 9 sentences longer than the largest bucket.\u001b[0m\n",
      "\u001b[34mWARNING: discarded 27 sentences longer than the largest bucket.\u001b[0m\n",
      "\u001b[34m[2020-05-25 23:15:15.557 ip-10-0-76-85.us-east-2.compute.internal:103 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2020-05-25 23:15:15.558 ip-10-0-76-85.us-east-2.compute.internal:103 INFO hook.py:170] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2020-05-25 23:15:15.558 ip-10-0-76-85.us-east-2.compute.internal:103 INFO hook.py:215] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2020-05-25 23:15:15.583 ip-10-0-76-85.us-east-2.compute.internal:103 INFO hook.py:351] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2020-05-25 23:15:15.596 ip-10-0-76-85.us-east-2.compute.internal:103 INFO hook.py:226] Registering hook for block softmaxcrossentropyloss0\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.6929130554199219,Timestamp=1590448515.6564546,IterationNumber=0)\u001b[0m\n",
      "\u001b[34mERROR:root:'NoneType' object has no attribute 'write'\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.48203080892562866,Timestamp=1590448517.5461679,IterationNumber=500)\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 1000] Training: accuracy=0.737388, 538.033063 samples/s\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.40209853649139404,Timestamp=1590448519.7969728,IterationNumber=1000)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.07480485737323761,Timestamp=1590448524.556141,IterationNumber=1500)\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 2000] Training: accuracy=0.775300, 271.801445 samples/s\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.34132832288742065,Timestamp=1590448530.9327826,IterationNumber=2000)\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.1680513322353363,Timestamp=1590448538.7392519,IterationNumber=2500)\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 3000] Training: accuracy=0.794818, 203.684855 samples/s\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.19035762548446655,Timestamp=1590448547.2512016,IterationNumber=3000)\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.5060407519340515,Timestamp=1590448556.3540409,IterationNumber=3500)\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 4000] Training: accuracy=0.810860, 189.653424 samples/s\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.16063553094863892,Timestamp=1590448566.081243,IterationNumber=4000)\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.031681545078754425,Timestamp=1590448575.8331254,IterationNumber=4500)\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 5000] Training: accuracy=0.821436, 197.456833 samples/s\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.18022628128528595,Timestamp=1590448585.7464125,IterationNumber=5000)\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.1311999410390854,Timestamp=1590448595.8918257,IterationNumber=5500)\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 6000] Training: accuracy=0.828320, 191.795506 samples/s\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.3846375346183777,Timestamp=1590448605.9154015,IterationNumber=6000)\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.03165110573172569,Timestamp=1590448616.0236962,IterationNumber=6500)\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 7000] Training: accuracy=0.834434, 197.050980 samples/s\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.22876334190368652,Timestamp=1590448626.0234497,IterationNumber=7000)\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.5022199749946594,Timestamp=1590448635.8779433,IterationNumber=7500)\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 8000] Training: accuracy=0.839426, 186.058965 samples/s\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.7187082767486572,Timestamp=1590448646.1902242,IterationNumber=8000)\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.6789032220840454,Timestamp=1590448656.6710105,IterationNumber=8500)\u001b[0m\n",
      "\u001b[34m[Epoch 0 Batch 9000] Training: accuracy=0.843559, 191.708938 samples/s\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.5745858550071716,Timestamp=1590448667.035698,IterationNumber=9000)\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.15226857364177704,Timestamp=1590448677.3134549,IterationNumber=9500)\u001b[0m\n",
      "\u001b[34m[Epoch 0] Training: accuracy=0.846120\u001b[0m\n",
      "\u001b[34m[Epoch 0] Validation: accuracy=0.821770\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.6355500817298889,Timestamp=1590448683.6975684,IterationNumber=10000)\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.13009612262248993,Timestamp=1590448693.9066691,IterationNumber=10500)\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 1000] Training: accuracy=0.899101, 393.743555 samples/s\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.7615936994552612,Timestamp=1590448703.9348574,IterationNumber=11000)\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.25921690464019775,Timestamp=1590448714.0465314,IterationNumber=11500)\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 2000] Training: accuracy=0.898301, 396.217034 samples/s\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.140949547290802,Timestamp=1590448723.934221,IterationNumber=12000)\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.1849595606327057,Timestamp=1590448733.957133,IterationNumber=12500)\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 3000] Training: accuracy=0.898284, 382.047092 samples/s\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.47778773307800293,Timestamp=1590448744.4439652,IterationNumber=13000)\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.12833477556705475,Timestamp=1590448754.9276776,IterationNumber=13500)\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 4000] Training: accuracy=0.897494, 415.915911 samples/s\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.14522193372249603,Timestamp=1590448764.941693,IterationNumber=14000)\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.13169075548648834,Timestamp=1590448775.2439785,IterationNumber=14500)\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 5000] Training: accuracy=0.896571, 401.897616 samples/s\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.06368748843669891,Timestamp=1590448785.4420512,IterationNumber=15000)\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.027679329738020897,Timestamp=1590448795.640744,IterationNumber=15500)\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 6000] Training: accuracy=0.896955, 394.136679 samples/s\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.12532390654087067,Timestamp=1590448805.7935014,IterationNumber=16000)\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.859214186668396,Timestamp=1590448816.197443,IterationNumber=16500)\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 7000] Training: accuracy=0.896658, 395.875790 samples/s\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.32073161005973816,Timestamp=1590448826.5054674,IterationNumber=17000)\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.04872264713048935,Timestamp=1590448836.7934194,IterationNumber=17500)\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 8000] Training: accuracy=0.895451, 400.100543 samples/s\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.1196436807513237,Timestamp=1590448847.1404264,IterationNumber=18000)\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.22362284362316132,Timestamp=1590448857.3433561,IterationNumber=18500)\u001b[0m\n",
      "\u001b[34m[Epoch 1 Batch 9000] Training: accuracy=0.895775, 375.694826 samples/s\u001b[0m\n",
      "\u001b[34mDEBUG:root:Writing metric: _RawMetricData(MetricName='softmaxcrossentropyloss0_output_0_GLOBAL',Value=0.274025022983551,Timestamp=1590448867.6804068,IterationNumber=19000)\u001b[0m\n",
      "\u001b[34m[Epoch 1] Training: accuracy=0.895547\u001b[0m\n",
      "\u001b[34m[Epoch 1] Validation: accuracy=0.809211\u001b[0m\n",
      "\u001b[34mVocabulary saved to \"%s\" /opt/ml/model/vocab.json\u001b[0m\n",
      "\u001b[34m[2020-05-25 23:21:16.497 ip-10-0-76-85.us-east-2.compute.internal:103 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\u001b[34m2020-05-25 23:21:16,635 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-05-25 23:21:28 Uploading - Uploading generated training model\n",
      "2020-05-25 23:21:28 Completed - Training job completed\n",
      "Training seconds: 419\n",
      "Billable seconds: 419\n"
     ]
    }
   ],
   "source": [
    "m2.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the logs, our model gets over 80% accuracy on the test set using the above hyperparameters.\n",
    "\n",
    "After training, we use our `MXNet` object to build and deploy an `MXNetPredictor` object. This creates a SageMaker Endpoint that we can use to perform inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using already existing model: mxnet-training-2020-05-25-23-12-41-684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------!"
     ]
    }
   ],
   "source": [
    "predictor = m2.deploy(initial_instance_count=1, instance_type='ml.t2.medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our predictor, we can perform inference on a JSON-encoded string array. \n",
    "\n",
    "The predictor runs inference on our input data and returns the predicted sentiment (1 for positive and 0 for negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "data = [\"this movie was extremely good .\",\n",
    "        \"the plot was very boring .\",\n",
    "        \"this film is so slick , superficial and trend-hoppy .\",\n",
    "        \"i just could not watch it till the end .\",\n",
    "        \"the movie was so enthralling !\"]\n",
    "\n",
    "response = predictor.predict(data)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "data = [\"the store was not very good i didnt have a pleasant time\"]\n",
    "\n",
    "response = predictor.predict(data)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1]\n"
     ]
    }
   ],
   "source": [
    "data = [\"the store was not very good i didnt have a pleasant time\", \n",
    "         \"one two three\"\n",
    "        ]\n",
    "\n",
    "response = predictor.predict(data)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "After you have finished with this example, remember to delete the prediction endpoint to release the instance(s) associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
