{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ObjectToVec model for MovieLens recommendation\n",
    "\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Data exploration and preparation](#Data-exploration-and-preparation)\n",
    "1. [Rating prediction task](#Rating-prediction-task)\n",
    "1. [Recommendation task](#Recommendation-task)\n",
    "1. [Movie retrieval in the embedding space](#Movie-retrieval-in-the-embedding-space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "### ObjectToVec\n",
    "*Object2Vec* is a highly customizable multi-purpose algorithm that can learn embeddings of pairs of objects. The embeddings are learned such that it preserves their pairwise **similarities** in the original space.\n",
    "- **Similarity** is user-defined: users need to provide the algorithm with pairs of objects that they define as similar (1) or dissimilar (0); alternatively, the users can define similarity in a continuous sense (provide a real-valued similarity score)\n",
    "- The learned embeddings can be used to efficiently compute nearest neighbors of objects, as well as to visualize natural clusters of related objects in the embedding space. In addition, the embeddings can also be used as features of the corresponding objects in downstream supervised tasks such as classification or regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this notebook example:\n",
    "We demonstrate how Object2Vec can be used to solve problems arising in recommendation systems. Specifically,\n",
    "\n",
    "- We provide the algorithm with (UserID, MovieID) pairs; for each such pair, we also provide a \"label\" that tells the algorithm whether the user and movie are similar or not\n",
    "\n",
    "     * When the labels are real-valued, we use the algorithm to predict the exact ratings of a movie given a user\n",
    "     * When the labels are binary, we use the algorithm to recommendation movies to users\n",
    "\n",
    "- The diagram below shows the customization of our model to the problem of predicting movie ratings, using a dataset that provides `(UserID, ItemID, Rating)` samples. Here, ratings are real-valued"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float:middle\" src=\"image_ml_rating.png\" width=\"480\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "- We use the MovieLens 100k dataset: https://grouplens.org/datasets/movielens/100k/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use cases\n",
    "\n",
    "- Task 1: Rating prediction (regression)\n",
    "- Task 2: Movie recommendation (classification)\n",
    "- Task 3: Nearest-neighbor movie retrieval in the learned embedding space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before running the notebook\n",
    "- Please use a Python 3 kernel for the notebook\n",
    "- Please make sure you have `jsonlines` package installed (if not, you can run the command below to install it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jsonlines\n",
      "  Downloading https://files.pythonhosted.org/packages/4f/9a/ab96291470e305504aa4b7a2e0ec132e930da89eb3ca7a82fbe03167c131/jsonlines-1.2.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from jsonlines) (1.11.0)\n",
      "Installing collected packages: jsonlines\n",
      "Successfully installed jsonlines-1.2.0\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv, jsonlines\n",
    "import numpy as np\n",
    "import copy\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data exploration and preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### License\n",
    "Please be aware of the following requirements about ackonwledgment, copyright and availability, cited from the [data set description page](http://files.grouplens.org/datasets/movielens/ml-100k-README.txt).\n",
    ">The data set may be used for any research\n",
    "purposes under the following conditions:\n",
    "     * The user may not state or imply any endorsement from the\n",
    "       University of Minnesota or the GroupLens Research Group.\n",
    "     * The user must acknowledge the use of the data set in\n",
    "       publications resulting from the use of the data set\n",
    "       (see below for citation information).\n",
    "     * The user may not redistribute the data without separate\n",
    "       permission.\n",
    "     * The user may not use this information for any commercial or\n",
    "       revenue-bearing purposes without first obtaining permission\n",
    "       from a faculty member of the GroupLens Research Project at the\n",
    "       University of Minnesota.\n",
    "If you have any further questions or comments, please contact GroupLens \\<grouplens-info@cs.umn.edu\\>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ml-100k.zip\n",
      "   creating: ml-100k/\n",
      "  inflating: ml-100k/allbut.pl       \n",
      "  inflating: ml-100k/mku.sh          \n",
      "  inflating: ml-100k/README          \n",
      "  inflating: ml-100k/u.data          \n",
      "  inflating: ml-100k/u.genre         \n",
      "  inflating: ml-100k/u.info          \n",
      "  inflating: ml-100k/u.item          \n",
      "  inflating: ml-100k/u.occupation    \n",
      "  inflating: ml-100k/u.user          \n",
      "  inflating: ml-100k/u1.base         \n",
      "  inflating: ml-100k/u1.test         \n",
      "  inflating: ml-100k/u2.base         \n",
      "  inflating: ml-100k/u2.test         \n",
      "  inflating: ml-100k/u3.base         \n",
      "  inflating: ml-100k/u3.test         \n",
      "  inflating: ml-100k/u4.base         \n",
      "  inflating: ml-100k/u4.test         \n",
      "  inflating: ml-100k/u5.base         \n",
      "  inflating: ml-100k/u5.test         \n",
      "  inflating: ml-100k/ua.base         \n",
      "  inflating: ml-100k/ua.test         \n",
      "  inflating: ml-100k/ub.base         \n",
      "  inflating: ml-100k/ub.test         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 4808k  100 4808k    0     0  5988k      0 --:--:-- --:--:-- --:--:-- 5980k\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "curl -o ml-100k.zip http://files.grouplens.org/datasets/movielens/ml-100k.zip\n",
    "unzip ml-100k.zip\n",
    "rm ml-100k.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first create some utility functions for data exploration and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## some utility functions\n",
    "\n",
    "def load_csv_data(filename, delimiter, verbose=True):\n",
    "    \"\"\"\n",
    "    input: a file readable as csv and separated by a delimiter\n",
    "    and has format users - movies - ratings - etc\n",
    "    output: a list, where each row of the list is of the form\n",
    "    {'in0':userID, 'in1':movieID, 'label':rating}\n",
    "    \"\"\"\n",
    "    to_data_list = list()\n",
    "    users = list()\n",
    "    movies = list()\n",
    "    ratings = list()\n",
    "    unique_users = set()\n",
    "    unique_movies = set()\n",
    "    with open(filename, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=delimiter)\n",
    "        for count, row in enumerate(reader):\n",
    "            #if count!=0:\n",
    "            to_data_list.append({'in0':[int(row[0])], 'in1':[int(row[1])], 'label':float(row[2])})\n",
    "            users.append(row[0])\n",
    "            movies.append(row[1])\n",
    "            ratings.append(float(row[2]))\n",
    "            unique_users.add(row[0])\n",
    "            unique_movies.add(row[1])\n",
    "    if verbose:\n",
    "        print(\"In file {}, there are {} ratings\".format(filename, len(ratings)))\n",
    "        print(\"The ratings have mean: {}, median: {}, and variance: {}\".format(\n",
    "                                            round(np.mean(ratings), 2), \n",
    "                                            round(np.median(ratings), 2), \n",
    "                                            round(np.var(ratings), 2)))\n",
    "        print(\"There are {} unique users and {} unique movies\".format(len(unique_users), len(unique_movies)))\n",
    "    return to_data_list\n",
    "\n",
    "\n",
    "def csv_to_augmented_data_dict(filename, delimiter):\n",
    "    \"\"\"\n",
    "    Input: a file that must be readable as csv and separated by delimiter (to make columns)\n",
    "    has format users - movies - ratings - etc\n",
    "    Output:\n",
    "      Users dictionary: keys as user ID's; each key corresponds to a list of movie ratings by that user\n",
    "      Movies dictionary: keys as movie ID's; each key corresponds a list of ratings of that movie by different users\n",
    "    \"\"\"\n",
    "    to_users_dict = dict() \n",
    "    to_movies_dict = dict()\n",
    "    with open(filename, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=delimiter)\n",
    "        for count, row in enumerate(reader):\n",
    "            #if count!=0:\n",
    "            if row[0] not in to_users_dict:\n",
    "                to_users_dict[row[0]] = [(row[1], row[2])]\n",
    "            else:\n",
    "                to_users_dict[row[0]].append((row[1], row[2]))\n",
    "            if row[1] not in to_movies_dict:\n",
    "                to_movies_dict[row[1]] = list(row[0])\n",
    "            else:\n",
    "                to_movies_dict[row[1]].append(row[0])\n",
    "    return to_users_dict, to_movies_dict\n",
    "\n",
    "\n",
    "def user_dict_to_data_list(user_dict):\n",
    "    # turn user_dict format to data list format (acceptable to the algorithm)\n",
    "    data_list = list()\n",
    "    for user, movie_rating_list in user_dict.items():\n",
    "        for movie, rating in movie_rating_list:\n",
    "            data_list.append({'in0':[int(user)], 'in1':[int(movie)], 'label':float(rating)})\n",
    "    return data_list\n",
    "\n",
    "def divide_user_dicts(user_dict, sp_ratio_dict):\n",
    "    \"\"\"\n",
    "    Input: A user dictionary, a ration dictionary\n",
    "         - format of sp_ratio_dict = {'train':0.8, \"test\":0.2}\n",
    "    Output: \n",
    "        A dictionary of dictionaries, with key corresponding to key provided by sp_ratio_dict\n",
    "        and each key corresponds to a subdivded user dictionary\n",
    "    \"\"\"\n",
    "    ratios = [val for _, val in sp_ratio_dict.items()]\n",
    "    assert np.sum(ratios) == 1, \"the sampling ratios must sum to 1!\"\n",
    "    divided_dict = {}\n",
    "    for user, movie_rating_list in user_dict.items():\n",
    "        sub_movies_ptr = 0\n",
    "        sub_movies_list = []\n",
    "        #movie_list, _ = zip(*movie_rating_list)\n",
    "        #print(movie_list)\n",
    "        for i, ratio in enumerate(ratios):\n",
    "            if i < len(ratios)-1:\n",
    "                sub_movies_ptr_end = sub_movies_ptr + int(len(movie_rating_list)*ratio)\n",
    "                sub_movies_list.append(movie_rating_list[sub_movies_ptr:sub_movies_ptr_end])\n",
    "                sub_movies_ptr = sub_movies_ptr_end\n",
    "            else:\n",
    "                sub_movies_list.append(movie_rating_list[sub_movies_ptr:])\n",
    "        for subset_name in sp_ratio_dict.keys():\n",
    "            if subset_name not in divided_dict:\n",
    "                divided_dict[subset_name] = {user: sub_movies_list.pop(0)}\n",
    "            else:\n",
    "                #access sub-dictionary\n",
    "                divided_dict[subset_name][user] = sub_movies_list.pop(0)\n",
    "    \n",
    "    return divided_dict\n",
    "\n",
    "def write_csv_to_jsonl(jsonl_fname, csv_fname, csv_delimiter):\n",
    "    \"\"\"\n",
    "    Input: a file readable as csv and separated by delimiter (to make columns)\n",
    "        - has format users - movies - ratings - etc\n",
    "    Output: a jsonline file converted from the csv file\n",
    "    \"\"\"\n",
    "    with jsonlines.open(jsonl_fname, mode='w') as writer:\n",
    "        with open(csv_fname, 'r') as csvfile:\n",
    "            reader = csv.reader(csvfile, delimiter=csv_delimiter)\n",
    "            for count, row in enumerate(reader):\n",
    "                #print(row)\n",
    "                #if count!=0:\n",
    "                writer.write({'in0':[int(row[0])], 'in1':[int(row[1])], 'label':float(row[2])})\n",
    "        print('Created {} jsonline file'.format(jsonl_fname))\n",
    "                    \n",
    "    \n",
    "def write_data_list_to_jsonl(data_list, to_fname):\n",
    "    \"\"\"\n",
    "    Input: a data list, where each row of the list is a Python dictionary taking form\n",
    "    {'in0':userID, 'in1':movieID, 'label':rating}\n",
    "    Output: save the list as a jsonline file\n",
    "    \"\"\"\n",
    "    with jsonlines.open(to_fname, mode='w') as writer:\n",
    "        for row in data_list:\n",
    "            #print(row)\n",
    "            writer.write({'in0':row['in0'], 'in1':row['in1'], 'label':row['label']})\n",
    "    print(\"Created {} jsonline file\".format(to_fname))\n",
    "\n",
    "def data_list_to_inference_format(data_list, binarize=True, label_thres=3):\n",
    "    \"\"\"\n",
    "    Input: a data list\n",
    "    Output: test data and label, acceptable by SageMaker for inference\n",
    "    \"\"\"\n",
    "    data_ = [({\"in0\":row['in0'], 'in1':row['in1']}, row['label']) for row in data_list]\n",
    "    data, label = zip(*data_)\n",
    "    infer_data = {\"instances\":data}\n",
    "    if binarize:\n",
    "        label = get_binarized_label(list(label), label_thres)\n",
    "    return infer_data, label\n",
    "\n",
    "\n",
    "def get_binarized_label(data_list, thres):\n",
    "    \"\"\"\n",
    "    Input: data list\n",
    "    Output: a binarized data list for recommendation task\n",
    "    \"\"\"\n",
    "    for i, row in enumerate(data_list):\n",
    "        if type(row) is dict:\n",
    "            #if i < 10:\n",
    "                #print(row['label'])\n",
    "            if row['label'] > thres:\n",
    "                #print(row)\n",
    "                data_list[i]['label'] = 1\n",
    "            else:\n",
    "                data_list[i]['label'] = 0\n",
    "        else:\n",
    "            if row > thres:\n",
    "                data_list[i] = 1\n",
    "            else:\n",
    "                data_list[i] = 0\n",
    "    return data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In file ml-100k/ua.base, there are 90570 ratings\n",
      "The ratings have mean: 3.52, median: 4.0, and variance: 1.27\n",
      "There are 943 unique users and 1680 unique movies\n",
      "In file ml-100k/ua.test, there are 9430 ratings\n",
      "The ratings have mean: 3.59, median: 4.0, and variance: 1.25\n",
      "There are 943 unique users and 1129 unique movies\n"
     ]
    }
   ],
   "source": [
    "## Load data and shuffle\n",
    "prefix = 'ml-100k'\n",
    "train_path = os.path.join(prefix, 'ua.base')\n",
    "valid_path = os.path.join(prefix, 'ua.test')\n",
    "test_path = os.path.join(prefix, 'ub.test')\n",
    "\n",
    "train_data_list = load_csv_data(train_path, '\\t')\n",
    "random.shuffle(train_data_list)\n",
    "validation_data_list = load_csv_data(valid_path, '\\t')\n",
    "random.shuffle(validation_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_users_dict, to_movies_dict = csv_to_augmented_data_dict(train_path, '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We perform some data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The min, max, and median 'movies per user' is 10, 727, and 55.0\n",
      "The min, max, and median 'users per movie' is 1, 495, and 25.0\n",
      "In the training set\n",
      "There are 213 users with no more than 20 movies\n",
      "There are 12 movies with no more than 2 user\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Users per movie')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Calculate min, max, median of number of movies per user\n",
    "movies_per_user = [len(val) for key, val in to_users_dict.items()]\n",
    "\n",
    "print(\"The min, max, and median 'movies per user' is {}, {}, and {}\".format(np.amin(movies_per_user),\n",
    "                                                                         np.amax(movies_per_user),\n",
    "                                                                         np.median(movies_per_user)))\n",
    "users_per_movie = [len(val) for key, val in to_movies_dict.items()]\n",
    "print(\"The min, max, and median 'users per movie' is {}, {}, and {}\".format(np.amin(users_per_movie),\n",
    "                                                                         np.amax(users_per_movie),\n",
    "                                                                          np.median(users_per_movie)))\n",
    "\n",
    "\n",
    "count = 0\n",
    "n_movies_lower_bound = 20\n",
    "for n_movies in movies_per_user:\n",
    "    if n_movies <= n_movies_lower_bound:\n",
    "        count += 1\n",
    "print(\"In the training set\")\n",
    "print('There are {} users with no more than {} movies'.format(count, n_movies_lower_bound))\n",
    "#\n",
    "count = 0\n",
    "n_users_lower_bound = 2\n",
    "for n_users in users_per_movie:\n",
    "    if n_users <= n_users_lower_bound:\n",
    "        count += 1\n",
    "print('There are {} movies with no more than {} user'.format(count, n_users_lower_bound))\n",
    "\n",
    "\n",
    "## figures\n",
    "\n",
    "f = plt.figure(1)\n",
    "plt.hist(movies_per_user)\n",
    "plt.title(\"Movies per user\")\n",
    "##\n",
    "g = plt.figure(2)\n",
    "plt.hist(users_per_movie)\n",
    "plt.title(\"Users per movie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the number of movies with an extremely small number of users (<3) is negligible compared to the total number of movies, we will not remove movies from the data set (same applies for users) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created train_r.jsonl jsonline file\n",
      "Created validation_r.jsonl jsonline file\n"
     ]
    }
   ],
   "source": [
    "## Save training and validation data locally for rating-prediction (regression) task\n",
    "\n",
    "write_data_list_to_jsonl(copy.deepcopy(train_data_list), 'train_r.jsonl')\n",
    "write_data_list_to_jsonl(copy.deepcopy(validation_data_list), 'validation_r.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created train_c.jsonl jsonline file\n",
      "Created validation_c.jsonl jsonline file\n"
     ]
    }
   ],
   "source": [
    "## Save training and validation data locally for recommendation (classification) task\n",
    "\n",
    "### binarize the data \n",
    "\n",
    "train_c = get_binarized_label(copy.deepcopy(train_data_list), 3.0)\n",
    "valid_c = get_binarized_label(copy.deepcopy(validation_data_list), 3.0)\n",
    "\n",
    "write_data_list_to_jsonl(train_c, 'train_c.jsonl')\n",
    "write_data_list_to_jsonl(valid_c, 'validation_c.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We check whether the two classes are balanced after binarization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0.5510213094843768 fraction of positive ratings in train_c.jsonl\n",
      "There are 0.5799575821845175 fraction of positive ratings in validation_c.jsonl\n"
     ]
    }
   ],
   "source": [
    "train_c_label = [row['label'] for row in train_c]\n",
    "valid_c_label = [row['label'] for row in valid_c]\n",
    "\n",
    "print(\"There are {} fraction of positive ratings in train_c.jsonl\".format(\n",
    "                                np.count_nonzero(train_c_label)/len(train_c_label)))\n",
    "print(\"There are {} fraction of positive ratings in validation_c.jsonl\".format(\n",
    "                                np.sum(valid_c_label)/len(valid_c_label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rating prediction task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mse_loss(res, labels):\n",
    "    if type(res) is dict:\n",
    "        res = res['predictions']\n",
    "    assert len(res)==len(labels), 'result and label length mismatch!'\n",
    "    loss = 0\n",
    "    for row, label in zip(res, labels):\n",
    "        if type(row)is dict:\n",
    "            loss += (row['scores'][0]-label)**2\n",
    "        else:\n",
    "            loss += (row-label)**2\n",
    "    return round(loss/float(len(labels)), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_r_data, valid_r_label = data_list_to_inference_format(copy.deepcopy(validation_data_list), binarize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We first test the problem on two baseline algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline 1\n",
    "\n",
    "A naive approach to predict movie ratings on unseen data is to use the global average of the user predictions in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Baseline 1 (global rating average) prediction is 3.52\n",
      "The validation mse loss of the Baseline 1 is 1.26\n"
     ]
    }
   ],
   "source": [
    "train_r_label = [row['label'] for row in copy.deepcopy(train_data_list)]\n",
    "\n",
    "bs1_prediction = round(np.mean(train_r_label), 2)\n",
    "print('The Baseline 1 (global rating average) prediction is {}'.format(bs1_prediction))\n",
    "print(\"The validation mse loss of the Baseline 1 is {}\".format(\n",
    "                                     get_mse_loss(len(valid_r_label)*[bs1_prediction], valid_r_label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use a better baseline, which is to perform prediction on unseen data based on the user-averaged ratings of movies on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bs2_predictor(test_data, user_dict, is_classification=False, thres=3):\n",
    "    test_data = copy.deepcopy(test_data['instances'])\n",
    "    predictions = list()\n",
    "    for row in test_data:\n",
    "        userID = str(row[\"in0\"][0])\n",
    "        # predict movie ID based on local average of user's prediction\n",
    "        local_movies, local_ratings = zip(*user_dict[userID])\n",
    "        local_ratings = [float(score) for score in local_ratings]\n",
    "        predictions.append(np.mean(local_ratings))\n",
    "        if is_classification:\n",
    "            predictions[-1] = int(predictions[-1] > 3)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation loss of the Baseline 2 (user-based rating average) is 1.09\n"
     ]
    }
   ],
   "source": [
    "bs2_prediction = bs2_predictor(valid_r_data, to_users_dict, is_classification=False)\n",
    "print(\"The validation loss of the Baseline 2 (user-based rating average) is {}\".format(\n",
    "                                     get_mse_loss(bs2_prediction, valid_r_label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use *Object2Vec* to predict the movie ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training and inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define S3 bucket that hosts data and model, and upload data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import boto3 \n",
    "import os\n",
    " \n",
    "bucket = 'ml-demo-data-store-demos\n",
    "input_prefix = 'object2vec/movielens/input'\n",
    "output_prefix = 'object2vec/movielens/output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload data to S3 and make data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded train data to s3://ml-demo-data-store-demos/object2vec/movielens/input/rating/train/train_r.jsonl and defined input path\n",
      "Uploaded validation data to s3://ml-demo-data-store-demos/object2vec/movielens/input/rating/validation/validation_r.jsonl and defined input path\n",
      "Trained model will be saved at s3://ml-demo-data-store-demos/object2vec/movielens/output\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.session import s3_input\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "input_paths = {}\n",
    "output_path = os.path.join('s3://', bucket, output_prefix)\n",
    "\n",
    "for data_name in ['train', 'validation']:\n",
    "    pre_key = os.path.join(input_prefix, 'rating', f'{data_name}')\n",
    "    fname = '{}_r.jsonl'.format(data_name)\n",
    "    data_path = os.path.join('s3://', bucket, pre_key, fname)\n",
    "    s3_client.upload_file(fname, bucket, os.path.join(pre_key, fname))\n",
    "    input_paths[data_name] = s3_input(data_path, distribution='ShardedByS3Key', content_type='application/jsonlines')\n",
    "    print('Uploaded {} data to {} and defined input path'.format(data_name, data_path))\n",
    "\n",
    "print('Trained model will be saved at', output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get ObjectToVec algorithm image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::722530644018:role/service-role/AmazonSageMaker-ExecutionRole-20200511T234976\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)\n",
    "\n",
    "## Get docker image of ObjectToVec algorithm\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(boto3.Session().region_name, 'object2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We first define training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"_kvstore\": \"device\",\n",
    "    \"_num_gpus\": \"auto\",\n",
    "    \"_num_kv_servers\": \"auto\",\n",
    "    \"bucket_width\": 0,\n",
    "    \"early_stopping_patience\": 3,\n",
    "    \"early_stopping_tolerance\": 0.01,\n",
    "    \"enc0_cnn_filter_width\": 3,\n",
    "    \"enc0_layers\": \"auto\",\n",
    "    \"enc0_max_seq_len\": 1,\n",
    "    \"enc0_network\": \"pooled_embedding\",\n",
    "    \"enc0_token_embedding_dim\": 300,\n",
    "    \"enc0_vocab_size\": 944,\n",
    "    \"enc1_layers\": \"auto\",\n",
    "    \"enc1_max_seq_len\": 1,\n",
    "    \"enc1_network\": \"pooled_embedding\",\n",
    "    \"enc1_token_embedding_dim\": 300,\n",
    "    \"enc1_vocab_size\": 1684,\n",
    "    \"enc_dim\": 1024,\n",
    "    \"epochs\": 20,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"mini_batch_size\": 64,\n",
    "    \"mlp_activation\": \"tanh\",\n",
    "    \"mlp_dim\": 256,\n",
    "    \"mlp_layers\": 1,\n",
    "    \"num_classes\": 2,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"output_layer\": \"mean_squared_error\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-19 02:12:04 Starting - Starting the training job...\n",
      "2020-05-19 02:12:07 Starting - Launching requested ML instances......\n",
      "2020-05-19 02:13:10 Starting - Preparing the instances for training......\n",
      "2020-05-19 02:14:19 Downloading - Downloading input data\n",
      "2020-05-19 02:14:19 Training - Downloading the training image.....\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:18 INFO 140615234824000] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'comparator_list': u'hadamard, concat, abs_diff', u'enc0_vocab_file': u'', u'output_layer': u'softmax', u'enc0_cnn_filter_width': 3, u'epochs': 30, u'mlp_dim': 512, u'enc0_freeze_pretrained_embedding': u'true', u'mlp_layers': 2, u'_num_kv_servers': u'auto', u'weight_decay': 0, u'enc0_pretrained_embedding_file': u'', u'token_embedding_storage_type': u'dense', u'enc0_token_embedding_dim': 300, u'tied_token_embedding_weight': u'false', u'learning_rate': 0.0004, u'enc1_cnn_filter_width': 3, u'negative_sampling_rate': 0, u'enc0_network': u'hcnn', u'enc1_layers': u'auto', u'early_stopping_patience': 3, u'optimizer': u'adam', u'_tuning_objective_metric': u'', u'early_stopping_tolerance': 0.01, u'dropout': 0, u'bucket_width': 0, u'enc_dim': 4096, u'enc1_vocab_file': u'', u'enc1_freeze_pretrained_embedding': u'true', u'enc0_layers': u'auto', u'mini_batch_size': 32, u'enc1_pretrained_embedding_file': u'', u'num_classes': 2, u'_num_gpus': u'auto', u'enc1_token_embedding_dim': 300, u'mlp_activation': u'linear', u'enc1_network': u'enc0', u'_kvstore': u'auto_gpu'}\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:18 INFO 140615234824000] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'output_layer': u'mean_squared_error', u'enc0_cnn_filter_width': u'3', u'epochs': u'20', u'mlp_dim': u'256', u'mlp_layers': u'1', u'_num_kv_servers': u'auto', u'mini_batch_size': u'64', u'enc0_token_embedding_dim': u'300', u'enc1_network': u'pooled_embedding', u'enc0_network': u'pooled_embedding', u'enc1_layers': u'auto', u'early_stopping_patience': u'3', u'optimizer': u'adam', u'enc1_max_seq_len': u'1', u'early_stopping_tolerance': u'0.01', u'learning_rate': u'0.001', u'bucket_width': u'0', u'enc_dim': u'1024', u'enc0_layers': u'auto', u'enc0_max_seq_len': u'1', u'num_classes': u'2', u'_num_gpus': u'auto', u'mlp_activation': u'tanh', u'enc1_token_embedding_dim': u'300', u'_kvstore': u'device', u'enc1_vocab_size': u'1684', u'enc0_vocab_size': u'944'}\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:18 INFO 140615234824000] Final configuration: {u'comparator_list': u'hadamard, concat, abs_diff', u'enc0_vocab_file': u'', u'output_layer': u'mean_squared_error', u'enc0_cnn_filter_width': u'3', u'epochs': u'20', u'mlp_dim': u'256', u'enc0_freeze_pretrained_embedding': u'true', u'mlp_layers': u'1', u'_num_kv_servers': u'auto', u'weight_decay': 0, u'enc0_pretrained_embedding_file': u'', u'enc0_max_seq_len': u'1', u'token_embedding_storage_type': u'dense', u'enc0_token_embedding_dim': u'300', u'tied_token_embedding_weight': u'false', u'learning_rate': u'0.001', u'enc1_cnn_filter_width': 3, u'negative_sampling_rate': 0, u'enc0_network': u'pooled_embedding', u'enc1_layers': u'auto', u'early_stopping_patience': u'3', u'optimizer': u'adam', u'enc1_max_seq_len': u'1', u'_tuning_objective_metric': u'', u'early_stopping_tolerance': u'0.01', u'dropout': 0, u'bucket_width': u'0', u'enc_dim': u'1024', u'enc1_vocab_file': u'', u'enc1_freeze_pretrained_embedding': u'true', u'enc0_layers': u'auto', u'mini_batch_size': u'64', u'enc1_pretrained_embedding_file': u'', u'num_classes': u'2', u'_num_gpus': u'auto', u'enc1_token_embedding_dim': u'300', u'mlp_activation': u'tanh', u'enc1_network': u'pooled_embedding', u'_kvstore': u'device', u'enc1_vocab_size': u'1684', u'enc0_vocab_size': u'944'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:18 INFO 140615234824000] Using default worker.\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:18 INFO 140615234824000] Loaded iterator creator application/jsonlines for content type ('application/jsonlines', '1.0')\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:18 INFO 140615234824000] create_iter params {u'comparator_list': u'hadamard, concat, abs_diff', u'enc0_vocab_file': u'', u'output_layer': u'mean_squared_error', u'enc0_cnn_filter_width': u'3', u'epochs': u'20', u'mlp_dim': u'256', u'enc0_freeze_pretrained_embedding': u'true', u'mlp_layers': u'1', u'_num_kv_servers': u'auto', u'weight_decay': 0, u'enc0_pretrained_embedding_file': u'', u'enc0_max_seq_len': u'1', u'token_embedding_storage_type': u'dense', u'enc0_token_embedding_dim': u'300', u'tied_token_embedding_weight': u'false', u'learning_rate': u'0.001', u'enc1_cnn_filter_width': 3, u'negative_sampling_rate': 0, u'enc0_network': u'pooled_embedding', u'enc1_layers': u'auto', u'early_stopping_patience': u'3', u'optimizer': u'adam', u'enc1_max_seq_len': u'1', u'_tuning_objective_metric': u'', u'early_stopping_tolerance': u'0.01', u'dropout': 0, u'bucket_width': u'0', u'enc_dim': u'1024', u'enc1_vocab_file': u'', u'enc1_freeze_pretrained_embedding': u'true', u'enc0_layers': u'auto', u'mini_batch_size': u'64', u'enc1_pretrained_embedding_file': u'', u'num_classes': u'2', u'_num_gpus': u'auto', u'enc1_token_embedding_dim': u'300', u'mlp_activation': u'tanh', u'enc1_network': u'pooled_embedding', u'_kvstore': u'device', u'enc1_vocab_size': u'1684', u'enc0_vocab_size': u'944'}\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:18 INFO 140615234824000] create_iter content_params {}\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:18 INFO 140615234824000] Parameters of encoders: [{u'layers': u'auto', u'vocab_size': u'944', u'network': u'pooled_embedding', u'vocab_file': u'', u'cnn_filter_width': u'3', u'token_embedding_dim': u'300', u'max_seq_len': u'1', u'freeze_pretrained_embedding': u'true', u'pretrained_embedding_file': u''}, {u'layers': u'auto', u'vocab_size': u'1684', u'network': u'pooled_embedding', u'vocab_file': u'', u'cnn_filter_width': 3, u'token_embedding_dim': u'300', u'max_seq_len': u'1', u'freeze_pretrained_embedding': u'true', u'pretrained_embedding_file': u''}]\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:18 INFO 140615234824000] output_layer is set to mean_squared_error. Ignoring the hyperparameter 'num_classes'.\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:18 INFO 140615234824000] Inconsistent enc_dim and token_embedding_dim found for token-embedding-encoder: 1024 vs 300. Setting token embedding dim to be 1024\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:18 INFO 140615234824000] Inconsistent enc_dim and token_embedding_dim found for token-embedding-encoder: 1024 vs 300. Setting token embedding dim to be 1024\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:18 INFO 140615234824000] Encoder configs: [{'vocab_size': 944, 'enc_index': 0, 'pretrained_embedding_file_path': None, 'vocab_dict': None, 'token_embedding_dim': 1024, 'vocab_file': '', 'freeze_pretrained_embedding': True, 'num_layers': 1, 'is_train': True, 'dropout': 0.0, 'pretrained_embedding_file': ''}, {'vocab_size': 1684, 'enc_index': 1, 'pretrained_embedding_file_path': None, 'vocab_dict': None, 'token_embedding_dim': 1024, 'vocab_file': '', 'freeze_pretrained_embedding': True, 'num_layers': 1, 'is_train': True, 'dropout': 0.0, 'pretrained_embedding_file': ''}]\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:18 INFO 140615234824000] Config: {'comparator_list': ['hadamard', 'concat', 'abs_diff'], 'epochs': 20, 'mini_batch_size': 64, 'optimizer': 'adam', 'output_layer': 'mean_squared_error', 'token_embedding_storage_type': 'dense', 'mlp_dim': 256, 'early_stopping_tolerance': 0.01, 'dropout': 0.0, 'bucket_width': 0, 'enc_dim': 1024, 'negative_sampling_rate': 0, 'early_stopping_patience': 3, 'learning_rate': 0.001, 'max_seq_lens': [1, 1], 'tied_token_embedding_weight': False, 'enc_configs': [{'vocab_size': 944, 'enc_index': 0, 'pretrained_embedding_file_path': None, 'vocab_dict': None, 'token_embedding_dim': 1024, 'vocab_file': '', 'freeze_pretrained_embedding': True, 'num_layers': 1, 'is_train': True, 'dropout': 0.0, 'pretrained_embedding_file': ''}, {'vocab_size': 1684, 'enc_index': 1, 'pretrained_embedding_file_path': None, 'vocab_dict': None, 'token_embedding_dim': 1024, 'vocab_file': '', 'freeze_pretrained_embedding': True, 'num_layers': 1, 'is_train': True, 'dropout': 0.0, 'pretrained_embedding_file': ''}], 'mlp_layers': 1, 'weight_decay': 0.0, 'mlp_activation': 'tanh', 'num_classes': None}\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:18 INFO 140615234824000] use bucketing: False\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:18 INFO 140615234824000] Creating data iterator for /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] Source words: 90570\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] Target words: 90570\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] Total: 90570 samples in 1 buckets\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] Bucket of (1, 1) : 90570 samples in 1415 batches of 64, approx 64.0 words/batch\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] 0 sentence pairs discarded\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] fill up mode: replicate\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] Negative sampling not used\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] Replicating 54 random sentences from bucket (1, 1) to size it to multiple of 64\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] Bucket batch sizes: [BucketBatchSize(batch_size=64, average_words_per_batch=64)]\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] create_iter params {u'comparator_list': u'hadamard, concat, abs_diff', u'enc0_vocab_file': u'', u'output_layer': u'mean_squared_error', u'enc0_cnn_filter_width': u'3', u'epochs': u'20', u'mlp_dim': u'256', u'enc0_freeze_pretrained_embedding': u'true', u'mlp_layers': u'1', u'_num_kv_servers': u'auto', u'weight_decay': 0, u'enc0_pretrained_embedding_file': u'', u'enc0_max_seq_len': u'1', u'token_embedding_storage_type': u'dense', u'enc0_token_embedding_dim': u'300', u'tied_token_embedding_weight': u'false', u'learning_rate': u'0.001', u'enc1_cnn_filter_width': 3, u'negative_sampling_rate': 0, u'enc0_network': u'pooled_embedding', u'enc1_layers': u'auto', u'early_stopping_patience': u'3', u'optimizer': u'adam', u'enc1_max_seq_len': u'1', u'_tuning_objective_metric': u'', u'early_stopping_tolerance': u'0.01', u'dropout': 0, u'bucket_width': u'0', u'enc_dim': u'1024', u'enc1_vocab_file': u'', u'enc1_freeze_pretrained_embedding': u'true', u'enc0_layers': u'auto', u'mini_batch_size': u'64', u'enc1_pretrained_embedding_file': u'', u'num_classes': u'2', u'_num_gpus': u'auto', u'enc1_token_embedding_dim': u'300', u'mlp_activation': u'tanh', u'enc1_network': u'pooled_embedding', u'_kvstore': u'device', u'enc1_vocab_size': u'1684', u'enc0_vocab_size': u'944'}\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] create_iter content_params {}\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] Parameters of encoders: [{u'layers': u'auto', u'vocab_size': u'944', u'network': u'pooled_embedding', u'vocab_file': u'', u'cnn_filter_width': u'3', u'token_embedding_dim': u'300', u'max_seq_len': u'1', u'freeze_pretrained_embedding': u'true', u'pretrained_embedding_file': u''}, {u'layers': u'auto', u'vocab_size': u'1684', u'network': u'pooled_embedding', u'vocab_file': u'', u'cnn_filter_width': 3, u'token_embedding_dim': u'300', u'max_seq_len': u'1', u'freeze_pretrained_embedding': u'true', u'pretrained_embedding_file': u''}]\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] output_layer is set to mean_squared_error. Ignoring the hyperparameter 'num_classes'.\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] Inconsistent enc_dim and token_embedding_dim found for token-embedding-encoder: 1024 vs 300. Setting token embedding dim to be 1024\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] Inconsistent enc_dim and token_embedding_dim found for token-embedding-encoder: 1024 vs 300. Setting token embedding dim to be 1024\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] Encoder configs: [{'vocab_size': 944, 'enc_index': 0, 'pretrained_embedding_file_path': None, 'vocab_dict': None, 'token_embedding_dim': 1024, 'vocab_file': '', 'freeze_pretrained_embedding': True, 'num_layers': 1, 'is_train': True, 'dropout': 0.0, 'pretrained_embedding_file': ''}, {'vocab_size': 1684, 'enc_index': 1, 'pretrained_embedding_file_path': None, 'vocab_dict': None, 'token_embedding_dim': 1024, 'vocab_file': '', 'freeze_pretrained_embedding': True, 'num_layers': 1, 'is_train': True, 'dropout': 0.0, 'pretrained_embedding_file': ''}]\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] Config: {'comparator_list': ['hadamard', 'concat', 'abs_diff'], 'epochs': 20, 'mini_batch_size': 64, 'optimizer': 'adam', 'output_layer': 'mean_squared_error', 'token_embedding_storage_type': 'dense', 'mlp_dim': 256, 'early_stopping_tolerance': 0.01, 'dropout': 0.0, 'bucket_width': 0, 'enc_dim': 1024, 'negative_sampling_rate': 0, 'early_stopping_patience': 3, 'learning_rate': 0.001, 'max_seq_lens': [1, 1], 'tied_token_embedding_weight': False, 'enc_configs': [{'vocab_size': 944, 'enc_index': 0, 'pretrained_embedding_file_path': None, 'vocab_dict': None, 'token_embedding_dim': 1024, 'vocab_file': '', 'freeze_pretrained_embedding': True, 'num_layers': 1, 'is_train': True, 'dropout': 0.0, 'pretrained_embedding_file': ''}, {'vocab_size': 1684, 'enc_index': 1, 'pretrained_embedding_file_path': None, 'vocab_dict': None, 'token_embedding_dim': 1024, 'vocab_file': '', 'freeze_pretrained_embedding': True, 'num_layers': 1, 'is_train': True, 'dropout': 0.0, 'pretrained_embedding_file': ''}], 'mlp_layers': 1, 'weight_decay': 0.0, 'mlp_activation': 'tanh', 'num_classes': None}\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] use bucketing: False\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] Creating data iterator for /opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] Source words: 9430\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] Target words: 9430\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] Total: 9430 samples in 1 buckets\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] Bucket of (1, 1) : 9430 samples in 147 batches of 64, approx 64.0 words/batch\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] 0 sentence pairs discarded\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] fill up mode: replicate\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] Negative sampling not used\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] Replicating 42 random sentences from bucket (1, 1) to size it to multiple of 64\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] Parameters of encoders: [{u'layers': u'auto', u'vocab_size': u'944', u'network': u'pooled_embedding', u'vocab_file': u'', u'cnn_filter_width': u'3', u'token_embedding_dim': u'300', u'max_seq_len': u'1', u'freeze_pretrained_embedding': u'true', u'pretrained_embedding_file': u''}, {u'layers': u'auto', u'vocab_size': u'1684', u'network': u'pooled_embedding', u'vocab_file': u'', u'cnn_filter_width': 3, u'token_embedding_dim': u'300', u'max_seq_len': u'1', u'freeze_pretrained_embedding': u'true', u'pretrained_embedding_file': u''}]\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] output_layer is set to mean_squared_error. Ignoring the hyperparameter 'num_classes'.\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] Inconsistent enc_dim and token_embedding_dim found for token-embedding-encoder: 1024 vs 300. Setting token embedding dim to be 1024\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] Inconsistent enc_dim and token_embedding_dim found for token-embedding-encoder: 1024 vs 300. Setting token embedding dim to be 1024\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] Encoder configs: [{'vocab_size': 944, 'enc_index': 0, 'pretrained_embedding_file_path': None, 'vocab_dict': None, 'token_embedding_dim': 1024, 'vocab_file': '', 'freeze_pretrained_embedding': True, 'num_layers': 1, 'is_train': True, 'dropout': 0.0, 'pretrained_embedding_file': ''}, {'vocab_size': 1684, 'enc_index': 1, 'pretrained_embedding_file_path': None, 'vocab_dict': None, 'token_embedding_dim': 1024, 'vocab_file': '', 'freeze_pretrained_embedding': True, 'num_layers': 1, 'is_train': True, 'dropout': 0.0, 'pretrained_embedding_file': ''}]\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] Config: {'comparator_list': ['hadamard', 'concat', 'abs_diff'], 'epochs': 20, 'mini_batch_size': 64, 'optimizer': 'adam', 'output_layer': 'mean_squared_error', 'token_embedding_storage_type': 'dense', 'mlp_dim': 256, 'early_stopping_tolerance': 0.01, 'dropout': 0.0, 'bucket_width': 0, 'enc_dim': 1024, 'negative_sampling_rate': 0, 'early_stopping_patience': 3, 'learning_rate': 0.001, 'max_seq_lens': [1, 1], 'tied_token_embedding_weight': False, 'enc_configs': [{'vocab_size': 944, 'enc_index': 0, 'pretrained_embedding_file_path': None, 'vocab_dict': None, 'token_embedding_dim': 1024, 'vocab_file': '', 'freeze_pretrained_embedding': True, 'num_layers': 1, 'is_train': True, 'dropout': 0.0, 'pretrained_embedding_file': ''}, {'vocab_size': 1684, 'enc_index': 1, 'pretrained_embedding_file_path': None, 'vocab_dict': None, 'token_embedding_dim': 1024, 'vocab_file': '', 'freeze_pretrained_embedding': True, 'num_layers': 1, 'is_train': True, 'dropout': 0.0, 'pretrained_embedding_file': ''}], 'mlp_layers': 1, 'weight_decay': 0.0, 'mlp_activation': 'tanh', 'num_classes': None}\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] Creating new state\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] params {u'comparator_list': u'hadamard, concat, abs_diff', u'enc0_vocab_file': u'', u'output_layer': u'mean_squared_error', u'enc0_cnn_filter_width': u'3', u'epochs': u'20', u'mlp_dim': u'256', u'enc0_freeze_pretrained_embedding': u'true', u'mlp_layers': u'1', u'_num_kv_servers': u'auto', u'weight_decay': 0, u'enc0_pretrained_embedding_file': u'', u'enc0_max_seq_len': u'1', u'token_embedding_storage_type': u'dense', u'enc0_token_embedding_dim': u'300', u'tied_token_embedding_weight': u'false', u'learning_rate': u'0.001', u'enc1_cnn_filter_width': 3, u'negative_sampling_rate': 0, u'enc0_network': u'pooled_embedding', u'enc1_layers': u'auto', u'early_stopping_patience': u'3', u'optimizer': u'adam', u'enc1_max_seq_len': u'1', u'_tuning_objective_metric': u'', u'early_stopping_tolerance': u'0.01', u'dropout': 0, u'bucket_width': u'0', u'enc_dim': u'1024', 'default_bucket_key': (1, 1), u'enc1_vocab_file': u'', u'enc1_freeze_pretrained_embedding': u'true', u'enc0_layers': u'auto', u'mini_batch_size': u'64', u'enc1_pretrained_embedding_file': u'', u'num_classes': u'2', u'_num_gpus': u'auto', u'enc1_token_embedding_dim': u'300', u'mlp_activation': u'tanh', u'enc1_network': u'pooled_embedding', u'_kvstore': u'device', u'enc1_vocab_size': u'1684', u'enc0_vocab_size': u'944'}\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] default_bucket_key (1, 1)\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] nvidia-smi took: 0.0251469612122 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] context [cpu(0)]\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] Create Store: device\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 WARNING 140615234824000] dense token embedding is used in a multi-gpu setting...consider changing 'token_embedding_storage_type' to 'row_sparse'\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] data_names: ['source', 'target']\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] label_names: ['out_layer_label']\u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mLayer (type)                                        Output Shape            Param #     Previous Layer                  \u001b[0m\n",
      "\u001b[34m========================================================================================================================\u001b[0m\n",
      "\u001b[34msource(null)                                        1                       0                                           \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34membed_0(Embedding)                                  1x1024                  0           source                          \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34m_not_equal_scalar0(_not_equal_scalar)               1                       0           source                          \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mreshape0(Reshape)                                   1x1                     0           _not_equal_scalar0              \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mbroadcast_mul0(broadcast_mul)                       1x1024                  0           embed_0                         \n",
      "                                                                                        reshape0                        \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34msum0(sum)                                           1024                    0           broadcast_mul0                  \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34msum1(sum)                                           1                       0           reshape0                        \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mzeros_like0(zeros_like)                             1                       0           sum1                            \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34m_equal0(_equal)                                     1                       0           sum1                            \n",
      "                                                                                        zeros_like0                     \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34m_plus0(elemwise_add)                                1                       0           sum1                            \n",
      "                                                                                        _equal0                         \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mbroadcast_div0(broadcast_div)                       1024                    0           sum0                            \n",
      "                                                                                        _plus0                          \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mdropout0(Dropout)                                   1024                    0           broadcast_div0                  \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34membed_1(Embedding)                                  1x1024                  0                                           \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34m_not_equal_scalar1(_not_equal_scalar)               1                       0                                           \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mreshape1(Reshape)                                   1x1                     0           _not_equal_scalar1              \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mbroadcast_mul1(broadcast_mul)                       1x1024                  0           embed_1                         \n",
      "                                                                                        reshape1                        \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34msum2(sum)                                           1024                    0           broadcast_mul1                  \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34msum3(sum)                                           1                       0           reshape1                        \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mzeros_like1(zeros_like)                             1                       0           sum3                            \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34m_equal1(_equal)                                     1                       0           sum3                            \n",
      "                                                                                        zeros_like1                     \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34m_plus1(elemwise_add)                                1                       0           sum3                            \n",
      "                                                                                        _equal1                         \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mbroadcast_div1(broadcast_div)                       1024                    0           sum2                            \n",
      "                                                                                        _plus1                          \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mdropout1(Dropout)                                   1024                    0           broadcast_div1                  \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34m_mul0(elemwise_mul)                                 1024                    0           dropout0                        \n",
      "                                                                                        dropout1                        \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34m_minus0(elemwise_sub)                               1024                    0           dropout0                        \n",
      "                                                                                        dropout1                        \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mabs0(abs)                                           1024                    0           _minus0                         \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mconcat0(Concat)                                     4096                    0           _mul0                           \n",
      "                                                                                        dropout0                        \n",
      "                                                                                        dropout1                        \n",
      "                                                                                        abs0                            \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mmlp_fc0(FullyConnected)                             256                     1048832     concat0                         \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mactivation0(Activation)                             256                     0           mlp_fc0                         \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mdropout2(Dropout)                                   256                     0           activation0                     \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34moutput_layer(FullyConnected)                        1                       257         dropout2                        \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mout_layer(LinearRegressionOutput)                   1                       0           output_layer                    \u001b[0m\n",
      "\u001b[34m========================================================================================================================\u001b[0m\n",
      "\u001b[34mTotal params: 1049089\u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] data_shapes [DataDesc[source,(64, 1),<type 'numpy.float32'>,NTC], DataDesc[target,(64, 1),<type 'numpy.float32'>,NTC]]\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] label_shapes [DataDesc[out_layer_label,(64,),<type 'numpy.float32'>,NTC]]\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] fixed_param_names: []\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] Initialized BucketingPlus Module\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] arg_params keys for module initialization: []\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:20 INFO 140615234824000] all params:['output_layer_weight', 'mlp_fc0_weight', 'mlp_fc0_bias', 'output_layer_bias', 'embed_1_weight', 'embed_0_weight']\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 101.16791725158691, \"sum\": 101.16791725158691, \"min\": 101.16791725158691}}, \"EndTime\": 1589854520.744969, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\"}, \"StartTime\": 1589854518.36253}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}}, \"EndTime\": 1589854520.745265, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\"}, \"StartTime\": 1589854520.745155}\n",
      "\u001b[0m\n",
      "\n",
      "2020-05-19 02:15:15 Training - Training image download completed. Training in progress.\u001b[34m[05/19/2020 02:15:25 INFO 140615234824000] Epoch: 0, batches: 100, num_examples: 6400, 1284.9 samples/sec, epoch time so far: 0:00:04.980997\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:25 INFO 140615234824000] #011Training metrics: mean_squared_error: 1.572 mean_absolute_error: 0.969 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:30 INFO 140615234824000] Epoch: 0, batches: 200, num_examples: 12800, 1292.6 samples/sec, epoch time so far: 0:00:09.902224\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:30 INFO 140615234824000] #011Training metrics: mean_squared_error: 1.330 mean_absolute_error: 0.901 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:35 INFO 140615234824000] Epoch: 0, batches: 300, num_examples: 19200, 1295.3 samples/sec, epoch time so far: 0:00:14.822858\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:35 INFO 140615234824000] #011Training metrics: mean_squared_error: 1.233 mean_absolute_error: 0.871 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:40 INFO 140615234824000] Epoch: 0, batches: 400, num_examples: 25600, 1292.0 samples/sec, epoch time so far: 0:00:19.813475\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:40 INFO 140615234824000] #011Training metrics: mean_squared_error: 1.180 mean_absolute_error: 0.854 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:45 INFO 140615234824000] Epoch: 0, batches: 500, num_examples: 32000, 1289.1 samples/sec, epoch time so far: 0:00:24.824081\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:45 INFO 140615234824000] #011Training metrics: mean_squared_error: 1.151 mean_absolute_error: 0.846 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:50 INFO 140615234824000] Epoch: 0, batches: 600, num_examples: 38400, 1289.4 samples/sec, epoch time so far: 0:00:29.780249\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:50 INFO 140615234824000] #011Training metrics: mean_squared_error: 1.127 mean_absolute_error: 0.838 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:55 INFO 140615234824000] Epoch: 0, batches: 700, num_examples: 44800, 1286.7 samples/sec, epoch time so far: 0:00:34.818415\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:15:55 INFO 140615234824000] #011Training metrics: mean_squared_error: 1.104 mean_absolute_error: 0.830 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:16:00 INFO 140615234824000] Epoch: 0, batches: 800, num_examples: 51200, 1281.4 samples/sec, epoch time so far: 0:00:39.957454\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:16:00 INFO 140615234824000] #011Training metrics: mean_squared_error: 1.086 mean_absolute_error: 0.823 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:16:06 INFO 140615234824000] Epoch: 0, batches: 900, num_examples: 57600, 1270.9 samples/sec, epoch time so far: 0:00:45.322723\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:16:06 INFO 140615234824000] #011Training metrics: mean_squared_error: 1.072 mean_absolute_error: 0.818 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:16:11 INFO 140615234824000] Epoch: 0, batches: 1000, num_examples: 64000, 1259.4 samples/sec, epoch time so far: 0:00:50.817366\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:16:11 INFO 140615234824000] #011Training metrics: mean_squared_error: 1.059 mean_absolute_error: 0.813 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:16:17 INFO 140615234824000] Epoch: 0, batches: 1100, num_examples: 70400, 1246.6 samples/sec, epoch time so far: 0:00:56.471539\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:16:17 INFO 140615234824000] #011Training metrics: mean_squared_error: 1.047 mean_absolute_error: 0.809 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:16:22 INFO 140615234824000] Epoch: 0, batches: 1200, num_examples: 76800, 1235.7 samples/sec, epoch time so far: 0:01:02.150266\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:16:22 INFO 140615234824000] #011Training metrics: mean_squared_error: 1.037 mean_absolute_error: 0.805 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:16:28 INFO 140615234824000] Epoch: 0, batches: 1300, num_examples: 83200, 1224.5 samples/sec, epoch time so far: 0:01:07.948828\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:16:28 INFO 140615234824000] #011Training metrics: mean_squared_error: 1.024 mean_absolute_error: 0.800 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:16:34 INFO 140615234824000] Epoch: 0, batches: 1400, num_examples: 89600, 1213.9 samples/sec, epoch time so far: 0:01:13.814682\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:16:34 INFO 140615234824000] #011Training metrics: mean_squared_error: 1.018 mean_absolute_error: 0.798 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:16:35 INFO 140615234824000] **************\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:16:35 INFO 140615234824000] Completed Epoch: 0, time taken: 0:01:14.760490\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:16:35 INFO 140615234824000] Epoch 0 Training metrics:   mean_squared_error: 1.017 mean_absolute_error: 0.797 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:16:35 INFO 140615234824000] #quality_metric: host=algo-1, epoch=0, train mean_squared_error <loss>=1.01667592334\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:16:36 INFO 140615234824000] Epoch 0 Validation metrics: mean_squared_error: 0.959 mean_absolute_error: 0.780 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:16:36 INFO 140615234824000] #quality_metric: host=algo-1, epoch=0, validation mean_squared_error <loss>=0.958805836133\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:16:36 INFO 140615234824000] **************\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}, \"early_stop.time\": {\"count\": 1, \"max\": 0.4169940948486328, \"sum\": 0.4169940948486328, \"min\": 0.4169940948486328}, \"update.time\": {\"count\": 1, \"max\": 75903.49388122559, \"sum\": 75903.49388122559, \"min\": 75903.49388122559}}, \"EndTime\": 1589854596.734513, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\"}, \"StartTime\": 1589854520.745099}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:16:36 INFO 140615234824000] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1416, \"sum\": 1416.0, \"min\": 1416}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1416, \"sum\": 1416.0, \"min\": 1416}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 90624, \"sum\": 90624.0, \"min\": 90624}, \"Total Batches Seen\": {\"count\": 1, \"max\": 1416, \"sum\": 1416.0, \"min\": 1416}, \"Total Records Seen\": {\"count\": 1, \"max\": 90624, \"sum\": 90624.0, \"min\": 90624}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 90624, \"sum\": 90624.0, \"min\": 90624}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1589854596.73545, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\", \"epoch\": 0}, \"StartTime\": 1589854520.830997}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:16:36 INFO 140615234824000] #throughput_metric: host=algo-1, train throughput=1193.9141049 records/second\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:16:43 INFO 140615234824000] Epoch: 1, batches: 100, num_examples: 6400, 1021.4 samples/sec, epoch time so far: 0:00:06.266143\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:16:43 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.709 mean_absolute_error: 0.662 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:16:49 INFO 140615234824000] Epoch: 1, batches: 200, num_examples: 12800, 1036.5 samples/sec, epoch time so far: 0:00:12.349789\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:16:49 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.713 mean_absolute_error: 0.664 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:16:55 INFO 140615234824000] Epoch: 1, batches: 300, num_examples: 19200, 1043.6 samples/sec, epoch time so far: 0:00:18.397348\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:16:55 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.710 mean_absolute_error: 0.663 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:17:01 INFO 140615234824000] Epoch: 1, batches: 400, num_examples: 25600, 1040.2 samples/sec, epoch time so far: 0:00:24.611206\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:17:01 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.702 mean_absolute_error: 0.659 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:17:07 INFO 140615234824000] Epoch: 1, batches: 500, num_examples: 32000, 1040.7 samples/sec, epoch time so far: 0:00:30.747592\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:17:07 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.699 mean_absolute_error: 0.657 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:17:13 INFO 140615234824000] Epoch: 1, batches: 600, num_examples: 38400, 1044.5 samples/sec, epoch time so far: 0:00:36.764306\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:17:13 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.688 mean_absolute_error: 0.652 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:17:19 INFO 140615234824000] Epoch: 1, batches: 700, num_examples: 44800, 1048.4 samples/sec, epoch time so far: 0:00:42.730590\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:17:19 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.680 mean_absolute_error: 0.648 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:17:25 INFO 140615234824000] Epoch: 1, batches: 800, num_examples: 51200, 1055.7 samples/sec, epoch time so far: 0:00:48.497896\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:17:25 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.677 mean_absolute_error: 0.647 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:17:31 INFO 140615234824000] Epoch: 1, batches: 900, num_examples: 57600, 1060.7 samples/sec, epoch time so far: 0:00:54.301950\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:17:31 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.672 mean_absolute_error: 0.644 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:17:36 INFO 140615234824000] Epoch: 1, batches: 1000, num_examples: 64000, 1066.5 samples/sec, epoch time so far: 0:01:00.009847\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:17:36 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.669 mean_absolute_error: 0.642 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:17:42 INFO 140615234824000] Epoch: 1, batches: 1100, num_examples: 70400, 1070.4 samples/sec, epoch time so far: 0:01:05.770120\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:17:42 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.664 mean_absolute_error: 0.640 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:17:48 INFO 140615234824000] Epoch: 1, batches: 1200, num_examples: 76800, 1070.1 samples/sec, epoch time so far: 0:01:11.770109\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:17:48 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.661 mean_absolute_error: 0.639 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:17:54 INFO 140615234824000] Epoch: 1, batches: 1300, num_examples: 83200, 1070.8 samples/sec, epoch time so far: 0:01:17.696546\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:17:54 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.659 mean_absolute_error: 0.638 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:18:00 INFO 140615234824000] Epoch: 1, batches: 1400, num_examples: 89600, 1072.0 samples/sec, epoch time so far: 0:01:23.579533\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:18:00 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.658 mean_absolute_error: 0.637 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:18:01 INFO 140615234824000] **************\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:18:01 INFO 140615234824000] Completed Epoch: 1, time taken: 0:01:24.505490\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:18:01 INFO 140615234824000] Epoch 1 Training metrics:   mean_squared_error: 0.658 mean_absolute_error: 0.637 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:18:01 INFO 140615234824000] #quality_metric: host=algo-1, epoch=1, train mean_squared_error <loss>=0.658084599396\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:18:02 INFO 140615234824000] Epoch 1 Validation metrics: mean_squared_error: 0.978 mean_absolute_error: 0.787 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:18:02 INFO 140615234824000] #quality_metric: host=algo-1, epoch=1, validation mean_squared_error <loss>=0.977615665342\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:18:02 INFO 140615234824000] **************\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"early_stop.time\": {\"count\": 1, \"max\": 0.02193450927734375, \"sum\": 0.02193450927734375, \"min\": 0.02193450927734375}, \"update.time\": {\"count\": 1, \"max\": 85494.37403678894, \"sum\": 85494.37403678894, \"min\": 85494.37403678894}}, \"EndTime\": 1589854682.283797, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\"}, \"StartTime\": 1589854596.735044}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:18:02 INFO 140615234824000] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1416, \"sum\": 1416.0, \"min\": 1416}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1416, \"sum\": 1416.0, \"min\": 1416}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 90624, \"sum\": 90624.0, \"min\": 90624}, \"Total Batches Seen\": {\"count\": 1, \"max\": 2832, \"sum\": 2832.0, \"min\": 2832}, \"Total Records Seen\": {\"count\": 1, \"max\": 181248, \"sum\": 181248.0, \"min\": 181248}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 90624, \"sum\": 90624.0, \"min\": 90624}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1589854682.284041, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\", \"epoch\": 1}, \"StartTime\": 1589854596.789404}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:18:02 INFO 140615234824000] #throughput_metric: host=algo-1, train throughput=1059.9945008 records/second\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:18:08 INFO 140615234824000] Epoch: 2, batches: 100, num_examples: 6400, 1055.0 samples/sec, epoch time so far: 0:00:06.066284\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:18:08 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.280 mean_absolute_error: 0.404 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:18:14 INFO 140615234824000] Epoch: 2, batches: 200, num_examples: 12800, 1063.6 samples/sec, epoch time so far: 0:00:12.034798\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:18:14 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.271 mean_absolute_error: 0.396 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:18:20 INFO 140615234824000] Epoch: 2, batches: 300, num_examples: 19200, 1061.7 samples/sec, epoch time so far: 0:00:18.084071\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:18:20 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.270 mean_absolute_error: 0.397 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:18:26 INFO 140615234824000] Epoch: 2, batches: 400, num_examples: 25600, 1064.6 samples/sec, epoch time so far: 0:00:24.046220\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:18:26 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.268 mean_absolute_error: 0.395 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:18:32 INFO 140615234824000] Epoch: 2, batches: 500, num_examples: 32000, 1071.5 samples/sec, epoch time so far: 0:00:29.864110\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:18:32 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.266 mean_absolute_error: 0.394 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:18:37 INFO 140615234824000] Epoch: 2, batches: 600, num_examples: 38400, 1076.7 samples/sec, epoch time so far: 0:00:35.663040\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:18:37 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.265 mean_absolute_error: 0.393 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:18:43 INFO 140615234824000] Epoch: 2, batches: 700, num_examples: 44800, 1080.5 samples/sec, epoch time so far: 0:00:41.462476\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:18:43 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.265 mean_absolute_error: 0.394 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:18:49 INFO 140615234824000] Epoch: 2, batches: 800, num_examples: 51200, 1080.8 samples/sec, epoch time so far: 0:00:47.371992\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:18:49 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.265 mean_absolute_error: 0.394 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:18:55 INFO 140615234824000] Epoch: 2, batches: 900, num_examples: 57600, 1081.7 samples/sec, epoch time so far: 0:00:53.249532\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:18:55 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.266 mean_absolute_error: 0.395 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:19:01 INFO 140615234824000] Epoch: 2, batches: 1000, num_examples: 64000, 1081.8 samples/sec, epoch time so far: 0:00:59.163076\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:19:01 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.267 mean_absolute_error: 0.396 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:19:07 INFO 140615234824000] Epoch: 2, batches: 1100, num_examples: 70400, 1081.6 samples/sec, epoch time so far: 0:01:05.086371\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:19:07 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.268 mean_absolute_error: 0.397 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:19:13 INFO 140615234824000] Epoch: 2, batches: 1200, num_examples: 76800, 1082.6 samples/sec, epoch time so far: 0:01:10.942136\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:19:13 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.270 mean_absolute_error: 0.398 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:19:19 INFO 140615234824000] Epoch: 2, batches: 1300, num_examples: 83200, 1082.6 samples/sec, epoch time so far: 0:01:16.853479\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:19:19 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.271 mean_absolute_error: 0.399 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:19:24 INFO 140615234824000] Epoch: 2, batches: 1400, num_examples: 89600, 1083.9 samples/sec, epoch time so far: 0:01:22.663523\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:19:24 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.272 mean_absolute_error: 0.400 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:19:25 INFO 140615234824000] **************\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:19:25 INFO 140615234824000] Completed Epoch: 2, time taken: 0:01:23.602069\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:19:25 INFO 140615234824000] Epoch 2 Training metrics:   mean_squared_error: 0.272 mean_absolute_error: 0.400 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:19:25 INFO 140615234824000] #quality_metric: host=algo-1, epoch=2, train mean_squared_error <loss>=0.272096234232\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:19:26 INFO 140615234824000] Epoch 2 Validation metrics: mean_squared_error: 0.938 mean_absolute_error: 0.756 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:19:26 INFO 140615234824000] #quality_metric: host=algo-1, epoch=2, validation mean_squared_error <loss>=0.938422757226\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:19:26 INFO 140615234824000] **************\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"early_stop.time\": {\"count\": 1, \"max\": 2.2590160369873047, \"sum\": 2.2590160369873047, \"min\": 2.2590160369873047}, \"update.time\": {\"count\": 1, \"max\": 84564.0799999237, \"sum\": 84564.0799999237, \"min\": 84564.0799999237}}, \"EndTime\": 1589854766.852266, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\"}, \"StartTime\": 1589854682.28387}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:19:26 INFO 140615234824000] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1416, \"sum\": 1416.0, \"min\": 1416}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1416, \"sum\": 1416.0, \"min\": 1416}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 90624, \"sum\": 90624.0, \"min\": 90624}, \"Total Batches Seen\": {\"count\": 1, \"max\": 4248, \"sum\": 4248.0, \"min\": 4248}, \"Total Records Seen\": {\"count\": 1, \"max\": 271872, \"sum\": 271872.0, \"min\": 271872}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 90624, \"sum\": 90624.0, \"min\": 90624}, \"Reset Count\": {\"count\": 1, \"max\": 3, \"sum\": 3.0, \"min\": 3}}, \"EndTime\": 1589854766.852542, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\", \"epoch\": 2}, \"StartTime\": 1589854682.288168}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:19:26 INFO 140615234824000] #throughput_metric: host=algo-1, train throughput=1071.6554652 records/second\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:19:32 INFO 140615234824000] Epoch: 3, batches: 100, num_examples: 6400, 1070.6 samples/sec, epoch time so far: 0:00:05.977834\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:19:32 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.166 mean_absolute_error: 0.299 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:19:38 INFO 140615234824000] Epoch: 3, batches: 200, num_examples: 12800, 1072.4 samples/sec, epoch time so far: 0:00:11.935804\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:19:38 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.162 mean_absolute_error: 0.296 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:19:44 INFO 140615234824000] Epoch: 3, batches: 300, num_examples: 19200, 1066.8 samples/sec, epoch time so far: 0:00:17.998446\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:19:44 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.159 mean_absolute_error: 0.294 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:19:50 INFO 140615234824000] Epoch: 3, batches: 400, num_examples: 25600, 1062.4 samples/sec, epoch time so far: 0:00:24.097281\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:19:50 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.158 mean_absolute_error: 0.293 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:19:56 INFO 140615234824000] Epoch: 3, batches: 500, num_examples: 32000, 1067.1 samples/sec, epoch time so far: 0:00:29.988109\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:19:56 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.157 mean_absolute_error: 0.291 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:20:02 INFO 140615234824000] Epoch: 3, batches: 600, num_examples: 38400, 1071.5 samples/sec, epoch time so far: 0:00:35.837778\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:20:02 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.156 mean_absolute_error: 0.292 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:20:08 INFO 140615234824000] Epoch: 3, batches: 700, num_examples: 44800, 1075.9 samples/sec, epoch time so far: 0:00:41.640467\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:20:08 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.155 mean_absolute_error: 0.291 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:20:14 INFO 140615234824000] Epoch: 3, batches: 800, num_examples: 51200, 1079.6 samples/sec, epoch time so far: 0:00:47.425053\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:20:14 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.156 mean_absolute_error: 0.292 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:20:20 INFO 140615234824000] Epoch: 3, batches: 900, num_examples: 57600, 1077.6 samples/sec, epoch time so far: 0:00:53.450232\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:20:20 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.157 mean_absolute_error: 0.293 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:20:26 INFO 140615234824000] Epoch: 3, batches: 1000, num_examples: 64000, 1078.7 samples/sec, epoch time so far: 0:00:59.331839\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:20:26 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.157 mean_absolute_error: 0.293 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:20:32 INFO 140615234824000] Epoch: 3, batches: 1100, num_examples: 70400, 1079.8 samples/sec, epoch time so far: 0:01:05.199275\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:20:32 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.158 mean_absolute_error: 0.294 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:20:38 INFO 140615234824000] Epoch: 3, batches: 1200, num_examples: 76800, 1080.0 samples/sec, epoch time so far: 0:01:11.113528\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:20:38 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.158 mean_absolute_error: 0.295 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:20:43 INFO 140615234824000] Epoch: 3, batches: 1300, num_examples: 83200, 1081.6 samples/sec, epoch time so far: 0:01:16.920119\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:20:43 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.158 mean_absolute_error: 0.295 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:20:49 INFO 140615234824000] Epoch: 3, batches: 1400, num_examples: 89600, 1082.8 samples/sec, epoch time so far: 0:01:22.749900\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:20:49 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.159 mean_absolute_error: 0.297 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:20:50 INFO 140615234824000] **************\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:20:50 INFO 140615234824000] Completed Epoch: 3, time taken: 0:01:23.672877\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:20:50 INFO 140615234824000] Epoch 3 Training metrics:   mean_squared_error: 0.159 mean_absolute_error: 0.296 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:20:50 INFO 140615234824000] #quality_metric: host=algo-1, epoch=3, train mean_squared_error <loss>=0.158694998735\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:20:51 INFO 140615234824000] Epoch 3 Validation metrics: mean_squared_error: 0.931 mean_absolute_error: 0.761 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:20:51 INFO 140615234824000] #quality_metric: host=algo-1, epoch=3, validation mean_squared_error <loss>=0.930597298451\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:20:51 INFO 140615234824000] **************\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:20:51 INFO 140615234824000] patience losses: [0.9588058361330548, 0.9776156653423567, 0.9384227572260676]\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:20:51 INFO 140615234824000] min patience losses: 0.938422757226\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:20:51 INFO 140615234824000] current loss: 0.930597298451\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:20:51 INFO 140615234824000] absolute loss difference: 0.0078254587747\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:20:51 INFO 140615234824000] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"early_stop.time\": {\"count\": 1, \"max\": 2.8738975524902344, \"sum\": 2.8738975524902344, \"min\": 2.8738975524902344}, \"update.time\": {\"count\": 1, \"max\": 84617.37990379333, \"sum\": 84617.37990379333, \"min\": 84617.37990379333}}, \"EndTime\": 1589854851.505559, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\"}, \"StartTime\": 1589854766.852361}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:20:51 INFO 140615234824000] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1416, \"sum\": 1416.0, \"min\": 1416}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1416, \"sum\": 1416.0, \"min\": 1416}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 90624, \"sum\": 90624.0, \"min\": 90624}, \"Total Batches Seen\": {\"count\": 1, \"max\": 5664, \"sum\": 5664.0, \"min\": 5664}, \"Total Records Seen\": {\"count\": 1, \"max\": 362496, \"sum\": 362496.0, \"min\": 362496}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 90624, \"sum\": 90624.0, \"min\": 90624}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1589854851.505911, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\", \"epoch\": 3}, \"StartTime\": 1589854766.888157}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:20:51 INFO 140615234824000] #throughput_metric: host=algo-1, train throughput=1070.97914858 records/second\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:20:57 INFO 140615234824000] Epoch: 4, batches: 100, num_examples: 6400, 1081.9 samples/sec, epoch time so far: 0:00:05.915701\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:20:57 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.121 mean_absolute_error: 0.260 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:21:03 INFO 140615234824000] Epoch: 4, batches: 200, num_examples: 12800, 1073.2 samples/sec, epoch time so far: 0:00:11.927223\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:21:03 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.115 mean_absolute_error: 0.254 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:21:09 INFO 140615234824000] Epoch: 4, batches: 300, num_examples: 19200, 1066.3 samples/sec, epoch time so far: 0:00:18.005415\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:21:09 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.111 mean_absolute_error: 0.251 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:21:15 INFO 140615234824000] Epoch: 4, batches: 400, num_examples: 25600, 1065.4 samples/sec, epoch time so far: 0:00:24.028097\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:21:15 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.111 mean_absolute_error: 0.250 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:21:21 INFO 140615234824000] Epoch: 4, batches: 500, num_examples: 32000, 1055.7 samples/sec, epoch time so far: 0:00:30.310689\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:21:21 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.110 mean_absolute_error: 0.249 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:21:27 INFO 140615234824000] Epoch: 4, batches: 600, num_examples: 38400, 1058.0 samples/sec, epoch time so far: 0:00:36.294457\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:21:27 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.109 mean_absolute_error: 0.249 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:21:33 INFO 140615234824000] Epoch: 4, batches: 700, num_examples: 44800, 1059.2 samples/sec, epoch time so far: 0:00:42.295036\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:21:33 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.108 mean_absolute_error: 0.248 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:21:39 INFO 140615234824000] Epoch: 4, batches: 800, num_examples: 51200, 1063.5 samples/sec, epoch time so far: 0:00:48.143248\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:21:39 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.108 mean_absolute_error: 0.248 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:21:45 INFO 140615234824000] Epoch: 4, batches: 900, num_examples: 57600, 1063.1 samples/sec, epoch time so far: 0:00:54.180062\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:21:45 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.108 mean_absolute_error: 0.248 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:21:51 INFO 140615234824000] Epoch: 4, batches: 1000, num_examples: 64000, 1063.7 samples/sec, epoch time so far: 0:01:00.164578\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:21:51 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.108 mean_absolute_error: 0.248 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:21:57 INFO 140615234824000] Epoch: 4, batches: 1100, num_examples: 70400, 1066.4 samples/sec, epoch time so far: 0:01:06.015978\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:21:57 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.108 mean_absolute_error: 0.249 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:22:03 INFO 140615234824000] Epoch: 4, batches: 1200, num_examples: 76800, 1068.7 samples/sec, epoch time so far: 0:01:11.862104\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:22:03 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.108 mean_absolute_error: 0.249 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:22:09 INFO 140615234824000] Epoch: 4, batches: 1300, num_examples: 83200, 1070.8 samples/sec, epoch time so far: 0:01:17.698568\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:22:09 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.108 mean_absolute_error: 0.249 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:22:15 INFO 140615234824000] Epoch: 4, batches: 1400, num_examples: 89600, 1072.8 samples/sec, epoch time so far: 0:01:23.519758\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:22:15 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.109 mean_absolute_error: 0.250 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:22:16 INFO 140615234824000] **************\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:22:16 INFO 140615234824000] Completed Epoch: 4, time taken: 0:01:24.487695\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:22:16 INFO 140615234824000] Epoch 4 Training metrics:   mean_squared_error: 0.109 mean_absolute_error: 0.250 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:22:16 INFO 140615234824000] #quality_metric: host=algo-1, epoch=4, train mean_squared_error <loss>=0.108872275421\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:22:17 INFO 140615234824000] Epoch 4 Validation metrics: mean_squared_error: 0.947 mean_absolute_error: 0.770 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:22:17 INFO 140615234824000] #quality_metric: host=algo-1, epoch=4, validation mean_squared_error <loss>=0.946558173041\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:22:17 INFO 140615234824000] **************\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:22:17 INFO 140615234824000] patience losses: [0.9776156653423567, 0.9384227572260676, 0.9305972984513721]\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:22:17 INFO 140615234824000] min patience losses: 0.930597298451\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:22:17 INFO 140615234824000] current loss: 0.946558173041\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:22:17 INFO 140615234824000] absolute loss difference: 0.0159608745897\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:22:17 INFO 140615234824000] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"early_stop.time\": {\"count\": 1, \"max\": 0.4839897155761719, \"sum\": 0.4839897155761719, \"min\": 0.4839897155761719}, \"update.time\": {\"count\": 1, \"max\": 85526.76486968994, \"sum\": 85526.76486968994, \"min\": 85526.76486968994}}, \"EndTime\": 1589854937.066796, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\"}, \"StartTime\": 1589854851.505642}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:22:17 INFO 140615234824000] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1416, \"sum\": 1416.0, \"min\": 1416}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1416, \"sum\": 1416.0, \"min\": 1416}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 90624, \"sum\": 90624.0, \"min\": 90624}, \"Total Batches Seen\": {\"count\": 1, \"max\": 7080, \"sum\": 7080.0, \"min\": 7080}, \"Total Records Seen\": {\"count\": 1, \"max\": 453120, \"sum\": 453120.0, \"min\": 453120}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 90624, \"sum\": 90624.0, \"min\": 90624}, \"Reset Count\": {\"count\": 1, \"max\": 5, \"sum\": 5.0, \"min\": 5}}, \"EndTime\": 1589854937.067036, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\", \"epoch\": 4}, \"StartTime\": 1589854851.540012}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:22:17 INFO 140615234824000] #throughput_metric: host=algo-1, train throughput=1059.59333166 records/second\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:22:23 INFO 140615234824000] Epoch: 5, batches: 100, num_examples: 6400, 1061.2 samples/sec, epoch time so far: 0:00:06.030756\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:22:23 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.092 mean_absolute_error: 0.234 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:22:29 INFO 140615234824000] Epoch: 5, batches: 200, num_examples: 12800, 1058.9 samples/sec, epoch time so far: 0:00:12.087953\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:22:29 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.091 mean_absolute_error: 0.231 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:22:35 INFO 140615234824000] Epoch: 5, batches: 300, num_examples: 19200, 1054.7 samples/sec, epoch time so far: 0:00:18.203873\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:22:35 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.092 mean_absolute_error: 0.233 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:22:41 INFO 140615234824000] Epoch: 5, batches: 400, num_examples: 25600, 1053.9 samples/sec, epoch time so far: 0:00:24.291191\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:22:41 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.091 mean_absolute_error: 0.232 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:22:47 INFO 140615234824000] Epoch: 5, batches: 500, num_examples: 32000, 1055.3 samples/sec, epoch time so far: 0:00:30.321742\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:22:47 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.091 mean_absolute_error: 0.232 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:22:53 INFO 140615234824000] Epoch: 5, batches: 600, num_examples: 38400, 1060.8 samples/sec, epoch time so far: 0:00:36.200019\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:22:53 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.090 mean_absolute_error: 0.232 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:22:58 INFO 140615234824000] Epoch: 5, batches: 700, num_examples: 44800, 1069.0 samples/sec, epoch time so far: 0:00:41.909615\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:22:58 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.090 mean_absolute_error: 0.231 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:23:04 INFO 140615234824000] Epoch: 5, batches: 800, num_examples: 51200, 1072.7 samples/sec, epoch time so far: 0:00:47.728313\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:23:04 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.090 mean_absolute_error: 0.231 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:23:10 INFO 140615234824000] Epoch: 5, batches: 900, num_examples: 57600, 1071.9 samples/sec, epoch time so far: 0:00:53.734519\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:23:10 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.089 mean_absolute_error: 0.230 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:23:16 INFO 140615234824000] Epoch: 5, batches: 1000, num_examples: 64000, 1070.5 samples/sec, epoch time so far: 0:00:59.785841\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:23:16 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.089 mean_absolute_error: 0.230 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:23:22 INFO 140615234824000] Epoch: 5, batches: 1100, num_examples: 70400, 1070.0 samples/sec, epoch time so far: 0:01:05.796337\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:23:22 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.089 mean_absolute_error: 0.230 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:23:28 INFO 140615234824000] Epoch: 5, batches: 1200, num_examples: 76800, 1070.6 samples/sec, epoch time so far: 0:01:11.732801\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:23:28 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.089 mean_absolute_error: 0.230 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:23:34 INFO 140615234824000] Epoch: 5, batches: 1300, num_examples: 83200, 1071.6 samples/sec, epoch time so far: 0:01:17.638067\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:23:34 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.089 mean_absolute_error: 0.230 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:23:40 INFO 140615234824000] Epoch: 5, batches: 1400, num_examples: 89600, 1072.7 samples/sec, epoch time so far: 0:01:23.530011\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:23:40 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.089 mean_absolute_error: 0.231 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:23:41 INFO 140615234824000] **************\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:23:41 INFO 140615234824000] Completed Epoch: 5, time taken: 0:01:24.476135\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:23:41 INFO 140615234824000] Epoch 5 Training metrics:   mean_squared_error: 0.089 mean_absolute_error: 0.231 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:23:41 INFO 140615234824000] #quality_metric: host=algo-1, epoch=5, train mean_squared_error <loss>=0.0891507722169\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:23:42 INFO 140615234824000] Epoch 5 Validation metrics: mean_squared_error: 0.929 mean_absolute_error: 0.765 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:23:42 INFO 140615234824000] #quality_metric: host=algo-1, epoch=5, validation mean_squared_error <loss>=0.92890310811\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:23:42 INFO 140615234824000] **************\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:23:42 INFO 140615234824000] patience losses: [0.9384227572260676, 0.9305972984513721, 0.9465581730410859]\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:23:42 INFO 140615234824000] min patience losses: 0.930597298451\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:23:42 INFO 140615234824000] current loss: 0.92890310811\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:23:42 INFO 140615234824000] absolute loss difference: 0.00169419034107\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:23:42 INFO 140615234824000] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"early_stop.time\": {\"count\": 1, \"max\": 2.2780895233154297, \"sum\": 2.2780895233154297, \"min\": 2.2780895233154297}, \"update.time\": {\"count\": 1, \"max\": 85544.36612129211, \"sum\": 85544.36612129211, \"min\": 85544.36612129211}}, \"EndTime\": 1589855022.615362, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\"}, \"StartTime\": 1589854937.066875}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:23:42 INFO 140615234824000] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1416, \"sum\": 1416.0, \"min\": 1416}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1416, \"sum\": 1416.0, \"min\": 1416}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 90624, \"sum\": 90624.0, \"min\": 90624}, \"Total Batches Seen\": {\"count\": 1, \"max\": 8496, \"sum\": 8496.0, \"min\": 8496}, \"Total Records Seen\": {\"count\": 1, \"max\": 543744, \"sum\": 543744.0, \"min\": 543744}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 90624, \"sum\": 90624.0, \"min\": 90624}, \"Reset Count\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}}, \"EndTime\": 1589855022.615593, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\", \"epoch\": 5}, \"StartTime\": 1589854937.070978}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:23:42 INFO 140615234824000] #throughput_metric: host=algo-1, train throughput=1059.37562811 records/second\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:23:48 INFO 140615234824000] Epoch: 6, batches: 100, num_examples: 6400, 1021.5 samples/sec, epoch time so far: 0:00:06.265438\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:23:48 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.087 mean_absolute_error: 0.229 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:23:54 INFO 140615234824000] Epoch: 6, batches: 200, num_examples: 12800, 1037.8 samples/sec, epoch time so far: 0:00:12.333305\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:23:54 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.086 mean_absolute_error: 0.227 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:24:00 INFO 140615234824000] Epoch: 6, batches: 300, num_examples: 19200, 1048.7 samples/sec, epoch time so far: 0:00:18.308422\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:24:00 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.085 mean_absolute_error: 0.226 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:24:06 INFO 140615234824000] Epoch: 6, batches: 400, num_examples: 25600, 1053.9 samples/sec, epoch time so far: 0:00:24.290884\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:24:06 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.084 mean_absolute_error: 0.224 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:24:12 INFO 140615234824000] Epoch: 6, batches: 500, num_examples: 32000, 1062.4 samples/sec, epoch time so far: 0:00:30.119068\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:24:12 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.083 mean_absolute_error: 0.223 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:24:18 INFO 140615234824000] Epoch: 6, batches: 600, num_examples: 38400, 1067.3 samples/sec, epoch time so far: 0:00:35.979446\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:24:18 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.082 mean_absolute_error: 0.222 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:24:24 INFO 140615234824000] Epoch: 6, batches: 700, num_examples: 44800, 1074.1 samples/sec, epoch time so far: 0:00:41.710601\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:24:24 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.082 mean_absolute_error: 0.222 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:24:30 INFO 140615234824000] Epoch: 6, batches: 800, num_examples: 51200, 1076.4 samples/sec, epoch time so far: 0:00:47.567453\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:24:30 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.082 mean_absolute_error: 0.221 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:24:36 INFO 140615234824000] Epoch: 6, batches: 900, num_examples: 57600, 1077.3 samples/sec, epoch time so far: 0:00:53.466745\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:24:36 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.082 mean_absolute_error: 0.221 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:24:42 INFO 140615234824000] Epoch: 6, batches: 1000, num_examples: 64000, 1075.8 samples/sec, epoch time so far: 0:00:59.489833\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:24:42 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.081 mean_absolute_error: 0.221 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:24:48 INFO 140615234824000] Epoch: 6, batches: 1100, num_examples: 70400, 1073.0 samples/sec, epoch time so far: 0:01:05.612251\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:24:48 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.081 mean_absolute_error: 0.221 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:24:54 INFO 140615234824000] Epoch: 6, batches: 1200, num_examples: 76800, 1072.3 samples/sec, epoch time so far: 0:01:11.624719\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:24:54 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.082 mean_absolute_error: 0.221 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:25:00 INFO 140615234824000] Epoch: 6, batches: 1300, num_examples: 83200, 1072.0 samples/sec, epoch time so far: 0:01:17.609367\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:25:00 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.082 mean_absolute_error: 0.222 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:25:06 INFO 140615234824000] Epoch: 6, batches: 1400, num_examples: 89600, 1072.1 samples/sec, epoch time so far: 0:01:23.570988\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:25:06 INFO 140615234824000] #011Training metrics: mean_squared_error: 0.082 mean_absolute_error: 0.222 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:25:07 INFO 140615234824000] **************\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:25:07 INFO 140615234824000] Completed Epoch: 6, time taken: 0:01:24.505223\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:25:07 INFO 140615234824000] Epoch 6 Training metrics:   mean_squared_error: 0.082 mean_absolute_error: 0.222 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:25:07 INFO 140615234824000] #quality_metric: host=algo-1, epoch=6, train mean_squared_error <loss>=0.0820897908692\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:25:08 INFO 140615234824000] Epoch 6 Validation metrics: mean_squared_error: 0.935 mean_absolute_error: 0.764 \u001b[0m\n",
      "\u001b[34m[05/19/2020 02:25:08 INFO 140615234824000] #quality_metric: host=algo-1, epoch=6, validation mean_squared_error <loss>=0.9348802784\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:25:08 INFO 140615234824000] **************\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:25:08 INFO 140615234824000] patience losses: [0.9305972984513721, 0.9465581730410859, 0.928903108110299]\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:25:08 INFO 140615234824000] min patience losses: 0.92890310811\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:25:08 INFO 140615234824000] current loss: 0.9348802784\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:25:08 INFO 140615234824000] absolute loss difference: 0.00597717029017\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:25:08 INFO 140615234824000] Bad epoch: loss has not improved (enough). Bad count:4\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:25:08 INFO 140615234824000] Bad epochs exceeded patience. Stopping training early!\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:25:08 INFO 140615234824000] Early stopping criterion met! Stopping training at epoch: 6\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"early_stop.time\": {\"count\": 1, \"max\": 0.5750656127929688, \"sum\": 0.5750656127929688, \"min\": 0.5750656127929688}, \"update.time\": {\"count\": 1, \"max\": 85453.07278633118, \"sum\": 85453.07278633118, \"min\": 85453.07278633118}}, \"EndTime\": 1589855108.114499, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\"}, \"StartTime\": 1589855022.615442}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:25:08 INFO 140615234824000] Early stop condition met. Stopping training.\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:25:08 INFO 140615234824000] #progress_metric: host=algo-1, completed 100 % epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 1416, \"sum\": 1416.0, \"min\": 1416}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 1416, \"sum\": 1416.0, \"min\": 1416}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 90624, \"sum\": 90624.0, \"min\": 90624}, \"Total Batches Seen\": {\"count\": 1, \"max\": 9912, \"sum\": 9912.0, \"min\": 9912}, \"Total Records Seen\": {\"count\": 1, \"max\": 634368, \"sum\": 634368.0, \"min\": 634368}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 90624, \"sum\": 90624.0, \"min\": 90624}, \"Reset Count\": {\"count\": 1, \"max\": 7, \"sum\": 7.0, \"min\": 7}}, \"EndTime\": 1589855108.114805, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\", \"epoch\": 6}, \"StartTime\": 1589855022.661405}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:25:08 INFO 140615234824000] #throughput_metric: host=algo-1, train throughput=1060.50601984 records/second\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:25:08 WARNING 140615234824000] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:25:08 INFO 140615234824000] Best model based on epoch 5. Best loss: 0.929\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 1.56402587890625, \"sum\": 1.56402587890625, \"min\": 1.56402587890625}}, \"EndTime\": 1589855108.116664, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\"}, \"StartTime\": 1589855108.11458}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:25:08 INFO 140615234824000] Serializing model to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:25:08 INFO 140615234824000] Saved checkpoint to \"/tmp/tmpA6KnW_/state-0001.params\"\u001b[0m\n",
      "\u001b[34m[05/19/2020 02:25:08 INFO 140615234824000] Test data is not provided.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 590068.0799484253, \"sum\": 590068.0799484253, \"min\": 590068.0799484253}, \"model.serialize.time\": {\"count\": 1, \"max\": 53.17211151123047, \"sum\": 53.17211151123047, \"min\": 53.17211151123047}, \"setuptime\": {\"count\": 1, \"max\": 224.86305236816406, \"sum\": 224.86305236816406, \"min\": 224.86305236816406}}, \"EndTime\": 1589855108.173813, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\"}, \"StartTime\": 1589855108.116734}\n",
      "\u001b[0m\n",
      "\n",
      "2020-05-19 02:25:25 Uploading - Uploading generated training model\n",
      "2020-05-19 02:25:25 Completed - Training job completed\n",
      "Training seconds: 680\n",
      "Billable seconds: 680\n"
     ]
    }
   ],
   "source": [
    "## get estimator\n",
    "regressor = sagemaker.estimator.Estimator(container,\n",
    "                                          role, \n",
    "                                          train_instance_count=1, \n",
    "                                          train_instance_type='ml.m5.large',\n",
    "                                          output_path=output_path,\n",
    "                                          sagemaker_session=sess)\n",
    "\n",
    "## set hyperparameters\n",
    "regressor.set_hyperparameters(**hyperparameters)\n",
    "\n",
    "## train the model\n",
    "regressor.fit(input_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that we can upload train (validation) data through the input data channel, and the algorithm will print out train (validation) evaluation metric during training. In addition, the algorithm uses the validation metric to perform early stopping. \n",
    "\n",
    "What if we want to send additional unlabeled data to the algorithm and get predictions from the trained model?\n",
    "This step is called *inference* in the Sagemaker framework. Next, we demonstrate how to use a trained model to perform inference on unseen data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference using trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "from sagemaker.predictor import json_serializer, json_deserializer\n",
    "\n",
    "# create a model using the trained algorithm\n",
    "regression_model = regressor.create_model(\n",
    "                        serializer=json_serializer,\n",
    "                        deserializer=json_deserializer,\n",
    "                        content_type='application/json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "# deploy the model\n",
    "predictor = regression_model.deploy(initial_instance_count=1, instance_type='ml.m4.large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we send validation data (without labels) to the deployed endpoint for inference. We will see that the resulting prediction error we get from post-training inference matches the best validation error from the training log in the console above (up to floating point error). If you follow the training instruction and parameter setup, you should get mean squared error on the validation set approximately 0.91."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean squared error on validation set is 0.930\n"
     ]
    }
   ],
   "source": [
    "# Send data to the endpoint to get predictions\n",
    "prediction = predictor.predict(valid_r_data)\n",
    "\n",
    "print(\"The mean squared error on validation set is %.3f\" %get_mse_loss(prediction, valid_r_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison against popular libraries\n",
    "\n",
    "Below we provide a chart that compares the performance of *Object2Vec* against several algorithms implemented by popular recommendation system libraries (LibRec https://www.librec.net/ and scikit-surprise http://surpriselib.com/). The error metric we use in the chart is **root mean squared** (RMSE) instead of MSE, so that our result can be compared against the reported results in the aforementioned libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ml-experiment-plot.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation task \n",
    "\n",
    "In this section, we showcase how to use *Object2Vec* to recommend movies, using the binarized rating labels. Here, if a movie rating label for a given user is binarized to `1`, then it means that the movie should be recommended to the user; otherwise, the label is binarized to `0`. The binarized data set is already obtained in the preprocessing section, so we will proceed to apply the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We upload the binarized datasets for classification task to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded data to s3://ml-demo-data-store-demos/object2vec/movielens/input/recommendation/train/train_c.jsonl\n",
      "Uploaded data to s3://ml-demo-data-store-demos/object2vec/movielens/input/recommendation/validation/validation_c.jsonl\n"
     ]
    }
   ],
   "source": [
    "for data_name in ['train', 'validation']:\n",
    "    fname = '{}_c.jsonl'.format(data_name)\n",
    "    pre_key = os.path.join(input_prefix, 'recommendation', f\"{data_name}\")\n",
    "    data_path = os.path.join('s3://', bucket, pre_key, fname)\n",
    "    s3_client.upload_file(fname, bucket, os.path.join(pre_key, fname))\n",
    "    input_paths[data_name] = s3_input(data_path, distribution='ShardedByS3Key', content_type='application/jsonlines')\n",
    "    print('Uploaded data to {}'.format(data_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we already get the algorithm image from the regression task, we can directly start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.session import s3_input\n",
    "\n",
    "hyperparameters_c = {\n",
    "    \"_kvstore\": \"device\",\n",
    "    \"_num_gpus\": \"auto\",\n",
    "    \"_num_kv_servers\": \"auto\",\n",
    "    \"bucket_width\": 0,\n",
    "    \"early_stopping_patience\": 3, \n",
    "    \"early_stopping_tolerance\": 0.01,\n",
    "    \"enc0_cnn_filter_width\": 3,\n",
    "    \"enc0_layers\": \"auto\",\n",
    "    \"enc0_max_seq_len\": 1,\n",
    "    \"enc0_network\": \"pooled_embedding\",\n",
    "    \"enc0_token_embedding_dim\": 300,\n",
    "    \"enc0_vocab_size\": 944,\n",
    "    \"enc1_cnn_filter_width\": 3,\n",
    "    \"enc1_layers\": \"auto\",\n",
    "    \"enc1_max_seq_len\": 1,\n",
    "    \"enc1_network\": \"pooled_embedding\",\n",
    "    \"enc1_token_embedding_dim\": 300,\n",
    "    \"enc1_vocab_size\": 1684,\n",
    "    \"enc_dim\": 2048,\n",
    "    \"epochs\": 20,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"mini_batch_size\": 2048,\n",
    "    \"mlp_activation\": \"relu\",\n",
    "    \"mlp_dim\": 1024,\n",
    "    \"mlp_layers\": 1,\n",
    "    \"num_classes\": 2,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"output_layer\": \"softmax\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-20 03:36:14 Starting - Starting the training job...\n",
      "2020-05-20 03:36:16 Starting - Launching requested ML instances......\n",
      "2020-05-20 03:37:18 Starting - Preparing the instances for training...\n",
      "2020-05-20 03:38:08 Downloading - Downloading input data...\n",
      "2020-05-20 03:38:22 Training - Downloading the training image...\n",
      "2020-05-20 03:39:03 Training - Training image download completed. Training in progress.\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:05 INFO 140279604574016] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/default-input.json: {u'comparator_list': u'hadamard, concat, abs_diff', u'enc0_vocab_file': u'', u'output_layer': u'softmax', u'enc0_cnn_filter_width': 3, u'epochs': 30, u'mlp_dim': 512, u'enc0_freeze_pretrained_embedding': u'true', u'mlp_layers': 2, u'_num_kv_servers': u'auto', u'weight_decay': 0, u'enc0_pretrained_embedding_file': u'', u'token_embedding_storage_type': u'dense', u'enc0_token_embedding_dim': 300, u'tied_token_embedding_weight': u'false', u'learning_rate': 0.0004, u'enc1_cnn_filter_width': 3, u'negative_sampling_rate': 0, u'enc0_network': u'hcnn', u'enc1_layers': u'auto', u'early_stopping_patience': 3, u'optimizer': u'adam', u'_tuning_objective_metric': u'', u'early_stopping_tolerance': 0.01, u'dropout': 0, u'bucket_width': 0, u'enc_dim': 4096, u'enc1_vocab_file': u'', u'enc1_freeze_pretrained_embedding': u'true', u'enc0_layers': u'auto', u'mini_batch_size': 32, u'enc1_pretrained_embedding_file': u'', u'num_classes': 2, u'_num_gpus': u'auto', u'enc1_token_embedding_dim': 300, u'mlp_activation': u'linear', u'enc1_network': u'enc0', u'_kvstore': u'auto_gpu'}\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:05 INFO 140279604574016] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'output_layer': u'softmax', u'enc0_cnn_filter_width': u'3', u'epochs': u'20', u'mlp_dim': u'1024', u'mlp_layers': u'1', u'_num_kv_servers': u'auto', u'mini_batch_size': u'2048', u'enc0_token_embedding_dim': u'300', u'enc1_network': u'pooled_embedding', u'early_stopping_tolerance': u'0.01', u'enc0_network': u'pooled_embedding', u'enc1_layers': u'auto', u'early_stopping_patience': u'3', u'optimizer': u'adam', u'enc1_max_seq_len': u'1', u'enc1_cnn_filter_width': u'3', u'learning_rate': u'0.001', u'bucket_width': u'0', u'enc_dim': u'2048', u'enc0_layers': u'auto', u'enc0_max_seq_len': u'1', u'num_classes': u'2', u'_num_gpus': u'auto', u'mlp_activation': u'relu', u'enc1_token_embedding_dim': u'300', u'_kvstore': u'device', u'enc1_vocab_size': u'1684', u'enc0_vocab_size': u'944'}\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:06 INFO 140279604574016] Final configuration: {u'comparator_list': u'hadamard, concat, abs_diff', u'enc0_vocab_file': u'', u'output_layer': u'softmax', u'enc0_cnn_filter_width': u'3', u'epochs': u'20', u'mlp_dim': u'1024', u'enc0_freeze_pretrained_embedding': u'true', u'mlp_layers': u'1', u'_num_kv_servers': u'auto', u'weight_decay': 0, u'enc0_pretrained_embedding_file': u'', u'enc0_max_seq_len': u'1', u'token_embedding_storage_type': u'dense', u'enc0_token_embedding_dim': u'300', u'tied_token_embedding_weight': u'false', u'learning_rate': u'0.001', u'enc1_cnn_filter_width': u'3', u'negative_sampling_rate': 0, u'enc0_network': u'pooled_embedding', u'enc1_layers': u'auto', u'early_stopping_patience': u'3', u'optimizer': u'adam', u'enc1_max_seq_len': u'1', u'_tuning_objective_metric': u'', u'early_stopping_tolerance': u'0.01', u'dropout': 0, u'bucket_width': u'0', u'enc_dim': u'2048', u'enc1_vocab_file': u'', u'enc1_freeze_pretrained_embedding': u'true', u'enc0_layers': u'auto', u'mini_batch_size': u'2048', u'enc1_pretrained_embedding_file': u'', u'num_classes': u'2', u'_num_gpus': u'auto', u'enc1_token_embedding_dim': u'300', u'mlp_activation': u'relu', u'enc1_network': u'pooled_embedding', u'_kvstore': u'device', u'enc1_vocab_size': u'1684', u'enc0_vocab_size': u'944'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:06 INFO 140279604574016] Using default worker.\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:06 INFO 140279604574016] Loaded iterator creator application/jsonlines for content type ('application/jsonlines', '1.0')\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:06 INFO 140279604574016] create_iter params {u'comparator_list': u'hadamard, concat, abs_diff', u'enc0_vocab_file': u'', u'output_layer': u'softmax', u'enc0_cnn_filter_width': u'3', u'epochs': u'20', u'mlp_dim': u'1024', u'enc0_freeze_pretrained_embedding': u'true', u'mlp_layers': u'1', u'_num_kv_servers': u'auto', u'weight_decay': 0, u'enc0_pretrained_embedding_file': u'', u'enc0_max_seq_len': u'1', u'token_embedding_storage_type': u'dense', u'enc0_token_embedding_dim': u'300', u'tied_token_embedding_weight': u'false', u'learning_rate': u'0.001', u'enc1_cnn_filter_width': u'3', u'negative_sampling_rate': 0, u'enc0_network': u'pooled_embedding', u'enc1_layers': u'auto', u'early_stopping_patience': u'3', u'optimizer': u'adam', u'enc1_max_seq_len': u'1', u'_tuning_objective_metric': u'', u'early_stopping_tolerance': u'0.01', u'dropout': 0, u'bucket_width': u'0', u'enc_dim': u'2048', u'enc1_vocab_file': u'', u'enc1_freeze_pretrained_embedding': u'true', u'enc0_layers': u'auto', u'mini_batch_size': u'2048', u'enc1_pretrained_embedding_file': u'', u'num_classes': u'2', u'_num_gpus': u'auto', u'enc1_token_embedding_dim': u'300', u'mlp_activation': u'relu', u'enc1_network': u'pooled_embedding', u'_kvstore': u'device', u'enc1_vocab_size': u'1684', u'enc0_vocab_size': u'944'}\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:06 INFO 140279604574016] create_iter content_params {}\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:06 INFO 140279604574016] Parameters of encoders: [{u'layers': u'auto', u'vocab_size': u'944', u'network': u'pooled_embedding', u'vocab_file': u'', u'cnn_filter_width': u'3', u'token_embedding_dim': u'300', u'max_seq_len': u'1', u'freeze_pretrained_embedding': u'true', u'pretrained_embedding_file': u''}, {u'layers': u'auto', u'vocab_size': u'1684', u'network': u'pooled_embedding', u'vocab_file': u'', u'cnn_filter_width': u'3', u'token_embedding_dim': u'300', u'max_seq_len': u'1', u'freeze_pretrained_embedding': u'true', u'pretrained_embedding_file': u''}]\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:06 INFO 140279604574016] Inconsistent enc_dim and token_embedding_dim found for token-embedding-encoder: 2048 vs 300. Setting token embedding dim to be 2048\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:06 INFO 140279604574016] Inconsistent enc_dim and token_embedding_dim found for token-embedding-encoder: 2048 vs 300. Setting token embedding dim to be 2048\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:06 INFO 140279604574016] Encoder configs: [{'vocab_size': 944, 'enc_index': 0, 'pretrained_embedding_file_path': None, 'vocab_dict': None, 'token_embedding_dim': 2048, 'vocab_file': '', 'freeze_pretrained_embedding': True, 'num_layers': 1, 'is_train': True, 'dropout': 0.0, 'pretrained_embedding_file': ''}, {'vocab_size': 1684, 'enc_index': 1, 'pretrained_embedding_file_path': None, 'vocab_dict': None, 'token_embedding_dim': 2048, 'vocab_file': '', 'freeze_pretrained_embedding': True, 'num_layers': 1, 'is_train': True, 'dropout': 0.0, 'pretrained_embedding_file': ''}]\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:06 INFO 140279604574016] Config: {'comparator_list': ['hadamard', 'concat', 'abs_diff'], 'epochs': 20, 'mini_batch_size': 2048, 'optimizer': 'adam', 'output_layer': 'softmax', 'token_embedding_storage_type': 'dense', 'mlp_dim': 1024, 'early_stopping_tolerance': 0.01, 'dropout': 0.0, 'bucket_width': 0, 'enc_dim': 2048, 'negative_sampling_rate': 0, 'early_stopping_patience': 3, 'learning_rate': 0.001, 'max_seq_lens': [1, 1], 'tied_token_embedding_weight': False, 'enc_configs': [{'vocab_size': 944, 'enc_index': 0, 'pretrained_embedding_file_path': None, 'vocab_dict': None, 'token_embedding_dim': 2048, 'vocab_file': '', 'freeze_pretrained_embedding': True, 'num_layers': 1, 'is_train': True, 'dropout': 0.0, 'pretrained_embedding_file': ''}, {'vocab_size': 1684, 'enc_index': 1, 'pretrained_embedding_file_path': None, 'vocab_dict': None, 'token_embedding_dim': 2048, 'vocab_file': '', 'freeze_pretrained_embedding': True, 'num_layers': 1, 'is_train': True, 'dropout': 0.0, 'pretrained_embedding_file': ''}], 'mlp_layers': 1, 'weight_decay': 0.0, 'mlp_activation': 'relu', 'num_classes': 2}\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:06 INFO 140279604574016] use bucketing: False\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:06 INFO 140279604574016] Creating data iterator for /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] Source words: 90570\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] Target words: 90570\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] Total: 90570 samples in 1 buckets\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] Bucket of (1, 1) : 90570 samples in 44 batches of 2048, approx 2048.0 words/batch\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] 0 sentence pairs discarded\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] fill up mode: replicate\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] \u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] Negative sampling not used\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] Replicating 1590 random sentences from bucket (1, 1) to size it to multiple of 2048\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] Bucket batch sizes: [BucketBatchSize(batch_size=2048, average_words_per_batch=2048)]\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] create_iter params {u'comparator_list': u'hadamard, concat, abs_diff', u'enc0_vocab_file': u'', u'output_layer': u'softmax', u'enc0_cnn_filter_width': u'3', u'epochs': u'20', u'mlp_dim': u'1024', u'enc0_freeze_pretrained_embedding': u'true', u'mlp_layers': u'1', u'_num_kv_servers': u'auto', u'weight_decay': 0, u'enc0_pretrained_embedding_file': u'', u'enc0_max_seq_len': u'1', u'token_embedding_storage_type': u'dense', u'enc0_token_embedding_dim': u'300', u'tied_token_embedding_weight': u'false', u'learning_rate': u'0.001', u'enc1_cnn_filter_width': u'3', u'negative_sampling_rate': 0, u'enc0_network': u'pooled_embedding', u'enc1_layers': u'auto', u'early_stopping_patience': u'3', u'optimizer': u'adam', u'enc1_max_seq_len': u'1', u'_tuning_objective_metric': u'', u'early_stopping_tolerance': u'0.01', u'dropout': 0, u'bucket_width': u'0', u'enc_dim': u'2048', u'enc1_vocab_file': u'', u'enc1_freeze_pretrained_embedding': u'true', u'enc0_layers': u'auto', u'mini_batch_size': u'2048', u'enc1_pretrained_embedding_file': u'', u'num_classes': u'2', u'_num_gpus': u'auto', u'enc1_token_embedding_dim': u'300', u'mlp_activation': u'relu', u'enc1_network': u'pooled_embedding', u'_kvstore': u'device', u'enc1_vocab_size': u'1684', u'enc0_vocab_size': u'944'}\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] create_iter content_params {}\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] Parameters of encoders: [{u'layers': u'auto', u'vocab_size': u'944', u'network': u'pooled_embedding', u'vocab_file': u'', u'cnn_filter_width': u'3', u'token_embedding_dim': u'300', u'max_seq_len': u'1', u'freeze_pretrained_embedding': u'true', u'pretrained_embedding_file': u''}, {u'layers': u'auto', u'vocab_size': u'1684', u'network': u'pooled_embedding', u'vocab_file': u'', u'cnn_filter_width': u'3', u'token_embedding_dim': u'300', u'max_seq_len': u'1', u'freeze_pretrained_embedding': u'true', u'pretrained_embedding_file': u''}]\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] Inconsistent enc_dim and token_embedding_dim found for token-embedding-encoder: 2048 vs 300. Setting token embedding dim to be 2048\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] Inconsistent enc_dim and token_embedding_dim found for token-embedding-encoder: 2048 vs 300. Setting token embedding dim to be 2048\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] Encoder configs: [{'vocab_size': 944, 'enc_index': 0, 'pretrained_embedding_file_path': None, 'vocab_dict': None, 'token_embedding_dim': 2048, 'vocab_file': '', 'freeze_pretrained_embedding': True, 'num_layers': 1, 'is_train': True, 'dropout': 0.0, 'pretrained_embedding_file': ''}, {'vocab_size': 1684, 'enc_index': 1, 'pretrained_embedding_file_path': None, 'vocab_dict': None, 'token_embedding_dim': 2048, 'vocab_file': '', 'freeze_pretrained_embedding': True, 'num_layers': 1, 'is_train': True, 'dropout': 0.0, 'pretrained_embedding_file': ''}]\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] Config: {'comparator_list': ['hadamard', 'concat', 'abs_diff'], 'epochs': 20, 'mini_batch_size': 2048, 'optimizer': 'adam', 'output_layer': 'softmax', 'token_embedding_storage_type': 'dense', 'mlp_dim': 1024, 'early_stopping_tolerance': 0.01, 'dropout': 0.0, 'bucket_width': 0, 'enc_dim': 2048, 'negative_sampling_rate': 0, 'early_stopping_patience': 3, 'learning_rate': 0.001, 'max_seq_lens': [1, 1], 'tied_token_embedding_weight': False, 'enc_configs': [{'vocab_size': 944, 'enc_index': 0, 'pretrained_embedding_file_path': None, 'vocab_dict': None, 'token_embedding_dim': 2048, 'vocab_file': '', 'freeze_pretrained_embedding': True, 'num_layers': 1, 'is_train': True, 'dropout': 0.0, 'pretrained_embedding_file': ''}, {'vocab_size': 1684, 'enc_index': 1, 'pretrained_embedding_file_path': None, 'vocab_dict': None, 'token_embedding_dim': 2048, 'vocab_file': '', 'freeze_pretrained_embedding': True, 'num_layers': 1, 'is_train': True, 'dropout': 0.0, 'pretrained_embedding_file': ''}], 'mlp_layers': 1, 'weight_decay': 0.0, 'mlp_activation': 'relu', 'num_classes': 2}\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] use bucketing: False\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] Creating data iterator for /opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] Source words: 9430\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] Target words: 9430\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] Total: 9430 samples in 1 buckets\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] Bucket of (1, 1) : 9430 samples in 4 batches of 2048, approx 2048.0 words/batch\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] 0 sentence pairs discarded\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] fill up mode: replicate\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] \u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] Negative sampling not used\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] Replicating 810 random sentences from bucket (1, 1) to size it to multiple of 2048\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] Parameters of encoders: [{u'layers': u'auto', u'vocab_size': u'944', u'network': u'pooled_embedding', u'vocab_file': u'', u'cnn_filter_width': u'3', u'token_embedding_dim': u'300', u'max_seq_len': u'1', u'freeze_pretrained_embedding': u'true', u'pretrained_embedding_file': u''}, {u'layers': u'auto', u'vocab_size': u'1684', u'network': u'pooled_embedding', u'vocab_file': u'', u'cnn_filter_width': u'3', u'token_embedding_dim': u'300', u'max_seq_len': u'1', u'freeze_pretrained_embedding': u'true', u'pretrained_embedding_file': u''}]\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] Inconsistent enc_dim and token_embedding_dim found for token-embedding-encoder: 2048 vs 300. Setting token embedding dim to be 2048\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] Inconsistent enc_dim and token_embedding_dim found for token-embedding-encoder: 2048 vs 300. Setting token embedding dim to be 2048\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] Encoder configs: [{'vocab_size': 944, 'enc_index': 0, 'pretrained_embedding_file_path': None, 'vocab_dict': None, 'token_embedding_dim': 2048, 'vocab_file': '', 'freeze_pretrained_embedding': True, 'num_layers': 1, 'is_train': True, 'dropout': 0.0, 'pretrained_embedding_file': ''}, {'vocab_size': 1684, 'enc_index': 1, 'pretrained_embedding_file_path': None, 'vocab_dict': None, 'token_embedding_dim': 2048, 'vocab_file': '', 'freeze_pretrained_embedding': True, 'num_layers': 1, 'is_train': True, 'dropout': 0.0, 'pretrained_embedding_file': ''}]\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] Config: {'comparator_list': ['hadamard', 'concat', 'abs_diff'], 'epochs': 20, 'mini_batch_size': 2048, 'optimizer': 'adam', 'output_layer': 'softmax', 'token_embedding_storage_type': 'dense', 'mlp_dim': 1024, 'early_stopping_tolerance': 0.01, 'dropout': 0.0, 'bucket_width': 0, 'enc_dim': 2048, 'negative_sampling_rate': 0, 'early_stopping_patience': 3, 'learning_rate': 0.001, 'max_seq_lens': [1, 1], 'tied_token_embedding_weight': False, 'enc_configs': [{'vocab_size': 944, 'enc_index': 0, 'pretrained_embedding_file_path': None, 'vocab_dict': None, 'token_embedding_dim': 2048, 'vocab_file': '', 'freeze_pretrained_embedding': True, 'num_layers': 1, 'is_train': True, 'dropout': 0.0, 'pretrained_embedding_file': ''}, {'vocab_size': 1684, 'enc_index': 1, 'pretrained_embedding_file_path': None, 'vocab_dict': None, 'token_embedding_dim': 2048, 'vocab_file': '', 'freeze_pretrained_embedding': True, 'num_layers': 1, 'is_train': True, 'dropout': 0.0, 'pretrained_embedding_file': ''}], 'mlp_layers': 1, 'weight_decay': 0.0, 'mlp_activation': 'relu', 'num_classes': 2}\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] Creating new state\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] params {u'comparator_list': u'hadamard, concat, abs_diff', u'enc0_vocab_file': u'', u'output_layer': u'softmax', u'enc0_cnn_filter_width': u'3', u'epochs': u'20', u'mlp_dim': u'1024', u'enc0_freeze_pretrained_embedding': u'true', u'mlp_layers': u'1', u'_num_kv_servers': u'auto', u'weight_decay': 0, u'enc0_pretrained_embedding_file': u'', u'enc0_max_seq_len': u'1', u'token_embedding_storage_type': u'dense', u'enc0_token_embedding_dim': u'300', u'tied_token_embedding_weight': u'false', u'learning_rate': u'0.001', u'enc1_cnn_filter_width': u'3', u'negative_sampling_rate': 0, u'enc0_network': u'pooled_embedding', u'enc1_layers': u'auto', u'early_stopping_patience': u'3', u'optimizer': u'adam', u'enc1_max_seq_len': u'1', u'_tuning_objective_metric': u'', u'early_stopping_tolerance': u'0.01', u'dropout': 0, u'bucket_width': u'0', u'enc_dim': u'2048', 'default_bucket_key': (1, 1), u'enc1_vocab_file': u'', u'enc1_freeze_pretrained_embedding': u'true', u'enc0_layers': u'auto', u'mini_batch_size': u'2048', u'enc1_pretrained_embedding_file': u'', u'num_classes': u'2', u'_num_gpus': u'auto', u'enc1_token_embedding_dim': u'300', u'mlp_activation': u'relu', u'enc1_network': u'pooled_embedding', u'_kvstore': u'device', u'enc1_vocab_size': u'1684', u'enc0_vocab_size': u'944'}\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] default_bucket_key (1, 1)\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] nvidia-smi took: 0.0251219272614 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] context [cpu(0)]\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] Create Store: device\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 WARNING 140279604574016] dense token embedding is used in a multi-gpu setting...consider changing 'token_embedding_storage_type' to 'row_sparse'\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] data_names: ['source', 'target']\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] label_names: ['out_layer_label']\u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mLayer (type)                                        Output Shape            Param #     Previous Layer                  \u001b[0m\n",
      "\u001b[34m========================================================================================================================\u001b[0m\n",
      "\u001b[34msource(null)                                        1                       0                                           \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34membed_0(Embedding)                                  1x2048                  0           source                          \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34m_not_equal_scalar0(_not_equal_scalar)               1                       0           source                          \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mreshape0(Reshape)                                   1x1                     0           _not_equal_scalar0              \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mbroadcast_mul0(broadcast_mul)                       1x2048                  0           embed_0                         \n",
      "                                                                                        reshape0                        \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34msum0(sum)                                           2048                    0           broadcast_mul0                  \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34msum1(sum)                                           1                       0           reshape0                        \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mzeros_like0(zeros_like)                             1                       0           sum1                            \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34m_equal0(_equal)                                     1                       0           sum1                            \n",
      "                                                                                        zeros_like0                     \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34m_plus0(elemwise_add)                                1                       0           sum1                            \n",
      "                                                                                        _equal0                         \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mbroadcast_div0(broadcast_div)                       2048                    0           sum0                            \n",
      "                                                                                        _plus0                          \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mdropout0(Dropout)                                   2048                    0           broadcast_div0                  \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34membed_1(Embedding)                                  1x2048                  0                                           \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34m_not_equal_scalar1(_not_equal_scalar)               1                       0                                           \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mreshape1(Reshape)                                   1x1                     0           _not_equal_scalar1              \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mbroadcast_mul1(broadcast_mul)                       1x2048                  0           embed_1                         \n",
      "                                                                                        reshape1                        \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34msum2(sum)                                           2048                    0           broadcast_mul1                  \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34msum3(sum)                                           1                       0           reshape1                        \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mzeros_like1(zeros_like)                             1                       0           sum3                            \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34m_equal1(_equal)                                     1                       0           sum3                            \n",
      "                                                                                        zeros_like1                     \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34m_plus1(elemwise_add)                                1                       0           sum3                            \n",
      "                                                                                        _equal1                         \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mbroadcast_div1(broadcast_div)                       2048                    0           sum2                            \n",
      "                                                                                        _plus1                          \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mdropout1(Dropout)                                   2048                    0           broadcast_div1                  \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34m_mul0(elemwise_mul)                                 2048                    0           dropout0                        \n",
      "                                                                                        dropout1                        \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34m_minus0(elemwise_sub)                               2048                    0           dropout0                        \n",
      "                                                                                        dropout1                        \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mabs0(abs)                                           2048                    0           _minus0                         \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mconcat0(Concat)                                     8192                    0           _mul0                           \n",
      "                                                                                        dropout0                        \n",
      "                                                                                        dropout1                        \n",
      "                                                                                        abs0                            \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mmlp_fc0(FullyConnected)                             1024                    8389632     concat0                         \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mactivation0(Activation)                             1024                    0           mlp_fc0                         \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mdropout2(Dropout)                                   1024                    0           activation0                     \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34moutput_layer(FullyConnected)                        2                       2050        dropout2                        \u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34mout_layer(SoftmaxOutput)                            2                       0           output_layer                    \u001b[0m\n",
      "\u001b[34m========================================================================================================================\u001b[0m\n",
      "\u001b[34mTotal params: 8391682\u001b[0m\n",
      "\u001b[34m________________________________________________________________________________________________________________________\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] data_shapes [DataDesc[source,(2048, 1),<type 'numpy.float32'>,NTC], DataDesc[target,(2048, 1),<type 'numpy.float32'>,NTC]]\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] label_shapes [DataDesc[out_layer_label,(2048,),<type 'numpy.float32'>,NTC]]\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] fixed_param_names: []\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] Initialized BucketingPlus Module\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] arg_params keys for module initialization: []\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:39:08 INFO 140279604574016] all params:['output_layer_weight', 'mlp_fc0_weight', 'mlp_fc0_bias', 'output_layer_bias', 'embed_1_weight', 'embed_0_weight']\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 58.48217010498047, \"sum\": 58.48217010498047, \"min\": 58.48217010498047}}, \"EndTime\": 1589945948.4095, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\"}, \"StartTime\": 1589945946.041476}\n",
      "\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}}, \"EndTime\": 1589945948.410017, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\"}, \"StartTime\": 1589945948.409958}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:40:24 INFO 140279604574016] **************\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:40:24 INFO 140279604574016] Completed Epoch: 0, time taken: 0:01:16.084684\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:40:24 INFO 140279604574016] Epoch 0 Training metrics:   perplexity: 1.848 cross_entropy: 0.614 accuracy: 0.660 \u001b[0m\n",
      "\u001b[34m[05/20/2020 03:40:24 INFO 140279604574016] #quality_metric: host=algo-1, epoch=0, train cross_entropy <loss>=0.61424517764\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:40:24 INFO 140279604574016] #quality_metric: host=algo-1, epoch=0, train accuracy <score>=0.66025390625\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:40:27 INFO 140279604574016] Epoch 0 Validation metrics: perplexity: 1.791 cross_entropy: 0.583 accuracy: 0.694 \u001b[0m\n",
      "\u001b[34m[05/20/2020 03:40:27 INFO 140279604574016] #quality_metric: host=algo-1, epoch=0, validation cross_entropy <loss>=0.582988667488\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:40:27 INFO 140279604574016] #quality_metric: host=algo-1, epoch=0, validation accuracy <score>=0.6935546875\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:40:27 INFO 140279604574016] **************\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 20, \"sum\": 20.0, \"min\": 20}, \"early_stop.time\": {\"count\": 1, \"max\": 0.3650188446044922, \"sum\": 0.3650188446044922, \"min\": 0.3650188446044922}, \"update.time\": {\"count\": 1, \"max\": 78655.15899658203, \"sum\": 78655.15899658203, \"min\": 78655.15899658203}}, \"EndTime\": 1589946027.346957, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\"}, \"StartTime\": 1589945948.409882}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:40:27 INFO 140279604574016] #progress_metric: host=algo-1, completed 5 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 45, \"sum\": 45.0, \"min\": 45}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 45, \"sum\": 45.0, \"min\": 45}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 92160, \"sum\": 92160.0, \"min\": 92160}, \"Total Batches Seen\": {\"count\": 1, \"max\": 45, \"sum\": 45.0, \"min\": 45}, \"Total Records Seen\": {\"count\": 1, \"max\": 92160, \"sum\": 92160.0, \"min\": 92160}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 92160, \"sum\": 92160.0, \"min\": 92160}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1589946027.347594, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\", \"epoch\": 0}, \"StartTime\": 1589945948.691774}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:40:27 INFO 140279604574016] #throughput_metric: host=algo-1, train throughput=1171.68184001 records/second\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:41:43 INFO 140279604574016] **************\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:41:43 INFO 140279604574016] Completed Epoch: 1, time taken: 0:01:16.062336\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:41:43 INFO 140279604574016] Epoch 1 Training metrics:   perplexity: 1.722 cross_entropy: 0.543 accuracy: 0.722 \u001b[0m\n",
      "\u001b[34m[05/20/2020 03:41:43 INFO 140279604574016] #quality_metric: host=algo-1, epoch=1, train cross_entropy <loss>=0.543368254768\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:41:43 INFO 140279604574016] #quality_metric: host=algo-1, epoch=1, train accuracy <score>=0.722352430556\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:41:46 INFO 140279604574016] Epoch 1 Validation metrics: perplexity: 1.774 cross_entropy: 0.573 accuracy: 0.700 \u001b[0m\n",
      "\u001b[34m[05/20/2020 03:41:46 INFO 140279604574016] #quality_metric: host=algo-1, epoch=1, validation cross_entropy <loss>=0.573460662365\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:41:46 INFO 140279604574016] #quality_metric: host=algo-1, epoch=1, validation accuracy <score>=0.7001953125\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:41:46 INFO 140279604574016] **************\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"early_stop.time\": {\"count\": 1, \"max\": 3.865957260131836, \"sum\": 3.865957260131836, \"min\": 3.865957260131836}, \"update.time\": {\"count\": 1, \"max\": 78775.54702758789, \"sum\": 78775.54702758789, \"min\": 78775.54702758789}}, \"EndTime\": 1589946106.376881, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\"}, \"StartTime\": 1589946027.347262}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:41:46 INFO 140279604574016] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 45, \"sum\": 45.0, \"min\": 45}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 45, \"sum\": 45.0, \"min\": 45}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 92160, \"sum\": 92160.0, \"min\": 92160}, \"Total Batches Seen\": {\"count\": 1, \"max\": 90, \"sum\": 90.0, \"min\": 90}, \"Total Records Seen\": {\"count\": 1, \"max\": 184320, \"sum\": 184320.0, \"min\": 184320}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 92160, \"sum\": 92160.0, \"min\": 92160}, \"Reset Count\": {\"count\": 1, \"max\": 2, \"sum\": 2.0, \"min\": 2}}, \"EndTime\": 1589946106.377743, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\", \"epoch\": 1}, \"StartTime\": 1589946027.601311}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:41:46 INFO 140279604574016] #throughput_metric: host=algo-1, train throughput=1169.88575536 records/second\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:43:02 INFO 140279604574016] **************\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:43:02 INFO 140279604574016] Completed Epoch: 2, time taken: 0:01:15.442165\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:43:02 INFO 140279604574016] Epoch 2 Training metrics:   perplexity: 1.651 cross_entropy: 0.501 accuracy: 0.753 \u001b[0m\n",
      "\u001b[34m[05/20/2020 03:43:02 INFO 140279604574016] #quality_metric: host=algo-1, epoch=2, train cross_entropy <loss>=0.501456522942\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:43:02 INFO 140279604574016] #quality_metric: host=algo-1, epoch=2, train accuracy <score>=0.753342013889\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:43:04 INFO 140279604574016] Epoch 2 Validation metrics: perplexity: 1.808 cross_entropy: 0.592 accuracy: 0.689 \u001b[0m\n",
      "\u001b[34m[05/20/2020 03:43:04 INFO 140279604574016] #quality_metric: host=algo-1, epoch=2, validation cross_entropy <loss>=0.592494726181\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:43:04 INFO 140279604574016] #quality_metric: host=algo-1, epoch=2, validation accuracy <score>=0.6890625\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:43:04 INFO 140279604574016] **************\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"early_stop.time\": {\"count\": 1, \"max\": 0.030040740966796875, \"sum\": 0.030040740966796875, \"min\": 0.030040740966796875}, \"update.time\": {\"count\": 1, \"max\": 77970.55101394653, \"sum\": 77970.55101394653, \"min\": 77970.55101394653}}, \"EndTime\": 1589946184.535455, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\"}, \"StartTime\": 1589946106.377351}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:43:04 INFO 140279604574016] #progress_metric: host=algo-1, completed 15 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 45, \"sum\": 45.0, \"min\": 45}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 45, \"sum\": 45.0, \"min\": 45}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 92160, \"sum\": 92160.0, \"min\": 92160}, \"Total Batches Seen\": {\"count\": 1, \"max\": 135, \"sum\": 135.0, \"min\": 135}, \"Total Records Seen\": {\"count\": 1, \"max\": 276480, \"sum\": 276480.0, \"min\": 276480}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 92160, \"sum\": 92160.0, \"min\": 92160}, \"Reset Count\": {\"count\": 1, \"max\": 3, \"sum\": 3.0, \"min\": 3}}, \"EndTime\": 1589946184.535767, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\", \"epoch\": 2}, \"StartTime\": 1589946106.564884}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:43:04 INFO 140279604574016] #throughput_metric: host=algo-1, train throughput=1181.97752158 records/second\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:44:19 INFO 140279604574016] **************\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:44:19 INFO 140279604574016] Completed Epoch: 3, time taken: 0:01:15.077572\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:44:19 INFO 140279604574016] Epoch 3 Training metrics:   perplexity: 1.384 cross_entropy: 0.325 accuracy: 0.867 \u001b[0m\n",
      "\u001b[34m[05/20/2020 03:44:19 INFO 140279604574016] #quality_metric: host=algo-1, epoch=3, train cross_entropy <loss>=0.325192125638\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:44:19 INFO 140279604574016] #quality_metric: host=algo-1, epoch=3, train accuracy <score>=0.86650390625\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:44:22 INFO 140279604574016] Epoch 3 Validation metrics: perplexity: 2.107 cross_entropy: 0.745 accuracy: 0.679 \u001b[0m\n",
      "\u001b[34m[05/20/2020 03:44:22 INFO 140279604574016] #quality_metric: host=algo-1, epoch=3, validation cross_entropy <loss>=0.745084154606\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:44:22 INFO 140279604574016] #quality_metric: host=algo-1, epoch=3, validation accuracy <score>=0.678515625\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:44:22 INFO 140279604574016] **************\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:44:22 INFO 140279604574016] patience losses: [0.5829886674880982, 0.5734606623649597, 0.5924947261810303]\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:44:22 INFO 140279604574016] min patience losses: 0.573460662365\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:44:22 INFO 140279604574016] current loss: 0.745084154606\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:44:22 INFO 140279604574016] absolute loss difference: 0.171623492241\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:44:22 INFO 140279604574016] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"early_stop.time\": {\"count\": 1, \"max\": 0.7100105285644531, \"sum\": 0.7100105285644531, \"min\": 0.7100105285644531}, \"update.time\": {\"count\": 1, \"max\": 77706.73418045044, \"sum\": 77706.73418045044, \"min\": 77706.73418045044}}, \"EndTime\": 1589946262.246138, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\"}, \"StartTime\": 1589946184.535526}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:44:22 INFO 140279604574016] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 45, \"sum\": 45.0, \"min\": 45}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 45, \"sum\": 45.0, \"min\": 45}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 92160, \"sum\": 92160.0, \"min\": 92160}, \"Total Batches Seen\": {\"count\": 1, \"max\": 180, \"sum\": 180.0, \"min\": 180}, \"Total Records Seen\": {\"count\": 1, \"max\": 368640, \"sum\": 368640.0, \"min\": 368640}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 92160, \"sum\": 92160.0, \"min\": 92160}, \"Reset Count\": {\"count\": 1, \"max\": 4, \"sum\": 4.0, \"min\": 4}}, \"EndTime\": 1589946262.246465, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\", \"epoch\": 3}, \"StartTime\": 1589946184.539389}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:44:22 INFO 140279604574016] #throughput_metric: host=algo-1, train throughput=1185.99080682 records/second\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:45:37 INFO 140279604574016] **************\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:45:37 INFO 140279604574016] Completed Epoch: 4, time taken: 0:01:15.480751\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:45:37 INFO 140279604574016] Epoch 4 Training metrics:   perplexity: 1.084 cross_entropy: 0.080 accuracy: 0.980 \u001b[0m\n",
      "\u001b[34m[05/20/2020 03:45:37 INFO 140279604574016] #quality_metric: host=algo-1, epoch=4, train cross_entropy <loss>=0.0803563637038\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:45:37 INFO 140279604574016] #quality_metric: host=algo-1, epoch=4, train accuracy <score>=0.980002170139\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:45:40 INFO 140279604574016] Epoch 4 Validation metrics: perplexity: 2.444 cross_entropy: 0.894 accuracy: 0.688 \u001b[0m\n",
      "\u001b[34m[05/20/2020 03:45:40 INFO 140279604574016] #quality_metric: host=algo-1, epoch=4, validation cross_entropy <loss>=0.893804359436\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:45:40 INFO 140279604574016] #quality_metric: host=algo-1, epoch=4, validation accuracy <score>=0.68828125\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:45:40 INFO 140279604574016] **************\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:45:40 INFO 140279604574016] patience losses: [0.5734606623649597, 0.5924947261810303, 0.7450841546058655]\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:45:40 INFO 140279604574016] min patience losses: 0.573460662365\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:45:40 INFO 140279604574016] current loss: 0.893804359436\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:45:40 INFO 140279604574016] absolute loss difference: 0.320343697071\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:45:40 INFO 140279604574016] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"early_stop.time\": {\"count\": 1, \"max\": 0.5710124969482422, \"sum\": 0.5710124969482422, \"min\": 0.5710124969482422}, \"update.time\": {\"count\": 1, \"max\": 77990.37003517151, \"sum\": 77990.37003517151, \"min\": 77990.37003517151}}, \"EndTime\": 1589946340.240443, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\"}, \"StartTime\": 1589946262.246201}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:45:40 INFO 140279604574016] #progress_metric: host=algo-1, completed 25 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 45, \"sum\": 45.0, \"min\": 45}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 45, \"sum\": 45.0, \"min\": 45}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 92160, \"sum\": 92160.0, \"min\": 92160}, \"Total Batches Seen\": {\"count\": 1, \"max\": 225, \"sum\": 225.0, \"min\": 225}, \"Total Records Seen\": {\"count\": 1, \"max\": 460800, \"sum\": 460800.0, \"min\": 460800}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 92160, \"sum\": 92160.0, \"min\": 92160}, \"Reset Count\": {\"count\": 1, \"max\": 5, \"sum\": 5.0, \"min\": 5}}, \"EndTime\": 1589946340.240715, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\", \"epoch\": 4}, \"StartTime\": 1589946262.250059}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:45:40 INFO 140279604574016] #throughput_metric: host=algo-1, train throughput=1181.67671693 records/second\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:46:55 INFO 140279604574016] **************\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:46:55 INFO 140279604574016] Completed Epoch: 5, time taken: 0:01:15.600524\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:46:55 INFO 140279604574016] Epoch 5 Training metrics:   perplexity: 1.014 cross_entropy: 0.013 accuracy: 0.999 \u001b[0m\n",
      "\u001b[34m[05/20/2020 03:46:55 INFO 140279604574016] #quality_metric: host=algo-1, epoch=5, train cross_entropy <loss>=0.013437590241\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:46:55 INFO 140279604574016] #quality_metric: host=algo-1, epoch=5, train accuracy <score>=0.999099392361\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:46:58 INFO 140279604574016] Epoch 5 Validation metrics: perplexity: 2.720 cross_entropy: 1.001 accuracy: 0.700 \u001b[0m\n",
      "\u001b[34m[05/20/2020 03:46:58 INFO 140279604574016] #quality_metric: host=algo-1, epoch=5, validation cross_entropy <loss>=1.00071872473\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:46:58 INFO 140279604574016] #quality_metric: host=algo-1, epoch=5, validation accuracy <score>=0.700390625\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:46:58 INFO 140279604574016] **************\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:46:58 INFO 140279604574016] patience losses: [0.5924947261810303, 0.7450841546058655, 0.8938043594360352]\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:46:58 INFO 140279604574016] min patience losses: 0.592494726181\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:46:58 INFO 140279604574016] current loss: 1.00071872473\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:46:58 INFO 140279604574016] absolute loss difference: 0.408223998547\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:46:58 INFO 140279604574016] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"early_stop.time\": {\"count\": 1, \"max\": 0.6899833679199219, \"sum\": 0.6899833679199219, \"min\": 0.6899833679199219}, \"update.time\": {\"count\": 1, \"max\": 78184.29112434387, \"sum\": 78184.29112434387, \"min\": 78184.29112434387}}, \"EndTime\": 1589946418.428675, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\"}, \"StartTime\": 1589946340.240505}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:46:58 INFO 140279604574016] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 45, \"sum\": 45.0, \"min\": 45}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 45, \"sum\": 45.0, \"min\": 45}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 92160, \"sum\": 92160.0, \"min\": 92160}, \"Total Batches Seen\": {\"count\": 1, \"max\": 270, \"sum\": 270.0, \"min\": 270}, \"Total Records Seen\": {\"count\": 1, \"max\": 552960, \"sum\": 552960.0, \"min\": 552960}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 92160, \"sum\": 92160.0, \"min\": 92160}, \"Reset Count\": {\"count\": 1, \"max\": 6, \"sum\": 6.0, \"min\": 6}}, \"EndTime\": 1589946418.429026, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\", \"epoch\": 5}, \"StartTime\": 1589946340.244371}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:46:58 INFO 140279604574016] #throughput_metric: host=algo-1, train throughput=1178.74640721 records/second\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:48:14 INFO 140279604574016] **************\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:48:14 INFO 140279604574016] Completed Epoch: 6, time taken: 0:01:15.795467\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:48:14 INFO 140279604574016] Epoch 6 Training metrics:   perplexity: 1.003 cross_entropy: 0.003 accuracy: 1.000 \u001b[0m\n",
      "\u001b[34m[05/20/2020 03:48:14 INFO 140279604574016] #quality_metric: host=algo-1, epoch=6, train cross_entropy <loss>=0.00322878143957\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:48:14 INFO 140279604574016] #quality_metric: host=algo-1, epoch=6, train accuracy <score>=0.999978298611\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:48:16 INFO 140279604574016] Epoch 6 Validation metrics: perplexity: 2.931 cross_entropy: 1.075 accuracy: 0.701 \u001b[0m\n",
      "\u001b[34m[05/20/2020 03:48:16 INFO 140279604574016] #quality_metric: host=algo-1, epoch=6, validation cross_entropy <loss>=1.07542462349\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:48:16 INFO 140279604574016] #quality_metric: host=algo-1, epoch=6, validation accuracy <score>=0.70068359375\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:48:16 INFO 140279604574016] **************\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:48:16 INFO 140279604574016] patience losses: [0.7450841546058655, 0.8938043594360352, 1.0007187247276306]\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:48:16 INFO 140279604574016] min patience losses: 0.745084154606\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:48:16 INFO 140279604574016] current loss: 1.07542462349\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:48:16 INFO 140279604574016] absolute loss difference: 0.330340468884\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:48:16 INFO 140279604574016] Bad epoch: loss has not improved (enough). Bad count:4\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:48:16 INFO 140279604574016] Bad epochs exceeded patience. Stopping training early!\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:48:16 INFO 140279604574016] Early stopping criterion met! Stopping training at epoch: 6\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"early_stop.time\": {\"count\": 1, \"max\": 0.6840229034423828, \"sum\": 0.6840229034423828, \"min\": 0.6840229034423828}, \"update.time\": {\"count\": 1, \"max\": 78356.7681312561, \"sum\": 78356.7681312561, \"min\": 78356.7681312561}}, \"EndTime\": 1589946496.789461, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\"}, \"StartTime\": 1589946418.428737}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:48:16 INFO 140279604574016] Early stop condition met. Stopping training.\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:48:16 INFO 140279604574016] #progress_metric: host=algo-1, completed 100 % epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 45, \"sum\": 45.0, \"min\": 45}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 45, \"sum\": 45.0, \"min\": 45}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 92160, \"sum\": 92160.0, \"min\": 92160}, \"Total Batches Seen\": {\"count\": 1, \"max\": 315, \"sum\": 315.0, \"min\": 315}, \"Total Records Seen\": {\"count\": 1, \"max\": 645120, \"sum\": 645120.0, \"min\": 645120}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 92160, \"sum\": 92160.0, \"min\": 92160}, \"Reset Count\": {\"count\": 1, \"max\": 7, \"sum\": 7.0, \"min\": 7}}, \"EndTime\": 1589946496.789748, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\", \"epoch\": 6}, \"StartTime\": 1589946418.432675}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:48:16 INFO 140279604574016] #throughput_metric: host=algo-1, train throughput=1176.15245323 records/second\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:48:16 WARNING 140279604574016] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:48:16 INFO 140279604574016] Best model based on epoch 1. Best loss: 0.573\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 1.291036605834961, \"sum\": 1.291036605834961, \"min\": 1.291036605834961}}, \"EndTime\": 1589946496.79134, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\"}, \"StartTime\": 1589946496.789533}\n",
      "\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:48:16 INFO 140279604574016] Serializing model to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:48:16 INFO 140279604574016] Saved checkpoint to \"/tmp/tmpOEUtc_/state-0001.params\"\u001b[0m\n",
      "\u001b[34m[05/20/2020 03:48:16 INFO 140279604574016] Test data is not provided.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 551240.1990890503, \"sum\": 551240.1990890503, \"min\": 551240.1990890503}, \"model.serialize.time\": {\"count\": 1, \"max\": 197.51214981079102, \"sum\": 197.51214981079102, \"min\": 197.51214981079102}, \"setuptime\": {\"count\": 1, \"max\": 238.95597457885742, \"sum\": 238.95597457885742, \"min\": 238.95597457885742}}, \"EndTime\": 1589946497.009107, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"ObjectToVec\"}, \"StartTime\": 1589946496.791594}\n",
      "\u001b[0m\n",
      "\n",
      "2020-05-20 03:49:21 Uploading - Uploading generated training model\n",
      "2020-05-20 03:49:21 Completed - Training job completed\n",
      "Training seconds: 673\n",
      "Billable seconds: 673\n"
     ]
    }
   ],
   "source": [
    "## get estimator\n",
    "classifier = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m5.large',\n",
    "                                    output_path=output_path,\n",
    "                                    sagemaker_session=sess)\n",
    "\n",
    "## set hyperparameters\n",
    "classifier.set_hyperparameters(**hyperparameters_c)\n",
    "\n",
    "## train, tune, and test the model\n",
    "classifier.fit(input_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can create, deploy, and validate the model after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------!"
     ]
    }
   ],
   "source": [
    "classification_model = classifier.create_model(\n",
    "                        serializer=json_serializer,\n",
    "                        deserializer=json_deserializer,\n",
    "                        content_type='application/json')\n",
    "\n",
    "predictor_2 = classification_model.deploy(initial_instance_count=1, instance_type='ml.t2.medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.predictor.RealTimePredictor at 0x7f8b76887278>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_c_data, valid_c_label = data_list_to_inference_format(copy.deepcopy(validation_data_list), \n",
    "                                                            label_thres=3, binarize=True)\n",
    "predictions = predictor_2.predict(valid_c_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the binarized validation set is 0.700\n"
     ]
    }
   ],
   "source": [
    "def get_class_accuracy(res, labels, thres):\n",
    "    if type(res) is dict:\n",
    "        res = res['predictions']\n",
    "    assert len(res)==len(labels), 'result and label length mismatch!'\n",
    "    accuracy = 0\n",
    "    for row, label in zip(res, labels):\n",
    "        if type(row) is dict:\n",
    "            if row['scores'][1] > thres:\n",
    "                prediction = 1\n",
    "            else: \n",
    "                prediction = 0\n",
    "            if label > thres:\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 0\n",
    "            accuracy += 1 - (prediction - label)**2\n",
    "    return accuracy / float(len(res))\n",
    "\n",
    "print(\"The accuracy on the binarized validation set is %.3f\" %get_class_accuracy(predictions, valid_c_label, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy on validation set you would get should be approximately 0.704."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie retrieval in the embedding space\n",
    "\n",
    "Since *Object2Vec* transforms user and movie ID's into embeddings as part of the training process. After training, it obtains user and movie embeddings in the left and right encoders, respectively. Intuitively, the embeddings should be tuned by the algorithm in a way that facilitates the supervised learning task: since for a specific user, similar movies should have similar ratings, we expect that similar movies should be **close-by** in the embedding space.\n",
    "\n",
    "In this section, we demonstrate how to find the nearest-neighbor (in Euclidean distance) of a given movie ID, among all movie ID's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_embedding_dict(movie_ids, trained_model):\n",
    "    input_instances = list()\n",
    "    for s_id in movie_ids:\n",
    "        input_instances.append({'in1': [s_id]})\n",
    "    data = {'instances': input_instances}\n",
    "    movie_embeddings = trained_model.predict(data)\n",
    "    embedding_dict = {}\n",
    "    for s_id, row in zip(movie_ids, movie_embeddings['predictions']):\n",
    "        embedding_dict[s_id] = np.array(row['embeddings'])\n",
    "    return embedding_dict\n",
    "\n",
    "\n",
    "def load_movie_id_name_map(item_file):\n",
    "    movieID_name_map = {}\n",
    "    with open(item_file, 'r', encoding=\"ISO-8859-1\") as f:\n",
    "        for row in f.readlines():\n",
    "            row = row.strip()\n",
    "            split = row.split('|')\n",
    "            movie_id = split[0]\n",
    "            movie_name = split[1]\n",
    "            sparse_tags = split[-19:]\n",
    "            movieID_name_map[int(movie_id)] = movie_name \n",
    "    return movieID_name_map\n",
    "\n",
    "            \n",
    "def get_nn_of_movie(movie_id, candidate_movie_ids, embedding_dict):\n",
    "    movie_emb = embedding_dict[movie_id]\n",
    "    min_dist = float('Inf')\n",
    "    best_id = candidate_movie_ids[0]\n",
    "    for idx, m_id in enumerate(candidate_movie_ids):\n",
    "        candidate_emb = embedding_dict[m_id]\n",
    "        curr_dist = np.linalg.norm(candidate_emb - movie_emb)\n",
    "        if curr_dist < min_dist:\n",
    "            best_id = m_id\n",
    "            min_dist = curr_dist\n",
    "    return best_id, min_dist\n",
    "\n",
    "\n",
    "def get_unique_movie_ids(data_list):\n",
    "    unique_movie_ids = set()\n",
    "    for row in data_list:\n",
    "        unique_movie_ids.add(row['in1'][0])\n",
    "    return list(unique_movie_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_list = load_csv_data(train_path, '\\t', verbose=False)\n",
    "unique_movie_ids = get_unique_movie_ids(train_data_list)\n",
    "embedding_dict = get_movie_embedding_dict(unique_movie_ids, predictor_2)\n",
    "candidate_movie_ids = unique_movie_ids.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the script below, you can check out what is the closest movie to any movie in the data set. Last time we ran it, the closest movie to `Terminator, The (1984)` in the embedding space was `Die Hard (1988)`. Note that, the result will likely differ slightly across different runs of the algorithm, due to randomness in initialization of model parameters.\n",
    "\n",
    "- Just plug in the movie id you want to examine \n",
    "   - For example, the movie ID for Terminator is 195; you can find the movie name and ID pair in the `u.item` file\n",
    "- Note that, the result will likely differ across different runs of the algorithm, due to inherent randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "movie_id_to_examine = 393 # Customize the movie ID you want to examine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The closest movie to Mrs. Doubtfire (1993) in the embedding space is Tomorrow Never Dies (1997)\n"
     ]
    }
   ],
   "source": [
    "candidate_movie_ids.remove(movie_id_to_examine)\n",
    "best_id, min_dist = get_nn_of_movie(movie_id_to_examine, candidate_movie_ids, embedding_dict)\n",
    "movieID_name_map = load_movie_id_name_map('ml-100k/u.item')\n",
    "print('The closest movie to {} in the embedding space is {}'.format(movieID_name_map[movie_id_to_examine],\n",
    "                                                                  movieID_name_map[best_id]))\n",
    "candidate_movie_ids.append(movie_id_to_examine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is recommended to always delete the endpoints used for hosting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clean up\n",
    "sess.delete_endpoint(predictor.endpoint)\n",
    "sess.delete_endpoint(predictor_2.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Give movie recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instances': ({'in0': [680], 'in1': [276]},\n",
       "  {'in0': [295], 'in1': [190]},\n",
       "  {'in0': [656], 'in1': [875]},\n",
       "  {'in0': [423], 'in1': [15]},\n",
       "  {'in0': [255], 'in1': [1034]},\n",
       "  {'in0': [801], 'in1': [333]},\n",
       "  {'in0': [24], 'in1': [191]},\n",
       "  {'in0': [87], 'in1': [40]},\n",
       "  {'in0': [14], 'in1': [709]},\n",
       "  {'in0': [850], 'in1': [202]},\n",
       "  {'in0': [111], 'in1': [269]},\n",
       "  {'in0': [239], 'in1': [1099]},\n",
       "  {'in0': [471], 'in1': [969]},\n",
       "  {'in0': [525], 'in1': [237]},\n",
       "  {'in0': [598], 'in1': [898]},\n",
       "  {'in0': [713], 'in1': [340]},\n",
       "  {'in0': [667], 'in1': [316]},\n",
       "  {'in0': [892], 'in1': [781]},\n",
       "  {'in0': [784], 'in1': [271]},\n",
       "  {'in0': [3], 'in1': [245]},\n",
       "  {'in0': [868], 'in1': [206]},\n",
       "  {'in0': [147], 'in1': [751]},\n",
       "  {'in0': [654], 'in1': [423]},\n",
       "  {'in0': [280], 'in1': [508]},\n",
       "  {'in0': [905], 'in1': [321]},\n",
       "  {'in0': [108], 'in1': [121]},\n",
       "  {'in0': [196], 'in1': [251]},\n",
       "  {'in0': [817], 'in1': [124]},\n",
       "  {'in0': [163], 'in1': [216]},\n",
       "  {'in0': [614], 'in1': [287]},\n",
       "  {'in0': [50], 'in1': [125]},\n",
       "  {'in0': [265], 'in1': [7]},\n",
       "  {'in0': [847], 'in1': [434]},\n",
       "  {'in0': [341], 'in1': [294]},\n",
       "  {'in0': [524], 'in1': [615]},\n",
       "  {'in0': [522], 'in1': [318]},\n",
       "  {'in0': [459], 'in1': [1060]},\n",
       "  {'in0': [820], 'in1': [271]},\n",
       "  {'in0': [721], 'in1': [87]},\n",
       "  {'in0': [816], 'in1': [355]},\n",
       "  {'in0': [24], 'in1': [64]},\n",
       "  {'in0': [126], 'in1': [905]},\n",
       "  {'in0': [40], 'in1': [880]},\n",
       "  {'in0': [613], 'in1': [1]},\n",
       "  {'in0': [787], 'in1': [333]},\n",
       "  {'in0': [578], 'in1': [294]},\n",
       "  {'in0': [168], 'in1': [1278]},\n",
       "  {'in0': [735], 'in1': [741]},\n",
       "  {'in0': [570], 'in1': [326]},\n",
       "  {'in0': [670], 'in1': [482]},\n",
       "  {'in0': [22], 'in1': [241]},\n",
       "  {'in0': [615], 'in1': [1192]},\n",
       "  {'in0': [713], 'in1': [1431]},\n",
       "  {'in0': [440], 'in1': [751]},\n",
       "  {'in0': [533], 'in1': [477]},\n",
       "  {'in0': [897], 'in1': [478]},\n",
       "  {'in0': [133], 'in1': [328]},\n",
       "  {'in0': [529], 'in1': [340]},\n",
       "  {'in0': [123], 'in1': [13]},\n",
       "  {'in0': [231], 'in1': [476]},\n",
       "  {'in0': [94], 'in1': [744]},\n",
       "  {'in0': [36], 'in1': [307]},\n",
       "  {'in0': [914], 'in1': [387]},\n",
       "  {'in0': [127], 'in1': [286]},\n",
       "  {'in0': [398], 'in1': [12]},\n",
       "  {'in0': [446], 'in1': [888]},\n",
       "  {'in0': [531], 'in1': [288]},\n",
       "  {'in0': [128], 'in1': [182]},\n",
       "  {'in0': [391], 'in1': [357]},\n",
       "  {'in0': [647], 'in1': [328]},\n",
       "  {'in0': [182], 'in1': [763]},\n",
       "  {'in0': [175], 'in1': [127]},\n",
       "  {'in0': [16], 'in1': [197]},\n",
       "  {'in0': [441], 'in1': [1]},\n",
       "  {'in0': [445], 'in1': [408]},\n",
       "  {'in0': [619], 'in1': [328]},\n",
       "  {'in0': [302], 'in1': [328]},\n",
       "  {'in0': [234], 'in1': [626]},\n",
       "  {'in0': [708], 'in1': [322]},\n",
       "  {'in0': [439], 'in1': [93]},\n",
       "  {'in0': [81], 'in1': [432]},\n",
       "  {'in0': [122], 'in1': [215]},\n",
       "  {'in0': [505], 'in1': [498]},\n",
       "  {'in0': [138], 'in1': [211]},\n",
       "  {'in0': [583], 'in1': [268]},\n",
       "  {'in0': [413], 'in1': [271]},\n",
       "  {'in0': [888], 'in1': [514]},\n",
       "  {'in0': [47], 'in1': [268]},\n",
       "  {'in0': [842], 'in1': [754]},\n",
       "  {'in0': [810], 'in1': [304]},\n",
       "  {'in0': [701], 'in1': [100]},\n",
       "  {'in0': [622], 'in1': [479]},\n",
       "  {'in0': [414], 'in1': [258]},\n",
       "  {'in0': [522], 'in1': [100]},\n",
       "  {'in0': [673], 'in1': [269]},\n",
       "  {'in0': [911], 'in1': [588]},\n",
       "  {'in0': [485], 'in1': [347]},\n",
       "  {'in0': [428], 'in1': [313]},\n",
       "  {'in0': [884], 'in1': [179]},\n",
       "  {'in0': [754], 'in1': [744]},\n",
       "  {'in0': [335], 'in1': [328]},\n",
       "  {'in0': [146], 'in1': [1022]},\n",
       "  {'in0': [500], 'in1': [244]},\n",
       "  {'in0': [431], 'in1': [300]},\n",
       "  {'in0': [331], 'in1': [479]},\n",
       "  {'in0': [405], 'in1': [1582]},\n",
       "  {'in0': [723], 'in1': [28]},\n",
       "  {'in0': [81], 'in1': [273]},\n",
       "  {'in0': [934], 'in1': [423]},\n",
       "  {'in0': [541], 'in1': [420]},\n",
       "  {'in0': [200], 'in1': [365]},\n",
       "  {'in0': [806], 'in1': [3]},\n",
       "  {'in0': [925], 'in1': [56]},\n",
       "  {'in0': [647], 'in1': [604]},\n",
       "  {'in0': [465], 'in1': [50]},\n",
       "  {'in0': [367], 'in1': [436]},\n",
       "  {'in0': [325], 'in1': [179]},\n",
       "  {'in0': [415], 'in1': [754]},\n",
       "  {'in0': [263], 'in1': [419]},\n",
       "  {'in0': [118], 'in1': [324]},\n",
       "  {'in0': [95], 'in1': [787]},\n",
       "  {'in0': [221], 'in1': [1250]},\n",
       "  {'in0': [155], 'in1': [325]},\n",
       "  {'in0': [921], 'in1': [24]},\n",
       "  {'in0': [122], 'in1': [509]},\n",
       "  {'in0': [152], 'in1': [685]},\n",
       "  {'in0': [469], 'in1': [513]},\n",
       "  {'in0': [422], 'in1': [126]},\n",
       "  {'in0': [650], 'in1': [630]},\n",
       "  {'in0': [7], 'in1': [492]},\n",
       "  {'in0': [334], 'in1': [635]},\n",
       "  {'in0': [706], 'in1': [100]},\n",
       "  {'in0': [267], 'in1': [403]},\n",
       "  {'in0': [195], 'in1': [300]},\n",
       "  {'in0': [509], 'in1': [260]},\n",
       "  {'in0': [619], 'in1': [403]},\n",
       "  {'in0': [903], 'in1': [746]},\n",
       "  {'in0': [366], 'in1': [164]},\n",
       "  {'in0': [151], 'in1': [929]},\n",
       "  {'in0': [559], 'in1': [73]},\n",
       "  {'in0': [63], 'in1': [100]},\n",
       "  {'in0': [199], 'in1': [221]},\n",
       "  {'in0': [814], 'in1': [200]},\n",
       "  {'in0': [589], 'in1': [340]},\n",
       "  {'in0': [424], 'in1': [275]},\n",
       "  {'in0': [871], 'in1': [174]},\n",
       "  {'in0': [847], 'in1': [405]},\n",
       "  {'in0': [394], 'in1': [161]},\n",
       "  {'in0': [891], 'in1': [278]},\n",
       "  {'in0': [313], 'in1': [117]},\n",
       "  {'in0': [400], 'in1': [749]},\n",
       "  {'in0': [902], 'in1': [295]},\n",
       "  {'in0': [556], 'in1': [327]},\n",
       "  {'in0': [803], 'in1': [243]},\n",
       "  {'in0': [890], 'in1': [324]},\n",
       "  {'in0': [727], 'in1': [125]},\n",
       "  {'in0': [846], 'in1': [627]},\n",
       "  {'in0': [766], 'in1': [82]},\n",
       "  {'in0': [399], 'in1': [813]},\n",
       "  {'in0': [645], 'in1': [69]},\n",
       "  {'in0': [47], 'in1': [323]},\n",
       "  {'in0': [239], 'in1': [491]},\n",
       "  {'in0': [312], 'in1': [494]},\n",
       "  {'in0': [53], 'in1': [281]},\n",
       "  {'in0': [204], 'in1': [191]},\n",
       "  {'in0': [459], 'in1': [134]},\n",
       "  {'in0': [79], 'in1': [937]},\n",
       "  {'in0': [197], 'in1': [300]},\n",
       "  {'in0': [937], 'in1': [242]},\n",
       "  {'in0': [324], 'in1': [283]},\n",
       "  {'in0': [329], 'in1': [129]},\n",
       "  {'in0': [276], 'in1': [1091]},\n",
       "  {'in0': [555], 'in1': [1054]},\n",
       "  {'in0': [886], 'in1': [175]},\n",
       "  {'in0': [659], 'in1': [96]},\n",
       "  {'in0': [883], 'in1': [792]},\n",
       "  {'in0': [601], 'in1': [176]},\n",
       "  {'in0': [512], 'in1': [97]},\n",
       "  {'in0': [123], 'in1': [606]},\n",
       "  {'in0': [584], 'in1': [40]},\n",
       "  {'in0': [88], 'in1': [750]},\n",
       "  {'in0': [112], 'in1': [354]},\n",
       "  {'in0': [728], 'in1': [678]},\n",
       "  {'in0': [843], 'in1': [448]},\n",
       "  {'in0': [89], 'in1': [936]},\n",
       "  {'in0': [521], 'in1': [288]},\n",
       "  {'in0': [879], 'in1': [1]},\n",
       "  {'in0': [342], 'in1': [288]},\n",
       "  {'in0': [915], 'in1': [313]},\n",
       "  {'in0': [195], 'in1': [496]},\n",
       "  {'in0': [262], 'in1': [1147]},\n",
       "  {'in0': [365], 'in1': [1017]},\n",
       "  {'in0': [46], 'in1': [328]},\n",
       "  {'in0': [804], 'in1': [399]},\n",
       "  {'in0': [98], 'in1': [745]},\n",
       "  {'in0': [211], 'in1': [300]},\n",
       "  {'in0': [187], 'in1': [70]},\n",
       "  {'in0': [234], 'in1': [448]},\n",
       "  {'in0': [268], 'in1': [145]},\n",
       "  {'in0': [314], 'in1': [535]},\n",
       "  {'in0': [431], 'in1': [754]},\n",
       "  {'in0': [466], 'in1': [331]},\n",
       "  {'in0': [87], 'in1': [210]},\n",
       "  {'in0': [675], 'in1': [1101]},\n",
       "  {'in0': [38], 'in1': [139]},\n",
       "  {'in0': [67], 'in1': [147]},\n",
       "  {'in0': [699], 'in1': [983]},\n",
       "  {'in0': [381], 'in1': [191]},\n",
       "  {'in0': [901], 'in1': [509]},\n",
       "  {'in0': [612], 'in1': [118]},\n",
       "  {'in0': [218], 'in1': [273]},\n",
       "  {'in0': [147], 'in1': [340]},\n",
       "  {'in0': [545], 'in1': [710]},\n",
       "  {'in0': [335], 'in1': [288]},\n",
       "  {'in0': [598], 'in1': [350]},\n",
       "  {'in0': [605], 'in1': [137]},\n",
       "  {'in0': [27], 'in1': [596]},\n",
       "  {'in0': [455], 'in1': [629]},\n",
       "  {'in0': [659], 'in1': [272]},\n",
       "  {'in0': [297], 'in1': [133]},\n",
       "  {'in0': [276], 'in1': [322]},\n",
       "  {'in0': [291], 'in1': [144]},\n",
       "  {'in0': [425], 'in1': [424]},\n",
       "  {'in0': [222], 'in1': [118]},\n",
       "  {'in0': [87], 'in1': [384]},\n",
       "  {'in0': [248], 'in1': [174]},\n",
       "  {'in0': [303], 'in1': [134]},\n",
       "  {'in0': [95], 'in1': [625]},\n",
       "  {'in0': [562], 'in1': [114]},\n",
       "  {'in0': [180], 'in1': [68]},\n",
       "  {'in0': [935], 'in1': [476]},\n",
       "  {'in0': [161], 'in1': [98]},\n",
       "  {'in0': [310], 'in1': [748]},\n",
       "  {'in0': [937], 'in1': [224]},\n",
       "  {'in0': [863], 'in1': [1062]},\n",
       "  {'in0': [867], 'in1': [96]},\n",
       "  {'in0': [891], 'in1': [280]},\n",
       "  {'in0': [934], 'in1': [202]},\n",
       "  {'in0': [138], 'in1': [484]},\n",
       "  {'in0': [415], 'in1': [479]},\n",
       "  {'in0': [821], 'in1': [181]},\n",
       "  {'in0': [34], 'in1': [332]},\n",
       "  {'in0': [11], 'in1': [425]},\n",
       "  {'in0': [695], 'in1': [301]},\n",
       "  {'in0': [281], 'in1': [877]},\n",
       "  {'in0': [308], 'in1': [295]},\n",
       "  {'in0': [788], 'in1': [568]},\n",
       "  {'in0': [467], 'in1': [240]},\n",
       "  {'in0': [474], 'in1': [736]},\n",
       "  {'in0': [397], 'in1': [194]},\n",
       "  {'in0': [155], 'in1': [323]},\n",
       "  {'in0': [88], 'in1': [302]},\n",
       "  {'in0': [46], 'in1': [181]},\n",
       "  {'in0': [37], 'in1': [403]},\n",
       "  {'in0': [681], 'in1': [690]},\n",
       "  {'in0': [908], 'in1': [173]},\n",
       "  {'in0': [179], 'in1': [258]},\n",
       "  {'in0': [630], 'in1': [278]},\n",
       "  {'in0': [842], 'in1': [313]},\n",
       "  {'in0': [121], 'in1': [180]},\n",
       "  {'in0': [771], 'in1': [237]},\n",
       "  {'in0': [648], 'in1': [473]},\n",
       "  {'in0': [30], 'in1': [82]},\n",
       "  {'in0': [75], 'in1': [1017]},\n",
       "  {'in0': [761], 'in1': [289]},\n",
       "  {'in0': [466], 'in1': [354]},\n",
       "  {'in0': [359], 'in1': [270]},\n",
       "  {'in0': [190], 'in1': [302]},\n",
       "  {'in0': [851], 'in1': [27]},\n",
       "  {'in0': [900], 'in1': [589]},\n",
       "  {'in0': [18], 'in1': [26]},\n",
       "  {'in0': [375], 'in1': [566]},\n",
       "  {'in0': [910], 'in1': [237]},\n",
       "  {'in0': [406], 'in1': [962]},\n",
       "  {'in0': [631], 'in1': [338]},\n",
       "  {'in0': [770], 'in1': [301]},\n",
       "  {'in0': [344], 'in1': [306]},\n",
       "  {'in0': [667], 'in1': [285]},\n",
       "  {'in0': [312], 'in1': [573]},\n",
       "  {'in0': [280], 'in1': [631]},\n",
       "  {'in0': [838], 'in1': [748]},\n",
       "  {'in0': [630], 'in1': [127]},\n",
       "  {'in0': [387], 'in1': [180]},\n",
       "  {'in0': [187], 'in1': [523]},\n",
       "  {'in0': [424], 'in1': [989]},\n",
       "  {'in0': [726], 'in1': [845]},\n",
       "  {'in0': [9], 'in1': [286]},\n",
       "  {'in0': [940], 'in1': [66]},\n",
       "  {'in0': [102], 'in1': [515]},\n",
       "  {'in0': [154], 'in1': [488]},\n",
       "  {'in0': [320], 'in1': [117]},\n",
       "  {'in0': [196], 'in1': [306]},\n",
       "  {'in0': [73], 'in1': [480]},\n",
       "  {'in0': [702], 'in1': [259]},\n",
       "  {'in0': [854], 'in1': [122]},\n",
       "  {'in0': [717], 'in1': [748]},\n",
       "  {'in0': [515], 'in1': [362]},\n",
       "  {'in0': [585], 'in1': [1347]},\n",
       "  {'in0': [812], 'in1': [1393]},\n",
       "  {'in0': [863], 'in1': [264]},\n",
       "  {'in0': [447], 'in1': [544]},\n",
       "  {'in0': [238], 'in1': [220]},\n",
       "  {'in0': [224], 'in1': [26]},\n",
       "  {'in0': [726], 'in1': [1028]},\n",
       "  {'in0': [415], 'in1': [328]},\n",
       "  {'in0': [480], 'in1': [234]},\n",
       "  {'in0': [927], 'in1': [7]},\n",
       "  {'in0': [887], 'in1': [993]},\n",
       "  {'in0': [274], 'in1': [25]},\n",
       "  {'in0': [376], 'in1': [269]},\n",
       "  {'in0': [363], 'in1': [616]},\n",
       "  {'in0': [557], 'in1': [254]},\n",
       "  {'in0': [405], 'in1': [953]},\n",
       "  {'in0': [860], 'in1': [514]},\n",
       "  {'in0': [410], 'in1': [886]},\n",
       "  {'in0': [304], 'in1': [275]},\n",
       "  {'in0': [410], 'in1': [286]},\n",
       "  {'in0': [144], 'in1': [182]},\n",
       "  {'in0': [631], 'in1': [289]},\n",
       "  {'in0': [773], 'in1': [169]},\n",
       "  {'in0': [335], 'in1': [333]},\n",
       "  {'in0': [609], 'in1': [538]},\n",
       "  {'in0': [240], 'in1': [272]},\n",
       "  {'in0': [743], 'in1': [100]},\n",
       "  {'in0': [537], 'in1': [28]},\n",
       "  {'in0': [588], 'in1': [721]},\n",
       "  {'in0': [307], 'in1': [687]},\n",
       "  {'in0': [807], 'in1': [399]},\n",
       "  {'in0': [10], 'in1': [611]},\n",
       "  {'in0': [217], 'in1': [597]},\n",
       "  {'in0': [113], 'in1': [329]},\n",
       "  {'in0': [545], 'in1': [222]},\n",
       "  {'in0': [523], 'in1': [186]},\n",
       "  {'in0': [260], 'in1': [300]},\n",
       "  {'in0': [735], 'in1': [744]},\n",
       "  {'in0': [630], 'in1': [1040]},\n",
       "  {'in0': [727], 'in1': [117]},\n",
       "  {'in0': [573], 'in1': [478]},\n",
       "  {'in0': [386], 'in1': [7]},\n",
       "  {'in0': [529], 'in1': [984]},\n",
       "  {'in0': [279], 'in1': [586]},\n",
       "  {'in0': [770], 'in1': [875]},\n",
       "  {'in0': [64], 'in1': [381]},\n",
       "  {'in0': [76], 'in1': [135]},\n",
       "  {'in0': [691], 'in1': [8]},\n",
       "  {'in0': [312], 'in1': [121]},\n",
       "  {'in0': [217], 'in1': [22]},\n",
       "  {'in0': [124], 'in1': [50]},\n",
       "  {'in0': [615], 'in1': [1021]},\n",
       "  {'in0': [132], 'in1': [127]},\n",
       "  {'in0': [868], 'in1': [568]},\n",
       "  {'in0': [222], 'in1': [278]},\n",
       "  {'in0': [20], 'in1': [208]},\n",
       "  {'in0': [510], 'in1': [286]},\n",
       "  {'in0': [300], 'in1': [872]},\n",
       "  {'in0': [451], 'in1': [872]},\n",
       "  {'in0': [255], 'in1': [879]},\n",
       "  {'in0': [830], 'in1': [225]},\n",
       "  {'in0': [171], 'in1': [292]},\n",
       "  {'in0': [693], 'in1': [604]},\n",
       "  {'in0': [249], 'in1': [239]},\n",
       "  {'in0': [342], 'in1': [591]},\n",
       "  {'in0': [819], 'in1': [321]},\n",
       "  {'in0': [879], 'in1': [15]},\n",
       "  {'in0': [286], 'in1': [357]},\n",
       "  {'in0': [206], 'in1': [1394]},\n",
       "  {'in0': [691], 'in1': [1]},\n",
       "  {'in0': [173], 'in1': [328]},\n",
       "  {'in0': [441], 'in1': [405]},\n",
       "  {'in0': [165], 'in1': [69]},\n",
       "  {'in0': [816], 'in1': [349]},\n",
       "  {'in0': [895], 'in1': [275]},\n",
       "  {'in0': [431], 'in1': [988]},\n",
       "  {'in0': [906], 'in1': [7]},\n",
       "  {'in0': [358], 'in1': [357]},\n",
       "  {'in0': [82], 'in1': [717]},\n",
       "  {'in0': [481], 'in1': [507]},\n",
       "  {'in0': [576], 'in1': [237]},\n",
       "  {'in0': [337], 'in1': [106]},\n",
       "  {'in0': [624], 'in1': [50]},\n",
       "  {'in0': [297], 'in1': [25]},\n",
       "  {'in0': [234], 'in1': [1462]},\n",
       "  {'in0': [594], 'in1': [319]},\n",
       "  {'in0': [537], 'in1': [682]},\n",
       "  {'in0': [806], 'in1': [629]},\n",
       "  {'in0': [928], 'in1': [98]},\n",
       "  {'in0': [633], 'in1': [234]},\n",
       "  {'in0': [524], 'in1': [89]},\n",
       "  {'in0': [666], 'in1': [173]},\n",
       "  {'in0': [853], 'in1': [301]},\n",
       "  {'in0': [940], 'in1': [272]},\n",
       "  {'in0': [858], 'in1': [307]},\n",
       "  {'in0': [379], 'in1': [69]},\n",
       "  {'in0': [99], 'in1': [4]},\n",
       "  {'in0': [162], 'in1': [25]},\n",
       "  {'in0': [435], 'in1': [462]},\n",
       "  {'in0': [453], 'in1': [246]},\n",
       "  {'in0': [208], 'in1': [204]},\n",
       "  {'in0': [785], 'in1': [183]},\n",
       "  {'in0': [230], 'in1': [742]},\n",
       "  {'in0': [781], 'in1': [322]},\n",
       "  {'in0': [413], 'in1': [222]},\n",
       "  {'in0': [829], 'in1': [250]},\n",
       "  {'in0': [671], 'in1': [204]},\n",
       "  {'in0': [552], 'in1': [151]},\n",
       "  {'in0': [228], 'in1': [886]},\n",
       "  {'in0': [70], 'in1': [419]},\n",
       "  {'in0': [318], 'in1': [768]},\n",
       "  {'in0': [7], 'in1': [648]},\n",
       "  {'in0': [592], 'in1': [79]},\n",
       "  {'in0': [159], 'in1': [259]},\n",
       "  {'in0': [561], 'in1': [23]},\n",
       "  {'in0': [645], 'in1': [197]},\n",
       "  {'in0': [419], 'in1': [173]},\n",
       "  {'in0': [712], 'in1': [72]},\n",
       "  {'in0': [308], 'in1': [187]},\n",
       "  {'in0': [634], 'in1': [273]},\n",
       "  {'in0': [340], 'in1': [204]},\n",
       "  {'in0': [87], 'in1': [367]},\n",
       "  {'in0': [906], 'in1': [100]},\n",
       "  {'in0': [315], 'in1': [17]},\n",
       "  {'in0': [615], 'in1': [87]},\n",
       "  {'in0': [482], 'in1': [298]},\n",
       "  {'in0': [494], 'in1': [191]},\n",
       "  {'in0': [96], 'in1': [87]},\n",
       "  {'in0': [507], 'in1': [147]},\n",
       "  {'in0': [478], 'in1': [151]},\n",
       "  {'in0': [607], 'in1': [528]},\n",
       "  {'in0': [3], 'in1': [294]},\n",
       "  {'in0': [171], 'in1': [262]},\n",
       "  {'in0': [154], 'in1': [357]},\n",
       "  {'in0': [208], 'in1': [663]},\n",
       "  {'in0': [472], 'in1': [1011]},\n",
       "  {'in0': [883], 'in1': [7]},\n",
       "  {'in0': [870], 'in1': [952]},\n",
       "  {'in0': [237], 'in1': [191]},\n",
       "  {'in0': [906], 'in1': [475]},\n",
       "  {'in0': [159], 'in1': [1037]},\n",
       "  {'in0': [872], 'in1': [323]},\n",
       "  {'in0': [455], 'in1': [56]},\n",
       "  {'in0': [472], 'in1': [877]},\n",
       "  {'in0': [597], 'in1': [1152]},\n",
       "  {'in0': [768], 'in1': [756]},\n",
       "  {'in0': [850], 'in1': [173]},\n",
       "  {'in0': [803], 'in1': [286]},\n",
       "  {'in0': [65], 'in1': [1142]},\n",
       "  {'in0': [412], 'in1': [92]},\n",
       "  {'in0': [872], 'in1': [815]},\n",
       "  {'in0': [17], 'in1': [508]},\n",
       "  {'in0': [570], 'in1': [302]},\n",
       "  {'in0': [768], 'in1': [845]},\n",
       "  {'in0': [508], 'in1': [735]},\n",
       "  {'in0': [218], 'in1': [33]},\n",
       "  {'in0': [367], 'in1': [774]},\n",
       "  {'in0': [92], 'in1': [168]},\n",
       "  {'in0': [494], 'in1': [50]},\n",
       "  {'in0': [484], 'in1': [235]},\n",
       "  {'in0': [853], 'in1': [245]},\n",
       "  {'in0': [827], 'in1': [329]},\n",
       "  {'in0': [663], 'in1': [181]},\n",
       "  {'in0': [649], 'in1': [323]},\n",
       "  {'in0': [446], 'in1': [311]},\n",
       "  {'in0': [734], 'in1': [198]},\n",
       "  {'in0': [260], 'in1': [272]},\n",
       "  {'in0': [348], 'in1': [742]},\n",
       "  {'in0': [606], 'in1': [147]},\n",
       "  {'in0': [696], 'in1': [344]},\n",
       "  {'in0': [229], 'in1': [349]},\n",
       "  {'in0': [920], 'in1': [300]},\n",
       "  {'in0': [347], 'in1': [416]},\n",
       "  {'in0': [454], 'in1': [87]},\n",
       "  {'in0': [715], 'in1': [216]},\n",
       "  {'in0': [122], 'in1': [1268]},\n",
       "  {'in0': [668], 'in1': [286]},\n",
       "  {'in0': [848], 'in1': [152]},\n",
       "  {'in0': [483], 'in1': [462]},\n",
       "  {'in0': [940], 'in1': [14]},\n",
       "  {'in0': [633], 'in1': [1132]},\n",
       "  {'in0': [411], 'in1': [209]},\n",
       "  {'in0': [668], 'in1': [354]},\n",
       "  {'in0': [247], 'in1': [50]},\n",
       "  {'in0': [572], 'in1': [476]},\n",
       "  {'in0': [743], 'in1': [321]},\n",
       "  {'in0': [689], 'in1': [151]},\n",
       "  {'in0': [250], 'in1': [404]},\n",
       "  {'in0': [426], 'in1': [633]},\n",
       "  {'in0': [758], 'in1': [298]},\n",
       "  {'in0': [97], 'in1': [670]},\n",
       "  {'in0': [571], 'in1': [496]},\n",
       "  {'in0': [88], 'in1': [886]},\n",
       "  {'in0': [796], 'in1': [184]},\n",
       "  {'in0': [709], 'in1': [22]},\n",
       "  {'in0': [544], 'in1': [258]},\n",
       "  {'in0': [506], 'in1': [1046]},\n",
       "  {'in0': [21], 'in1': [370]},\n",
       "  {'in0': [285], 'in1': [269]},\n",
       "  {'in0': [215], 'in1': [421]},\n",
       "  {'in0': [753], 'in1': [96]},\n",
       "  {'in0': [211], 'in1': [257]},\n",
       "  {'in0': [885], 'in1': [210]},\n",
       "  {'in0': [805], 'in1': [715]},\n",
       "  {'in0': [926], 'in1': [288]},\n",
       "  {'in0': [782], 'in1': [984]},\n",
       "  {'in0': [266], 'in1': [874]},\n",
       "  {'in0': [229], 'in1': [315]},\n",
       "  {'in0': [260], 'in1': [538]},\n",
       "  {'in0': [298], 'in1': [486]},\n",
       "  {'in0': [5], 'in1': [1]},\n",
       "  {'in0': [271], 'in1': [346]},\n",
       "  {'in0': [134], 'in1': [286]},\n",
       "  {'in0': [313], 'in1': [566]},\n",
       "  {'in0': [27], 'in1': [925]},\n",
       "  {'in0': [241], 'in1': [294]},\n",
       "  {'in0': [638], 'in1': [183]},\n",
       "  {'in0': [88], 'in1': [301]},\n",
       "  {'in0': [679], 'in1': [327]},\n",
       "  {'in0': [572], 'in1': [1137]},\n",
       "  {'in0': [43], 'in1': [120]},\n",
       "  {'in0': [888], 'in1': [535]},\n",
       "  {'in0': [371], 'in1': [175]},\n",
       "  {'in0': [220], 'in1': [298]},\n",
       "  {'in0': [14], 'in1': [357]},\n",
       "  {'in0': [683], 'in1': [264]},\n",
       "  {'in0': [92], 'in1': [1079]},\n",
       "  {'in0': [290], 'in1': [143]},\n",
       "  {'in0': [668], 'in1': [258]},\n",
       "  {'in0': [247], 'in1': [751]},\n",
       "  {'in0': [297], 'in1': [73]},\n",
       "  {'in0': [531], 'in1': [332]},\n",
       "  {'in0': [620], 'in1': [930]},\n",
       "  {'in0': [537], 'in1': [150]},\n",
       "  {'in0': [881], 'in1': [54]},\n",
       "  {'in0': [385], 'in1': [1367]},\n",
       "  {'in0': [8], 'in1': [550]},\n",
       "  {'in0': [23], 'in1': [258]},\n",
       "  {'in0': [51], 'in1': [64]},\n",
       "  {'in0': [523], 'in1': [393]},\n",
       "  {'in0': [729], 'in1': [690]},\n",
       "  {'in0': [669], 'in1': [521]},\n",
       "  {'in0': [669], 'in1': [505]},\n",
       "  {'in0': [166], 'in1': [300]},\n",
       "  {'in0': [5], 'in1': [363]},\n",
       "  {'in0': [169], 'in1': [181]},\n",
       "  {'in0': [846], 'in1': [1074]},\n",
       "  {'in0': [209], 'in1': [898]},\n",
       "  {'in0': [634], 'in1': [286]},\n",
       "  {'in0': [99], 'in1': [1016]},\n",
       "  {'in0': [867], 'in1': [480]},\n",
       "  {'in0': [547], 'in1': [347]},\n",
       "  {'in0': [80], 'in1': [423]},\n",
       "  {'in0': [745], 'in1': [174]},\n",
       "  {'in0': [602], 'in1': [988]},\n",
       "  {'in0': [17], 'in1': [151]},\n",
       "  {'in0': [738], 'in1': [651]},\n",
       "  {'in0': [9], 'in1': [487]},\n",
       "  {'in0': [376], 'in1': [237]},\n",
       "  {'in0': [595], 'in1': [763]},\n",
       "  {'in0': [683], 'in1': [748]},\n",
       "  {'in0': [551], 'in1': [756]},\n",
       "  {'in0': [389], 'in1': [109]},\n",
       "  {'in0': [262], 'in1': [553]},\n",
       "  {'in0': [479], 'in1': [228]},\n",
       "  {'in0': [47], 'in1': [324]},\n",
       "  {'in0': [128], 'in1': [603]},\n",
       "  {'in0': [493], 'in1': [252]},\n",
       "  {'in0': [497], 'in1': [840]},\n",
       "  {'in0': [753], 'in1': [79]},\n",
       "  {'in0': [101], 'in1': [282]},\n",
       "  {'in0': [604], 'in1': [201]},\n",
       "  {'in0': [349], 'in1': [823]},\n",
       "  {'in0': [492], 'in1': [511]},\n",
       "  {'in0': [29], 'in1': [189]},\n",
       "  {'in0': [27], 'in1': [121]},\n",
       "  {'in0': [165], 'in1': [419]},\n",
       "  {'in0': [846], 'in1': [377]},\n",
       "  {'in0': [513], 'in1': [118]},\n",
       "  {'in0': [369], 'in1': [172]},\n",
       "  {'in0': [149], 'in1': [301]},\n",
       "  {'in0': [905], 'in1': [328]},\n",
       "  {'in0': [65], 'in1': [97]},\n",
       "  {'in0': [467], 'in1': [109]},\n",
       "  {'in0': [231], 'in1': [126]},\n",
       "  {'in0': [512], 'in1': [265]},\n",
       "  {'in0': [225], 'in1': [64]},\n",
       "  {'in0': [5], 'in1': [424]},\n",
       "  {'in0': [40], 'in1': [754]},\n",
       "  {'in0': [197], 'in1': [55]},\n",
       "  {'in0': [438], 'in1': [476]},\n",
       "  {'in0': [644], 'in1': [294]},\n",
       "  {'in0': [260], 'in1': [1025]},\n",
       "  {'in0': [505], 'in1': [989]},\n",
       "  {'in0': [727], 'in1': [181]},\n",
       "  {'in0': [148], 'in1': [140]},\n",
       "  {'in0': [909], 'in1': [275]},\n",
       "  {'in0': [599], 'in1': [988]},\n",
       "  {'in0': [555], 'in1': [480]},\n",
       "  {'in0': [775], 'in1': [286]},\n",
       "  {'in0': [568], 'in1': [923]},\n",
       "  {'in0': [169], 'in1': [301]},\n",
       "  {'in0': [399], 'in1': [284]},\n",
       "  {'in0': [284], 'in1': [305]},\n",
       "  {'in0': [880], 'in1': [651]},\n",
       "  {'in0': [623], 'in1': [523]},\n",
       "  {'in0': [164], 'in1': [299]},\n",
       "  {'in0': [442], 'in1': [54]},\n",
       "  {'in0': [180], 'in1': [431]},\n",
       "  {'in0': [74], 'in1': [272]},\n",
       "  {'in0': [366], 'in1': [447]},\n",
       "  {'in0': [230], 'in1': [202]},\n",
       "  {'in0': [251], 'in1': [1014]},\n",
       "  {'in0': [742], 'in1': [250]},\n",
       "  {'in0': [623], 'in1': [275]},\n",
       "  {'in0': [928], 'in1': [877]},\n",
       "  {'in0': [685], 'in1': [991]},\n",
       "  {'in0': [923], 'in1': [333]},\n",
       "  {'in0': [627], 'in1': [403]},\n",
       "  {'in0': [802], 'in1': [327]},\n",
       "  {'in0': [354], 'in1': [133]},\n",
       "  {'in0': [150], 'in1': [293]},\n",
       "  {'in0': [401], 'in1': [135]},\n",
       "  {'in0': [170], 'in1': [333]},\n",
       "  {'in0': [609], 'in1': [287]},\n",
       "  {'in0': [382], 'in1': [25]},\n",
       "  {'in0': [214], 'in1': [302]},\n",
       "  {'in0': [427], 'in1': [680]},\n",
       "  {'in0': [812], 'in1': [294]},\n",
       "  {'in0': [57], 'in1': [682]},\n",
       "  {'in0': [55], 'in1': [257]},\n",
       "  {'in0': [258], 'in1': [323]},\n",
       "  {'in0': [820], 'in1': [748]},\n",
       "  {'in0': [455], 'in1': [20]},\n",
       "  {'in0': [596], 'in1': [323]},\n",
       "  {'in0': [78], 'in1': [269]},\n",
       "  {'in0': [768], 'in1': [257]},\n",
       "  {'in0': [861], 'in1': [275]},\n",
       "  {'in0': [679], 'in1': [121]},\n",
       "  {'in0': [635], 'in1': [269]},\n",
       "  {'in0': [627], 'in1': [1004]},\n",
       "  {'in0': [77], 'in1': [474]},\n",
       "  {'in0': [331], 'in1': [47]},\n",
       "  {'in0': [784], 'in1': [328]},\n",
       "  {'in0': [594], 'in1': [988]},\n",
       "  {'in0': [432], 'in1': [546]},\n",
       "  {'in0': [152], 'in1': [25]},\n",
       "  {'in0': [345], 'in1': [1160]},\n",
       "  {'in0': [403], 'in1': [121]},\n",
       "  {'in0': [33], 'in1': [313]},\n",
       "  {'in0': [559], 'in1': [514]},\n",
       "  {'in0': [703], 'in1': [147]},\n",
       "  {'in0': [697], 'in1': [294]},\n",
       "  {'in0': [22], 'in1': [128]},\n",
       "  {'in0': [236], 'in1': [632]},\n",
       "  {'in0': [136], 'in1': [14]},\n",
       "  {'in0': [198], 'in1': [118]},\n",
       "  {'in0': [877], 'in1': [31]},\n",
       "  {'in0': [870], 'in1': [517]},\n",
       "  {'in0': [14], 'in1': [530]},\n",
       "  {'in0': [924], 'in1': [523]},\n",
       "  {'in0': [921], 'in1': [313]},\n",
       "  {'in0': [604], 'in1': [185]},\n",
       "  {'in0': [598], 'in1': [349]},\n",
       "  {'in0': [423], 'in1': [1134]},\n",
       "  {'in0': [782], 'in1': [250]},\n",
       "  {'in0': [195], 'in1': [273]},\n",
       "  {'in0': [281], 'in1': [682]},\n",
       "  {'in0': [271], 'in1': [709]},\n",
       "  {'in0': [224], 'in1': [29]},\n",
       "  {'in0': [607], 'in1': [847]},\n",
       "  {'in0': [170], 'in1': [299]},\n",
       "  {'in0': [549], 'in1': [288]},\n",
       "  {'in0': [360], 'in1': [334]},\n",
       "  {'in0': [672], 'in1': [50]},\n",
       "  {'in0': [421], 'in1': [156]},\n",
       "  {'in0': [160], 'in1': [234]},\n",
       "  {'in0': [292], 'in1': [11]},\n",
       "  {'in0': [526], 'in1': [270]},\n",
       "  {'in0': [740], 'in1': [748]},\n",
       "  {'in0': [510], 'in1': [358]},\n",
       "  {'in0': [381], 'in1': [212]},\n",
       "  {'in0': [860], 'in1': [283]},\n",
       "  {'in0': [676], 'in1': [265]},\n",
       "  {'in0': [691], 'in1': [478]},\n",
       "  {'in0': [470], 'in1': [824]},\n",
       "  {'in0': [26], 'in1': [15]},\n",
       "  {'in0': [286], 'in1': [208]},\n",
       "  {'in0': [83], 'in1': [576]},\n",
       "  {'in0': [595], 'in1': [1134]},\n",
       "  {'in0': [696], 'in1': [523]},\n",
       "  {'in0': [36], 'in1': [882]},\n",
       "  {'in0': [707], 'in1': [663]},\n",
       "  {'in0': [622], 'in1': [993]},\n",
       "  {'in0': [176], 'in1': [324]},\n",
       "  {'in0': [715], 'in1': [33]},\n",
       "  {'in0': [344], 'in1': [181]},\n",
       "  {'in0': [774], 'in1': [294]},\n",
       "  {'in0': [801], 'in1': [326]},\n",
       "  {'in0': [667], 'in1': [318]},\n",
       "  {'in0': [333], 'in1': [88]},\n",
       "  {'in0': [350], 'in1': [530]},\n",
       "  {'in0': [509], 'in1': [879]},\n",
       "  {'in0': [603], 'in1': [174]},\n",
       "  {'in0': [146], 'in1': [300]},\n",
       "  {'in0': [257], 'in1': [289]},\n",
       "  {'in0': [36], 'in1': [748]},\n",
       "  {'in0': [894], 'in1': [15]},\n",
       "  {'in0': [14], 'in1': [474]},\n",
       "  {'in0': [883], 'in1': [727]},\n",
       "  {'in0': [465], 'in1': [202]},\n",
       "  {'in0': [236], 'in1': [286]},\n",
       "  {'in0': [250], 'in1': [140]},\n",
       "  {'in0': [227], 'in1': [126]},\n",
       "  {'in0': [751], 'in1': [472]},\n",
       "  {'in0': [249], 'in1': [930]},\n",
       "  {'in0': [282], 'in1': [327]},\n",
       "  {'in0': [427], 'in1': [938]},\n",
       "  {'in0': [472], 'in1': [826]},\n",
       "  {'in0': [228], 'in1': [87]},\n",
       "  {'in0': [504], 'in1': [66]},\n",
       "  {'in0': [160], 'in1': [160]},\n",
       "  {'in0': [128], 'in1': [485]},\n",
       "  {'in0': [584], 'in1': [109]},\n",
       "  {'in0': [870], 'in1': [511]},\n",
       "  {'in0': [887], 'in1': [111]},\n",
       "  {'in0': [842], 'in1': [268]},\n",
       "  {'in0': [339], 'in1': [47]},\n",
       "  {'in0': [172], 'in1': [580]},\n",
       "  {'in0': [785], 'in1': [22]},\n",
       "  {'in0': [462], 'in1': [313]},\n",
       "  {'in0': [347], 'in1': [168]},\n",
       "  {'in0': [702], 'in1': [313]},\n",
       "  {'in0': [745], 'in1': [190]},\n",
       "  {'in0': [204], 'in1': [286]},\n",
       "  {'in0': [658], 'in1': [772]},\n",
       "  {'in0': [278], 'in1': [311]},\n",
       "  {'in0': [204], 'in1': [748]},\n",
       "  {'in0': [567], 'in1': [636]},\n",
       "  {'in0': [512], 'in1': [318]},\n",
       "  {'in0': [515], 'in1': [893]},\n",
       "  {'in0': [907], 'in1': [100]},\n",
       "  {'in0': [824], 'in1': [991]},\n",
       "  {'in0': [816], 'in1': [258]},\n",
       "  {'in0': [321], 'in1': [523]},\n",
       "  {'in0': [574], 'in1': [690]},\n",
       "  {'in0': [558], 'in1': [124]},\n",
       "  {'in0': [706], 'in1': [1]},\n",
       "  {'in0': [240], 'in1': [343]},\n",
       "  {'in0': [467], 'in1': [150]},\n",
       "  {'in0': [626], 'in1': [988]},\n",
       "  {'in0': [498], 'in1': [197]},\n",
       "  {'in0': [151], 'in1': [13]},\n",
       "  {'in0': [913], 'in1': [1112]},\n",
       "  {'in0': [631], 'in1': [346]},\n",
       "  {'in0': [781], 'in1': [483]},\n",
       "  {'in0': [82], 'in1': [194]},\n",
       "  {'in0': [919], 'in1': [19]},\n",
       "  {'in0': [857], 'in1': [898]},\n",
       "  {'in0': [459], 'in1': [194]},\n",
       "  {'in0': [651], 'in1': [268]},\n",
       "  {'in0': [746], 'in1': [96]},\n",
       "  {'in0': [899], 'in1': [685]},\n",
       "  {'in0': [862], 'in1': [127]},\n",
       "  {'in0': [863], 'in1': [333]},\n",
       "  {'in0': [660], 'in1': [184]},\n",
       "  {'in0': [354], 'in1': [847]},\n",
       "  {'in0': [106], 'in1': [526]},\n",
       "  {'in0': [33], 'in1': [895]},\n",
       "  {'in0': [446], 'in1': [294]},\n",
       "  {'in0': [525], 'in1': [300]},\n",
       "  {'in0': [295], 'in1': [218]},\n",
       "  {'in0': [261], 'in1': [597]},\n",
       "  {'in0': [758], 'in1': [597]},\n",
       "  {'in0': [351], 'in1': [313]},\n",
       "  {'in0': [199], 'in1': [268]},\n",
       "  {'in0': [385], 'in1': [367]},\n",
       "  {'in0': [282], 'in1': [338]},\n",
       "  {'in0': [608], 'in1': [65]},\n",
       "  {'in0': [263], 'in1': [117]},\n",
       "  {'in0': [924], 'in1': [174]},\n",
       "  {'in0': [681], 'in1': [294]},\n",
       "  {'in0': [762], 'in1': [116]},\n",
       "  {'in0': [843], 'in1': [186]},\n",
       "  {'in0': [771], 'in1': [222]},\n",
       "  {'in0': [907], 'in1': [1057]},\n",
       "  {'in0': [270], 'in1': [97]},\n",
       "  {'in0': [507], 'in1': [826]},\n",
       "  {'in0': [336], 'in1': [66]},\n",
       "  {'in0': [908], 'in1': [701]},\n",
       "  {'in0': [504], 'in1': [807]},\n",
       "  {'in0': [682], 'in1': [325]},\n",
       "  {'in0': [294], 'in1': [1245]},\n",
       "  {'in0': [237], 'in1': [1192]},\n",
       "  {'in0': [941], 'in1': [258]},\n",
       "  {'in0': [873], 'in1': [321]},\n",
       "  {'in0': [298], 'in1': [281]},\n",
       "  {'in0': [812], 'in1': [678]},\n",
       "  {'in0': [354], 'in1': [242]},\n",
       "  {'in0': [496], 'in1': [132]},\n",
       "  {'in0': [107], 'in1': [258]},\n",
       "  {'in0': [943], 'in1': [570]},\n",
       "  {'in0': [479], 'in1': [325]},\n",
       "  {'in0': [338], 'in1': [79]},\n",
       "  {'in0': [423], 'in1': [323]},\n",
       "  {'in0': [107], 'in1': [340]},\n",
       "  {'in0': [874], 'in1': [289]},\n",
       "  {'in0': [465], 'in1': [474]},\n",
       "  {'in0': [780], 'in1': [202]},\n",
       "  {'in0': [690], 'in1': [67]},\n",
       "  {'in0': [627], 'in1': [26]},\n",
       "  {'in0': [660], 'in1': [96]},\n",
       "  {'in0': [714], 'in1': [237]},\n",
       "  {'in0': [159], 'in1': [815]},\n",
       "  {'in0': [296], 'in1': [20]},\n",
       "  {'in0': [304], 'in1': [259]},\n",
       "  {'in0': [578], 'in1': [250]},\n",
       "  {'in0': [71], 'in1': [276]},\n",
       "  {'in0': [563], 'in1': [237]},\n",
       "  {'in0': [835], 'in1': [660]},\n",
       "  {'in0': [127], 'in1': [227]},\n",
       "  {'in0': [834], 'in1': [282]},\n",
       "  {'in0': [864], 'in1': [642]},\n",
       "  {'in0': [261], 'in1': [339]},\n",
       "  {'in0': [500], 'in1': [665]},\n",
       "  {'in0': [469], 'in1': [605]},\n",
       "  {'in0': [212], 'in1': [515]},\n",
       "  {'in0': [656], 'in1': [689]},\n",
       "  {'in0': [329], 'in1': [100]},\n",
       "  {'in0': [669], 'in1': [462]},\n",
       "  {'in0': [760], 'in1': [278]},\n",
       "  {'in0': [461], 'in1': [121]},\n",
       "  {'in0': [144], 'in1': [709]},\n",
       "  {'in0': [470], 'in1': [1097]},\n",
       "  {'in0': [374], 'in1': [356]},\n",
       "  {'in0': [33], 'in1': [872]},\n",
       "  {'in0': [173], 'in1': [300]},\n",
       "  {'in0': [595], 'in1': [825]},\n",
       "  {'in0': [761], 'in1': [1197]},\n",
       "  {'in0': [124], 'in1': [96]},\n",
       "  {'in0': [666], 'in1': [203]},\n",
       "  {'in0': [832], 'in1': [328]},\n",
       "  {'in0': [925], 'in1': [299]},\n",
       "  {'in0': [375], 'in1': [583]},\n",
       "  {'in0': [897], 'in1': [472]},\n",
       "  {'in0': [576], 'in1': [181]},\n",
       "  {'in0': [865], 'in1': [676]},\n",
       "  {'in0': [813], 'in1': [294]},\n",
       "  {'in0': [749], 'in1': [1]},\n",
       "  {'in0': [216], 'in1': [697]},\n",
       "  {'in0': [445], 'in1': [1591]},\n",
       "  {'in0': [204], 'in1': [292]},\n",
       "  {'in0': [319], 'in1': [307]},\n",
       "  {'in0': [708], 'in1': [471]},\n",
       "  {'in0': [934], 'in1': [177]},\n",
       "  {'in0': [316], 'in1': [292]},\n",
       "  {'in0': [575], 'in1': [506]},\n",
       "  {'in0': [386], 'in1': [281]},\n",
       "  {'in0': [781], 'in1': [127]},\n",
       "  {'in0': [536], 'in1': [70]},\n",
       "  {'in0': [602], 'in1': [125]},\n",
       "  {'in0': [331], 'in1': [69]},\n",
       "  {'in0': [540], 'in1': [742]},\n",
       "  {'in0': [584], 'in1': [82]},\n",
       "  {'in0': [331], 'in1': [486]},\n",
       "  {'in0': [382], 'in1': [127]},\n",
       "  {'in0': [674], 'in1': [50]},\n",
       "  {'in0': [124], 'in1': [226]},\n",
       "  {'in0': [626], 'in1': [289]},\n",
       "  {'in0': [282], 'in1': [288]},\n",
       "  {'in0': [816], 'in1': [288]},\n",
       "  {'in0': [723], 'in1': [191]},\n",
       "  {'in0': [733], 'in1': [10]},\n",
       "  {'in0': [1], 'in1': [33]},\n",
       "  {'in0': [793], 'in1': [276]},\n",
       "  {'in0': [799], 'in1': [319]},\n",
       "  {'in0': [587], 'in1': [303]},\n",
       "  {'in0': [917], 'in1': [150]},\n",
       "  {'in0': [260], 'in1': [682]},\n",
       "  {'in0': [235], 'in1': [70]},\n",
       "  {'in0': [131], 'in1': [221]},\n",
       "  {'in0': [542], 'in1': [265]},\n",
       "  {'in0': [509], 'in1': [310]},\n",
       "  {'in0': [282], 'in1': [271]},\n",
       "  {'in0': [591], 'in1': [286]},\n",
       "  {'in0': [615], 'in1': [170]},\n",
       "  {'in0': [764], 'in1': [140]},\n",
       "  {'in0': [931], 'in1': [678]},\n",
       "  {'in0': [836], 'in1': [507]},\n",
       "  {'in0': [63], 'in1': [50]},\n",
       "  {'in0': [208], 'in1': [197]},\n",
       "  {'in0': [562], 'in1': [191]},\n",
       "  {'in0': [165], 'in1': [181]},\n",
       "  {'in0': [396], 'in1': [106]},\n",
       "  {'in0': [774], 'in1': [1419]},\n",
       "  {'in0': [469], 'in1': [153]},\n",
       "  {'in0': [367], 'in1': [17]},\n",
       "  {'in0': [196], 'in1': [67]},\n",
       "  {'in0': [70], 'in1': [210]},\n",
       "  {'in0': [704], 'in1': [461]},\n",
       "  {'in0': [702], 'in1': [288]},\n",
       "  {'in0': [337], 'in1': [1133]},\n",
       "  {'in0': [178], 'in1': [385]},\n",
       "  {'in0': [685], 'in1': [325]},\n",
       "  {'in0': [517], 'in1': [284]},\n",
       "  {'in0': [18], 'in1': [408]},\n",
       "  {'in0': [810], 'in1': [294]},\n",
       "  {'in0': [317], 'in1': [328]},\n",
       "  {'in0': [570], 'in1': [301]},\n",
       "  {'in0': [273], 'in1': [305]},\n",
       "  {'in0': [911], 'in1': [174]},\n",
       "  {'in0': [724], 'in1': [342]},\n",
       "  {'in0': [806], 'in1': [192]},\n",
       "  {'in0': [760], 'in1': [819]},\n",
       "  {'in0': [457], 'in1': [775]},\n",
       "  {'in0': [263], 'in1': [416]},\n",
       "  {'in0': [809], 'in1': [748]},\n",
       "  {'in0': [132], 'in1': [124]},\n",
       "  {'in0': [438], 'in1': [237]},\n",
       "  {'in0': [262], 'in1': [617]},\n",
       "  {'in0': [141], 'in1': [472]},\n",
       "  {'in0': [544], 'in1': [286]},\n",
       "  {'in0': [847], 'in1': [290]},\n",
       "  {'in0': [179], 'in1': [751]},\n",
       "  {'in0': [759], 'in1': [121]},\n",
       "  {'in0': [411], 'in1': [770]},\n",
       "  {'in0': [730], 'in1': [815]},\n",
       "  {'in0': [915], 'in1': [328]},\n",
       "  {'in0': [819], 'in1': [340]},\n",
       "  {'in0': [619], 'in1': [332]},\n",
       "  {'in0': [913], 'in1': [260]},\n",
       "  {'in0': [535], 'in1': [836]},\n",
       "  {'in0': [715], 'in1': [181]},\n",
       "  {'in0': [897], 'in1': [393]},\n",
       "  {'in0': [793], 'in1': [824]},\n",
       "  {'in0': [886], 'in1': [160]},\n",
       "  {'in0': [615], 'in1': [517]},\n",
       "  {'in0': [117], 'in1': [151]},\n",
       "  {'in0': [373], 'in1': [707]},\n",
       "  {'in0': [323], 'in1': [117]},\n",
       "  {'in0': [595], 'in1': [1059]},\n",
       "  {'in0': [611], 'in1': [300]},\n",
       "  {'in0': [695], 'in1': [319]},\n",
       "  {'in0': [92], 'in1': [501]},\n",
       "  {'in0': [703], 'in1': [117]},\n",
       "  {'in0': [546], 'in1': [928]},\n",
       "  {'in0': [32], 'in1': [1012]},\n",
       "  {'in0': [110], 'in1': [688]},\n",
       "  {'in0': [3], 'in1': [343]},\n",
       "  {'in0': [788], 'in1': [723]},\n",
       "  {'in0': [449], 'in1': [381]},\n",
       "  {'in0': [214], 'in1': [478]},\n",
       "  {'in0': [241], 'in1': [307]},\n",
       "  {'in0': [347], 'in1': [227]},\n",
       "  {'in0': [747], 'in1': [432]},\n",
       "  {'in0': [484], 'in1': [70]},\n",
       "  {'in0': [576], 'in1': [381]},\n",
       "  {'in0': [747], 'in1': [223]},\n",
       "  {'in0': [841], 'in1': [353]},\n",
       "  {'in0': [524], 'in1': [1268]},\n",
       "  {'in0': [29], 'in1': [98]},\n",
       "  {'in0': [257], 'in1': [137]},\n",
       "  {'in0': [65], 'in1': [100]},\n",
       "  {'in0': [643], 'in1': [1149]},\n",
       "  {'in0': [447], 'in1': [1034]},\n",
       "  {'in0': [581], 'in1': [221]},\n",
       "  {'in0': [559], 'in1': [188]},\n",
       "  {'in0': [566], 'in1': [1193]},\n",
       "  {'in0': [611], 'in1': [333]},\n",
       "  {'in0': [528], 'in1': [193]},\n",
       "  {'in0': [860], 'in1': [303]},\n",
       "  {'in0': [737], 'in1': [89]},\n",
       "  {'in0': [719], 'in1': [659]},\n",
       "  {'in0': [709], 'in1': [219]},\n",
       "  {'in0': [607], 'in1': [86]},\n",
       "  {'in0': [838], 'in1': [497]},\n",
       "  {'in0': [784], 'in1': [312]},\n",
       "  {'in0': [198], 'in1': [58]},\n",
       "  {'in0': [734], 'in1': [591]},\n",
       "  {'in0': [330], 'in1': [215]},\n",
       "  {'in0': [716], 'in1': [483]},\n",
       "  {'in0': [844], 'in1': [154]},\n",
       "  {'in0': [276], 'in1': [142]},\n",
       "  {'in0': [475], 'in1': [313]},\n",
       "  {'in0': [115], 'in1': [127]},\n",
       "  {'in0': [492], 'in1': [1021]},\n",
       "  {'in0': [157], 'in1': [150]},\n",
       "  {'in0': [525], 'in1': [412]},\n",
       "  {'in0': [548], 'in1': [118]},\n",
       "  {'in0': [52], 'in1': [95]},\n",
       "  {'in0': [491], 'in1': [285]},\n",
       "  {'in0': [521], 'in1': [81]},\n",
       "  {'in0': [943], 'in1': [808]},\n",
       "  {'in0': [855], 'in1': [86]},\n",
       "  {'in0': [55], 'in1': [254]},\n",
       "  {'in0': [303], 'in1': [69]},\n",
       "  {'in0': [932], 'in1': [435]},\n",
       "  {'in0': [532], 'in1': [603]},\n",
       "  {'in0': [637], 'in1': [1374]},\n",
       "  {'in0': [793], 'in1': [150]},\n",
       "  {'in0': [320], 'in1': [42]},\n",
       "  {'in0': [886], 'in1': [233]},\n",
       "  {'in0': [942], 'in1': [584]},\n",
       "  ...)}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_c_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dat = pd.DataFrame.from_dict(movieID_name_map, orient = 'index')\n",
    "dat= dat.reset_index().rename({'index':'id'}, axis=1)\n",
    "dat.to_csv(\"movie_mapping.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'Toy Story (1995)',\n",
       " 2: 'GoldenEye (1995)',\n",
       " 3: 'Four Rooms (1995)',\n",
       " 4: 'Get Shorty (1995)',\n",
       " 5: 'Copycat (1995)',\n",
       " 6: 'Shanghai Triad (Yao a yao yao dao waipo qiao) (1995)',\n",
       " 7: 'Twelve Monkeys (1995)',\n",
       " 8: 'Babe (1995)',\n",
       " 9: 'Dead Man Walking (1995)',\n",
       " 10: 'Richard III (1995)',\n",
       " 11: 'Seven (Se7en) (1995)',\n",
       " 12: 'Usual Suspects, The (1995)',\n",
       " 13: 'Mighty Aphrodite (1995)',\n",
       " 14: 'Postino, Il (1994)',\n",
       " 15: \"Mr. Holland's Opus (1995)\",\n",
       " 16: 'French Twist (Gazon maudit) (1995)',\n",
       " 17: 'From Dusk Till Dawn (1996)',\n",
       " 18: 'White Balloon, The (1995)',\n",
       " 19: \"Antonia's Line (1995)\",\n",
       " 20: 'Angels and Insects (1995)',\n",
       " 21: 'Muppet Treasure Island (1996)',\n",
       " 22: 'Braveheart (1995)',\n",
       " 23: 'Taxi Driver (1976)',\n",
       " 24: 'Rumble in the Bronx (1995)',\n",
       " 25: 'Birdcage, The (1996)',\n",
       " 26: 'Brothers McMullen, The (1995)',\n",
       " 27: 'Bad Boys (1995)',\n",
       " 28: 'Apollo 13 (1995)',\n",
       " 29: 'Batman Forever (1995)',\n",
       " 30: 'Belle de jour (1967)',\n",
       " 31: 'Crimson Tide (1995)',\n",
       " 32: 'Crumb (1994)',\n",
       " 33: 'Desperado (1995)',\n",
       " 34: 'Doom Generation, The (1995)',\n",
       " 35: 'Free Willy 2: The Adventure Home (1995)',\n",
       " 36: 'Mad Love (1995)',\n",
       " 37: 'Nadja (1994)',\n",
       " 38: 'Net, The (1995)',\n",
       " 39: 'Strange Days (1995)',\n",
       " 40: 'To Wong Foo, Thanks for Everything! Julie Newmar (1995)',\n",
       " 41: 'Billy Madison (1995)',\n",
       " 42: 'Clerks (1994)',\n",
       " 43: 'Disclosure (1994)',\n",
       " 44: 'Dolores Claiborne (1994)',\n",
       " 45: 'Eat Drink Man Woman (1994)',\n",
       " 46: 'Exotica (1994)',\n",
       " 47: 'Ed Wood (1994)',\n",
       " 48: 'Hoop Dreams (1994)',\n",
       " 49: 'I.Q. (1994)',\n",
       " 50: 'Star Wars (1977)',\n",
       " 51: 'Legends of the Fall (1994)',\n",
       " 52: 'Madness of King George, The (1994)',\n",
       " 53: 'Natural Born Killers (1994)',\n",
       " 54: 'Outbreak (1995)',\n",
       " 55: 'Professional, The (1994)',\n",
       " 56: 'Pulp Fiction (1994)',\n",
       " 57: 'Priest (1994)',\n",
       " 58: 'Quiz Show (1994)',\n",
       " 59: 'Three Colors: Red (1994)',\n",
       " 60: 'Three Colors: Blue (1993)',\n",
       " 61: 'Three Colors: White (1994)',\n",
       " 62: 'Stargate (1994)',\n",
       " 63: 'Santa Clause, The (1994)',\n",
       " 64: 'Shawshank Redemption, The (1994)',\n",
       " 65: \"What's Eating Gilbert Grape (1993)\",\n",
       " 66: 'While You Were Sleeping (1995)',\n",
       " 67: 'Ace Ventura: Pet Detective (1994)',\n",
       " 68: 'Crow, The (1994)',\n",
       " 69: 'Forrest Gump (1994)',\n",
       " 70: 'Four Weddings and a Funeral (1994)',\n",
       " 71: 'Lion King, The (1994)',\n",
       " 72: 'Mask, The (1994)',\n",
       " 73: 'Maverick (1994)',\n",
       " 74: 'Faster Pussycat! Kill! Kill! (1965)',\n",
       " 75: 'Brother Minister: The Assassination of Malcolm X (1994)',\n",
       " 76: \"Carlito's Way (1993)\",\n",
       " 77: 'Firm, The (1993)',\n",
       " 78: 'Free Willy (1993)',\n",
       " 79: 'Fugitive, The (1993)',\n",
       " 80: 'Hot Shots! Part Deux (1993)',\n",
       " 81: 'Hudsucker Proxy, The (1994)',\n",
       " 82: 'Jurassic Park (1993)',\n",
       " 83: 'Much Ado About Nothing (1993)',\n",
       " 84: \"Robert A. Heinlein's The Puppet Masters (1994)\",\n",
       " 85: 'Ref, The (1994)',\n",
       " 86: 'Remains of the Day, The (1993)',\n",
       " 87: 'Searching for Bobby Fischer (1993)',\n",
       " 88: 'Sleepless in Seattle (1993)',\n",
       " 89: 'Blade Runner (1982)',\n",
       " 90: 'So I Married an Axe Murderer (1993)',\n",
       " 91: 'Nightmare Before Christmas, The (1993)',\n",
       " 92: 'True Romance (1993)',\n",
       " 93: 'Welcome to the Dollhouse (1995)',\n",
       " 94: 'Home Alone (1990)',\n",
       " 95: 'Aladdin (1992)',\n",
       " 96: 'Terminator 2: Judgment Day (1991)',\n",
       " 97: 'Dances with Wolves (1990)',\n",
       " 98: 'Silence of the Lambs, The (1991)',\n",
       " 99: 'Snow White and the Seven Dwarfs (1937)',\n",
       " 100: 'Fargo (1996)',\n",
       " 101: 'Heavy Metal (1981)',\n",
       " 102: 'Aristocats, The (1970)',\n",
       " 103: 'All Dogs Go to Heaven 2 (1996)',\n",
       " 104: 'Theodore Rex (1995)',\n",
       " 105: 'Sgt. Bilko (1996)',\n",
       " 106: 'Diabolique (1996)',\n",
       " 107: 'Moll Flanders (1996)',\n",
       " 108: 'Kids in the Hall: Brain Candy (1996)',\n",
       " 109: 'Mystery Science Theater 3000: The Movie (1996)',\n",
       " 110: 'Operation Dumbo Drop (1995)',\n",
       " 111: 'Truth About Cats & Dogs, The (1996)',\n",
       " 112: 'Flipper (1996)',\n",
       " 113: 'Horseman on the Roof, The (Hussard sur le toit, Le) (1995)',\n",
       " 114: 'Wallace & Gromit: The Best of Aardman Animation (1996)',\n",
       " 115: 'Haunted World of Edward D. Wood Jr., The (1995)',\n",
       " 116: 'Cold Comfort Farm (1995)',\n",
       " 117: 'Rock, The (1996)',\n",
       " 118: 'Twister (1996)',\n",
       " 119: 'Maya Lin: A Strong Clear Vision (1994)',\n",
       " 120: 'Striptease (1996)',\n",
       " 121: 'Independence Day (ID4) (1996)',\n",
       " 122: 'Cable Guy, The (1996)',\n",
       " 123: 'Frighteners, The (1996)',\n",
       " 124: 'Lone Star (1996)',\n",
       " 125: 'Phenomenon (1996)',\n",
       " 126: 'Spitfire Grill, The (1996)',\n",
       " 127: 'Godfather, The (1972)',\n",
       " 128: 'Supercop (1992)',\n",
       " 129: 'Bound (1996)',\n",
       " 130: 'Kansas City (1996)',\n",
       " 131: \"Breakfast at Tiffany's (1961)\",\n",
       " 132: 'Wizard of Oz, The (1939)',\n",
       " 133: 'Gone with the Wind (1939)',\n",
       " 134: 'Citizen Kane (1941)',\n",
       " 135: '2001: A Space Odyssey (1968)',\n",
       " 136: 'Mr. Smith Goes to Washington (1939)',\n",
       " 137: 'Big Night (1996)',\n",
       " 138: 'D3: The Mighty Ducks (1996)',\n",
       " 139: 'Love Bug, The (1969)',\n",
       " 140: 'Homeward Bound: The Incredible Journey (1993)',\n",
       " 141: '20,000 Leagues Under the Sea (1954)',\n",
       " 142: 'Bedknobs and Broomsticks (1971)',\n",
       " 143: 'Sound of Music, The (1965)',\n",
       " 144: 'Die Hard (1988)',\n",
       " 145: 'Lawnmower Man, The (1992)',\n",
       " 146: 'Unhook the Stars (1996)',\n",
       " 147: 'Long Kiss Goodnight, The (1996)',\n",
       " 148: 'Ghost and the Darkness, The (1996)',\n",
       " 149: 'Jude (1996)',\n",
       " 150: 'Swingers (1996)',\n",
       " 151: 'Willy Wonka and the Chocolate Factory (1971)',\n",
       " 152: 'Sleeper (1973)',\n",
       " 153: 'Fish Called Wanda, A (1988)',\n",
       " 154: \"Monty Python's Life of Brian (1979)\",\n",
       " 155: 'Dirty Dancing (1987)',\n",
       " 156: 'Reservoir Dogs (1992)',\n",
       " 157: 'Platoon (1986)',\n",
       " 158: \"Weekend at Bernie's (1989)\",\n",
       " 159: 'Basic Instinct (1992)',\n",
       " 160: 'Glengarry Glen Ross (1992)',\n",
       " 161: 'Top Gun (1986)',\n",
       " 162: 'On Golden Pond (1981)',\n",
       " 163: 'Return of the Pink Panther, The (1974)',\n",
       " 164: 'Abyss, The (1989)',\n",
       " 165: 'Jean de Florette (1986)',\n",
       " 166: 'Manon of the Spring (Manon des sources) (1986)',\n",
       " 167: 'Private Benjamin (1980)',\n",
       " 168: 'Monty Python and the Holy Grail (1974)',\n",
       " 169: 'Wrong Trousers, The (1993)',\n",
       " 170: 'Cinema Paradiso (1988)',\n",
       " 171: 'Delicatessen (1991)',\n",
       " 172: 'Empire Strikes Back, The (1980)',\n",
       " 173: 'Princess Bride, The (1987)',\n",
       " 174: 'Raiders of the Lost Ark (1981)',\n",
       " 175: 'Brazil (1985)',\n",
       " 176: 'Aliens (1986)',\n",
       " 177: 'Good, The Bad and The Ugly, The (1966)',\n",
       " 178: '12 Angry Men (1957)',\n",
       " 179: 'Clockwork Orange, A (1971)',\n",
       " 180: 'Apocalypse Now (1979)',\n",
       " 181: 'Return of the Jedi (1983)',\n",
       " 182: 'GoodFellas (1990)',\n",
       " 183: 'Alien (1979)',\n",
       " 184: 'Army of Darkness (1993)',\n",
       " 185: 'Psycho (1960)',\n",
       " 186: 'Blues Brothers, The (1980)',\n",
       " 187: 'Godfather: Part II, The (1974)',\n",
       " 188: 'Full Metal Jacket (1987)',\n",
       " 189: 'Grand Day Out, A (1992)',\n",
       " 190: 'Henry V (1989)',\n",
       " 191: 'Amadeus (1984)',\n",
       " 192: 'Raging Bull (1980)',\n",
       " 193: 'Right Stuff, The (1983)',\n",
       " 194: 'Sting, The (1973)',\n",
       " 195: 'Terminator, The (1984)',\n",
       " 196: 'Dead Poets Society (1989)',\n",
       " 197: 'Graduate, The (1967)',\n",
       " 198: 'Nikita (La Femme Nikita) (1990)',\n",
       " 199: 'Bridge on the River Kwai, The (1957)',\n",
       " 200: 'Shining, The (1980)',\n",
       " 201: 'Evil Dead II (1987)',\n",
       " 202: 'Groundhog Day (1993)',\n",
       " 203: 'Unforgiven (1992)',\n",
       " 204: 'Back to the Future (1985)',\n",
       " 205: 'Patton (1970)',\n",
       " 206: 'Akira (1988)',\n",
       " 207: 'Cyrano de Bergerac (1990)',\n",
       " 208: 'Young Frankenstein (1974)',\n",
       " 209: 'This Is Spinal Tap (1984)',\n",
       " 210: 'Indiana Jones and the Last Crusade (1989)',\n",
       " 211: 'M*A*S*H (1970)',\n",
       " 212: 'Unbearable Lightness of Being, The (1988)',\n",
       " 213: 'Room with a View, A (1986)',\n",
       " 214: 'Pink Floyd - The Wall (1982)',\n",
       " 215: 'Field of Dreams (1989)',\n",
       " 216: 'When Harry Met Sally... (1989)',\n",
       " 217: \"Bram Stoker's Dracula (1992)\",\n",
       " 218: 'Cape Fear (1991)',\n",
       " 219: 'Nightmare on Elm Street, A (1984)',\n",
       " 220: 'Mirror Has Two Faces, The (1996)',\n",
       " 221: 'Breaking the Waves (1996)',\n",
       " 222: 'Star Trek: First Contact (1996)',\n",
       " 223: 'Sling Blade (1996)',\n",
       " 224: 'Ridicule (1996)',\n",
       " 225: '101 Dalmatians (1996)',\n",
       " 226: 'Die Hard 2 (1990)',\n",
       " 227: 'Star Trek VI: The Undiscovered Country (1991)',\n",
       " 228: 'Star Trek: The Wrath of Khan (1982)',\n",
       " 229: 'Star Trek III: The Search for Spock (1984)',\n",
       " 230: 'Star Trek IV: The Voyage Home (1986)',\n",
       " 231: 'Batman Returns (1992)',\n",
       " 232: 'Young Guns (1988)',\n",
       " 233: 'Under Siege (1992)',\n",
       " 234: 'Jaws (1975)',\n",
       " 235: 'Mars Attacks! (1996)',\n",
       " 236: 'Citizen Ruth (1996)',\n",
       " 237: 'Jerry Maguire (1996)',\n",
       " 238: 'Raising Arizona (1987)',\n",
       " 239: 'Sneakers (1992)',\n",
       " 240: 'Beavis and Butt-head Do America (1996)',\n",
       " 241: 'Last of the Mohicans, The (1992)',\n",
       " 242: 'Kolya (1996)',\n",
       " 243: 'Jungle2Jungle (1997)',\n",
       " 244: \"Smilla's Sense of Snow (1997)\",\n",
       " 245: \"Devil's Own, The (1997)\",\n",
       " 246: 'Chasing Amy (1997)',\n",
       " 247: 'Turbo: A Power Rangers Movie (1997)',\n",
       " 248: 'Grosse Pointe Blank (1997)',\n",
       " 249: 'Austin Powers: International Man of Mystery (1997)',\n",
       " 250: 'Fifth Element, The (1997)',\n",
       " 251: 'Shall We Dance? (1996)',\n",
       " 252: 'Lost World: Jurassic Park, The (1997)',\n",
       " 253: 'Pillow Book, The (1995)',\n",
       " 254: 'Batman & Robin (1997)',\n",
       " 255: \"My Best Friend's Wedding (1997)\",\n",
       " 256: 'When the Cats Away (Chacun cherche son chat) (1996)',\n",
       " 257: 'Men in Black (1997)',\n",
       " 258: 'Contact (1997)',\n",
       " 259: 'George of the Jungle (1997)',\n",
       " 260: 'Event Horizon (1997)',\n",
       " 261: 'Air Bud (1997)',\n",
       " 262: 'In the Company of Men (1997)',\n",
       " 263: 'Steel (1997)',\n",
       " 264: 'Mimic (1997)',\n",
       " 265: 'Hunt for Red October, The (1990)',\n",
       " 266: 'Kull the Conqueror (1997)',\n",
       " 267: 'unknown',\n",
       " 268: 'Chasing Amy (1997)',\n",
       " 269: 'Full Monty, The (1997)',\n",
       " 270: 'Gattaca (1997)',\n",
       " 271: 'Starship Troopers (1997)',\n",
       " 272: 'Good Will Hunting (1997)',\n",
       " 273: 'Heat (1995)',\n",
       " 274: 'Sabrina (1995)',\n",
       " 275: 'Sense and Sensibility (1995)',\n",
       " 276: 'Leaving Las Vegas (1995)',\n",
       " 277: 'Restoration (1995)',\n",
       " 278: 'Bed of Roses (1996)',\n",
       " 279: 'Once Upon a Time... When We Were Colored (1995)',\n",
       " 280: 'Up Close and Personal (1996)',\n",
       " 281: 'River Wild, The (1994)',\n",
       " 282: 'Time to Kill, A (1996)',\n",
       " 283: 'Emma (1996)',\n",
       " 284: 'Tin Cup (1996)',\n",
       " 285: 'Secrets & Lies (1996)',\n",
       " 286: 'English Patient, The (1996)',\n",
       " 287: \"Marvin's Room (1996)\",\n",
       " 288: 'Scream (1996)',\n",
       " 289: 'Evita (1996)',\n",
       " 290: 'Fierce Creatures (1997)',\n",
       " 291: 'Absolute Power (1997)',\n",
       " 292: 'Rosewood (1997)',\n",
       " 293: 'Donnie Brasco (1997)',\n",
       " 294: 'Liar Liar (1997)',\n",
       " 295: 'Breakdown (1997)',\n",
       " 296: 'Promesse, La (1996)',\n",
       " 297: \"Ulee's Gold (1997)\",\n",
       " 298: 'Face/Off (1997)',\n",
       " 299: 'Hoodlum (1997)',\n",
       " 300: 'Air Force One (1997)',\n",
       " 301: 'In & Out (1997)',\n",
       " 302: 'L.A. Confidential (1997)',\n",
       " 303: \"Ulee's Gold (1997)\",\n",
       " 304: 'Fly Away Home (1996)',\n",
       " 305: 'Ice Storm, The (1997)',\n",
       " 306: 'Mrs. Brown (Her Majesty, Mrs. Brown) (1997)',\n",
       " 307: \"Devil's Advocate, The (1997)\",\n",
       " 308: 'FairyTale: A True Story (1997)',\n",
       " 309: 'Deceiver (1997)',\n",
       " 310: 'Rainmaker, The (1997)',\n",
       " 311: 'Wings of the Dove, The (1997)',\n",
       " 312: 'Midnight in the Garden of Good and Evil (1997)',\n",
       " 313: 'Titanic (1997)',\n",
       " 314: '3 Ninjas: High Noon At Mega Mountain (1998)',\n",
       " 315: 'Apt Pupil (1998)',\n",
       " 316: 'As Good As It Gets (1997)',\n",
       " 317: 'In the Name of the Father (1993)',\n",
       " 318: \"Schindler's List (1993)\",\n",
       " 319: 'Everyone Says I Love You (1996)',\n",
       " 320: 'Paradise Lost: The Child Murders at Robin Hood Hills (1996)',\n",
       " 321: 'Mother (1996)',\n",
       " 322: 'Murder at 1600 (1997)',\n",
       " 323: \"Dante's Peak (1997)\",\n",
       " 324: 'Lost Highway (1997)',\n",
       " 325: 'Crash (1996)',\n",
       " 326: 'G.I. Jane (1997)',\n",
       " 327: 'Cop Land (1997)',\n",
       " 328: 'Conspiracy Theory (1997)',\n",
       " 329: 'Desperate Measures (1998)',\n",
       " 330: '187 (1997)',\n",
       " 331: 'Edge, The (1997)',\n",
       " 332: 'Kiss the Girls (1997)',\n",
       " 333: 'Game, The (1997)',\n",
       " 334: 'U Turn (1997)',\n",
       " 335: 'How to Be a Player (1997)',\n",
       " 336: 'Playing God (1997)',\n",
       " 337: 'House of Yes, The (1997)',\n",
       " 338: 'Bean (1997)',\n",
       " 339: 'Mad City (1997)',\n",
       " 340: 'Boogie Nights (1997)',\n",
       " 341: 'Critical Care (1997)',\n",
       " 342: 'Man Who Knew Too Little, The (1997)',\n",
       " 343: 'Alien: Resurrection (1997)',\n",
       " 344: 'Apostle, The (1997)',\n",
       " 345: 'Deconstructing Harry (1997)',\n",
       " 346: 'Jackie Brown (1997)',\n",
       " 347: 'Wag the Dog (1997)',\n",
       " 348: 'Desperate Measures (1998)',\n",
       " 349: 'Hard Rain (1998)',\n",
       " 350: 'Fallen (1998)',\n",
       " 351: 'Prophecy II, The (1998)',\n",
       " 352: 'Spice World (1997)',\n",
       " 353: 'Deep Rising (1998)',\n",
       " 354: 'Wedding Singer, The (1998)',\n",
       " 355: 'Sphere (1998)',\n",
       " 356: 'Client, The (1994)',\n",
       " 357: \"One Flew Over the Cuckoo's Nest (1975)\",\n",
       " 358: 'Spawn (1997)',\n",
       " 359: 'Assignment, The (1997)',\n",
       " 360: 'Wonderland (1997)',\n",
       " 361: 'Incognito (1997)',\n",
       " 362: 'Blues Brothers 2000 (1998)',\n",
       " 363: 'Sudden Death (1995)',\n",
       " 364: 'Ace Ventura: When Nature Calls (1995)',\n",
       " 365: 'Powder (1995)',\n",
       " 366: 'Dangerous Minds (1995)',\n",
       " 367: 'Clueless (1995)',\n",
       " 368: 'Bio-Dome (1996)',\n",
       " 369: 'Black Sheep (1996)',\n",
       " 370: 'Mary Reilly (1996)',\n",
       " 371: 'Bridges of Madison County, The (1995)',\n",
       " 372: 'Jeffrey (1995)',\n",
       " 373: 'Judge Dredd (1995)',\n",
       " 374: 'Mighty Morphin Power Rangers: The Movie (1995)',\n",
       " 375: 'Showgirls (1995)',\n",
       " 376: 'Houseguest (1994)',\n",
       " 377: 'Heavyweights (1994)',\n",
       " 378: 'Miracle on 34th Street (1994)',\n",
       " 379: 'Tales From the Crypt Presents: Demon Knight (1995)',\n",
       " 380: 'Star Trek: Generations (1994)',\n",
       " 381: \"Muriel's Wedding (1994)\",\n",
       " 382: 'Adventures of Priscilla, Queen of the Desert, The (1994)',\n",
       " 383: 'Flintstones, The (1994)',\n",
       " 384: 'Naked Gun 33 1/3: The Final Insult (1994)',\n",
       " 385: 'True Lies (1994)',\n",
       " 386: 'Addams Family Values (1993)',\n",
       " 387: 'Age of Innocence, The (1993)',\n",
       " 388: 'Beverly Hills Cop III (1994)',\n",
       " 389: 'Black Beauty (1994)',\n",
       " 390: 'Fear of a Black Hat (1993)',\n",
       " 391: 'Last Action Hero (1993)',\n",
       " 392: 'Man Without a Face, The (1993)',\n",
       " 393: 'Mrs. Doubtfire (1993)',\n",
       " 394: 'Radioland Murders (1994)',\n",
       " 395: 'Robin Hood: Men in Tights (1993)',\n",
       " 396: 'Serial Mom (1994)',\n",
       " 397: 'Striking Distance (1993)',\n",
       " 398: 'Super Mario Bros. (1993)',\n",
       " 399: 'Three Musketeers, The (1993)',\n",
       " 400: 'Little Rascals, The (1994)',\n",
       " 401: 'Brady Bunch Movie, The (1995)',\n",
       " 402: 'Ghost (1990)',\n",
       " 403: 'Batman (1989)',\n",
       " 404: 'Pinocchio (1940)',\n",
       " 405: 'Mission: Impossible (1996)',\n",
       " 406: 'Thinner (1996)',\n",
       " 407: 'Spy Hard (1996)',\n",
       " 408: 'Close Shave, A (1995)',\n",
       " 409: 'Jack (1996)',\n",
       " 410: 'Kingpin (1996)',\n",
       " 411: 'Nutty Professor, The (1996)',\n",
       " 412: 'Very Brady Sequel, A (1996)',\n",
       " 413: 'Tales from the Crypt Presents: Bordello of Blood (1996)',\n",
       " 414: 'My Favorite Year (1982)',\n",
       " 415: 'Apple Dumpling Gang, The (1975)',\n",
       " 416: 'Old Yeller (1957)',\n",
       " 417: 'Parent Trap, The (1961)',\n",
       " 418: 'Cinderella (1950)',\n",
       " 419: 'Mary Poppins (1964)',\n",
       " 420: 'Alice in Wonderland (1951)',\n",
       " 421: \"William Shakespeare's Romeo and Juliet (1996)\",\n",
       " 422: 'Aladdin and the King of Thieves (1996)',\n",
       " 423: 'E.T. the Extra-Terrestrial (1982)',\n",
       " 424: 'Children of the Corn: The Gathering (1996)',\n",
       " 425: 'Bob Roberts (1992)',\n",
       " 426: 'Transformers: The Movie, The (1986)',\n",
       " 427: 'To Kill a Mockingbird (1962)',\n",
       " 428: 'Harold and Maude (1971)',\n",
       " 429: 'Day the Earth Stood Still, The (1951)',\n",
       " 430: 'Duck Soup (1933)',\n",
       " 431: 'Highlander (1986)',\n",
       " 432: 'Fantasia (1940)',\n",
       " 433: 'Heathers (1989)',\n",
       " 434: 'Forbidden Planet (1956)',\n",
       " 435: 'Butch Cassidy and the Sundance Kid (1969)',\n",
       " 436: 'American Werewolf in London, An (1981)',\n",
       " 437: \"Amityville 1992: It's About Time (1992)\",\n",
       " 438: 'Amityville 3-D (1983)',\n",
       " 439: 'Amityville: A New Generation (1993)',\n",
       " 440: 'Amityville II: The Possession (1982)',\n",
       " 441: 'Amityville Horror, The (1979)',\n",
       " 442: 'Amityville Curse, The (1990)',\n",
       " 443: 'Birds, The (1963)',\n",
       " 444: 'Blob, The (1958)',\n",
       " 445: 'Body Snatcher, The (1945)',\n",
       " 446: 'Burnt Offerings (1976)',\n",
       " 447: 'Carrie (1976)',\n",
       " 448: 'Omen, The (1976)',\n",
       " 449: 'Star Trek: The Motion Picture (1979)',\n",
       " 450: 'Star Trek V: The Final Frontier (1989)',\n",
       " 451: 'Grease (1978)',\n",
       " 452: 'Jaws 2 (1978)',\n",
       " 453: 'Jaws 3-D (1983)',\n",
       " 454: 'Bastard Out of Carolina (1996)',\n",
       " 455: \"Jackie Chan's First Strike (1996)\",\n",
       " 456: 'Beverly Hills Ninja (1997)',\n",
       " 457: 'Free Willy 3: The Rescue (1997)',\n",
       " 458: 'Nixon (1995)',\n",
       " 459: 'Cry, the Beloved Country (1995)',\n",
       " 460: 'Crossing Guard, The (1995)',\n",
       " 461: 'Smoke (1995)',\n",
       " 462: 'Like Water For Chocolate (Como agua para chocolate) (1992)',\n",
       " 463: 'Secret of Roan Inish, The (1994)',\n",
       " 464: 'Vanya on 42nd Street (1994)',\n",
       " 465: 'Jungle Book, The (1994)',\n",
       " 466: 'Red Rock West (1992)',\n",
       " 467: 'Bronx Tale, A (1993)',\n",
       " 468: 'Rudy (1993)',\n",
       " 469: 'Short Cuts (1993)',\n",
       " 470: 'Tombstone (1993)',\n",
       " 471: 'Courage Under Fire (1996)',\n",
       " 472: 'Dragonheart (1996)',\n",
       " 473: 'James and the Giant Peach (1996)',\n",
       " 474: 'Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1963)',\n",
       " 475: 'Trainspotting (1996)',\n",
       " 476: 'First Wives Club, The (1996)',\n",
       " 477: 'Matilda (1996)',\n",
       " 478: 'Philadelphia Story, The (1940)',\n",
       " 479: 'Vertigo (1958)',\n",
       " 480: 'North by Northwest (1959)',\n",
       " 481: 'Apartment, The (1960)',\n",
       " 482: 'Some Like It Hot (1959)',\n",
       " 483: 'Casablanca (1942)',\n",
       " 484: 'Maltese Falcon, The (1941)',\n",
       " 485: 'My Fair Lady (1964)',\n",
       " 486: 'Sabrina (1954)',\n",
       " 487: 'Roman Holiday (1953)',\n",
       " 488: 'Sunset Blvd. (1950)',\n",
       " 489: 'Notorious (1946)',\n",
       " 490: 'To Catch a Thief (1955)',\n",
       " 491: 'Adventures of Robin Hood, The (1938)',\n",
       " 492: 'East of Eden (1955)',\n",
       " 493: 'Thin Man, The (1934)',\n",
       " 494: 'His Girl Friday (1940)',\n",
       " 495: 'Around the World in 80 Days (1956)',\n",
       " 496: \"It's a Wonderful Life (1946)\",\n",
       " 497: 'Bringing Up Baby (1938)',\n",
       " 498: 'African Queen, The (1951)',\n",
       " 499: 'Cat on a Hot Tin Roof (1958)',\n",
       " 500: 'Fly Away Home (1996)',\n",
       " 501: 'Dumbo (1941)',\n",
       " 502: 'Bananas (1971)',\n",
       " 503: 'Candidate, The (1972)',\n",
       " 504: 'Bonnie and Clyde (1967)',\n",
       " 505: 'Dial M for Murder (1954)',\n",
       " 506: 'Rebel Without a Cause (1955)',\n",
       " 507: 'Streetcar Named Desire, A (1951)',\n",
       " 508: 'People vs. Larry Flynt, The (1996)',\n",
       " 509: 'My Left Foot (1989)',\n",
       " 510: 'Magnificent Seven, The (1954)',\n",
       " 511: 'Lawrence of Arabia (1962)',\n",
       " 512: 'Wings of Desire (1987)',\n",
       " 513: 'Third Man, The (1949)',\n",
       " 514: 'Annie Hall (1977)',\n",
       " 515: 'Boot, Das (1981)',\n",
       " 516: 'Local Hero (1983)',\n",
       " 517: 'Manhattan (1979)',\n",
       " 518: \"Miller's Crossing (1990)\",\n",
       " 519: 'Treasure of the Sierra Madre, The (1948)',\n",
       " 520: 'Great Escape, The (1963)',\n",
       " 521: 'Deer Hunter, The (1978)',\n",
       " 522: 'Down by Law (1986)',\n",
       " 523: 'Cool Hand Luke (1967)',\n",
       " 524: 'Great Dictator, The (1940)',\n",
       " 525: 'Big Sleep, The (1946)',\n",
       " 526: 'Ben-Hur (1959)',\n",
       " 527: 'Gandhi (1982)',\n",
       " 528: 'Killing Fields, The (1984)',\n",
       " 529: 'My Life as a Dog (Mitt liv som hund) (1985)',\n",
       " 530: 'Man Who Would Be King, The (1975)',\n",
       " 531: 'Shine (1996)',\n",
       " 532: 'Kama Sutra: A Tale of Love (1996)',\n",
       " 533: 'Daytrippers, The (1996)',\n",
       " 534: 'Traveller (1997)',\n",
       " 535: 'Addicted to Love (1997)',\n",
       " 536: 'Ponette (1996)',\n",
       " 537: 'My Own Private Idaho (1991)',\n",
       " 538: 'Anastasia (1997)',\n",
       " 539: 'Mouse Hunt (1997)',\n",
       " 540: 'Money Train (1995)',\n",
       " 541: 'Mortal Kombat (1995)',\n",
       " 542: 'Pocahontas (1995)',\n",
       " 543: 'Misrables, Les (1995)',\n",
       " 544: \"Things to Do in Denver when You're Dead (1995)\",\n",
       " 545: 'Vampire in Brooklyn (1995)',\n",
       " 546: 'Broken Arrow (1996)',\n",
       " 547: \"Young Poisoner's Handbook, The (1995)\",\n",
       " 548: 'NeverEnding Story III, The (1994)',\n",
       " 549: 'Rob Roy (1995)',\n",
       " 550: 'Die Hard: With a Vengeance (1995)',\n",
       " 551: 'Lord of Illusions (1995)',\n",
       " 552: 'Species (1995)',\n",
       " 553: 'Walk in the Clouds, A (1995)',\n",
       " 554: 'Waterworld (1995)',\n",
       " 555: \"White Man's Burden (1995)\",\n",
       " 556: 'Wild Bill (1995)',\n",
       " 557: 'Farinelli: il castrato (1994)',\n",
       " 558: 'Heavenly Creatures (1994)',\n",
       " 559: 'Interview with the Vampire (1994)',\n",
       " 560: \"Kid in King Arthur's Court, A (1995)\",\n",
       " 561: \"Mary Shelley's Frankenstein (1994)\",\n",
       " 562: 'Quick and the Dead, The (1995)',\n",
       " 563: \"Stephen King's The Langoliers (1995)\",\n",
       " 564: 'Tales from the Hood (1995)',\n",
       " 565: 'Village of the Damned (1995)',\n",
       " 566: 'Clear and Present Danger (1994)',\n",
       " 567: \"Wes Craven's New Nightmare (1994)\",\n",
       " 568: 'Speed (1994)',\n",
       " 569: 'Wolf (1994)',\n",
       " 570: 'Wyatt Earp (1994)',\n",
       " 571: 'Another Stakeout (1993)',\n",
       " 572: 'Blown Away (1994)',\n",
       " 573: 'Body Snatchers (1993)',\n",
       " 574: 'Boxing Helena (1993)',\n",
       " 575: \"City Slickers II: The Legend of Curly's Gold (1994)\",\n",
       " 576: 'Cliffhanger (1993)',\n",
       " 577: 'Coneheads (1993)',\n",
       " 578: 'Demolition Man (1993)',\n",
       " 579: 'Fatal Instinct (1993)',\n",
       " 580: 'Englishman Who Went Up a Hill, But Came Down a Mountain, The (1995)',\n",
       " 581: 'Kalifornia (1993)',\n",
       " 582: 'Piano, The (1993)',\n",
       " 583: 'Romeo Is Bleeding (1993)',\n",
       " 584: 'Secret Garden, The (1993)',\n",
       " 585: 'Son in Law (1993)',\n",
       " 586: 'Terminal Velocity (1994)',\n",
       " 587: 'Hour of the Pig, The (1993)',\n",
       " 588: 'Beauty and the Beast (1991)',\n",
       " 589: 'Wild Bunch, The (1969)',\n",
       " 590: 'Hellraiser: Bloodline (1996)',\n",
       " 591: 'Primal Fear (1996)',\n",
       " 592: 'True Crime (1995)',\n",
       " 593: 'Stalingrad (1993)',\n",
       " 594: 'Heavy (1995)',\n",
       " 595: 'Fan, The (1996)',\n",
       " 596: 'Hunchback of Notre Dame, The (1996)',\n",
       " 597: 'Eraser (1996)',\n",
       " 598: 'Big Squeeze, The (1996)',\n",
       " 599: 'Police Story 4: Project S (Chao ji ji hua) (1993)',\n",
       " 600: \"Daniel Defoe's Robinson Crusoe (1996)\",\n",
       " 601: 'For Whom the Bell Tolls (1943)',\n",
       " 602: 'American in Paris, An (1951)',\n",
       " 603: 'Rear Window (1954)',\n",
       " 604: 'It Happened One Night (1934)',\n",
       " 605: 'Meet Me in St. Louis (1944)',\n",
       " 606: 'All About Eve (1950)',\n",
       " 607: 'Rebecca (1940)',\n",
       " 608: 'Spellbound (1945)',\n",
       " 609: 'Father of the Bride (1950)',\n",
       " 610: 'Gigi (1958)',\n",
       " 611: 'Laura (1944)',\n",
       " 612: 'Lost Horizon (1937)',\n",
       " 613: 'My Man Godfrey (1936)',\n",
       " 614: 'Giant (1956)',\n",
       " 615: '39 Steps, The (1935)',\n",
       " 616: 'Night of the Living Dead (1968)',\n",
       " 617: 'Blue Angel, The (Blaue Engel, Der) (1930)',\n",
       " 618: 'Picnic (1955)',\n",
       " 619: 'Extreme Measures (1996)',\n",
       " 620: 'Chamber, The (1996)',\n",
       " 621: 'Davy Crockett, King of the Wild Frontier (1955)',\n",
       " 622: 'Swiss Family Robinson (1960)',\n",
       " 623: 'Angels in the Outfield (1994)',\n",
       " 624: 'Three Caballeros, The (1945)',\n",
       " 625: 'Sword in the Stone, The (1963)',\n",
       " 626: 'So Dear to My Heart (1949)',\n",
       " 627: 'Robin Hood: Prince of Thieves (1991)',\n",
       " 628: 'Sleepers (1996)',\n",
       " 629: 'Victor/Victoria (1982)',\n",
       " 630: 'Great Race, The (1965)',\n",
       " 631: 'Crying Game, The (1992)',\n",
       " 632: \"Sophie's Choice (1982)\",\n",
       " 633: 'Christmas Carol, A (1938)',\n",
       " 634: \"Microcosmos: Le peuple de l'herbe (1996)\",\n",
       " 635: 'Fog, The (1980)',\n",
       " 636: 'Escape from New York (1981)',\n",
       " 637: 'Howling, The (1981)',\n",
       " 638: 'Return of Martin Guerre, The (Retour de Martin Guerre, Le) (1982)',\n",
       " 639: 'Tin Drum, The (Blechtrommel, Die) (1979)',\n",
       " 640: 'Cook the Thief His Wife & Her Lover, The (1989)',\n",
       " 641: 'Paths of Glory (1957)',\n",
       " 642: 'Grifters, The (1990)',\n",
       " 643: 'The Innocent (1994)',\n",
       " 644: 'Thin Blue Line, The (1988)',\n",
       " 645: 'Paris Is Burning (1990)',\n",
       " 646: 'Once Upon a Time in the West (1969)',\n",
       " 647: 'Ran (1985)',\n",
       " 648: 'Quiet Man, The (1952)',\n",
       " 649: 'Once Upon a Time in America (1984)',\n",
       " 650: 'Seventh Seal, The (Sjunde inseglet, Det) (1957)',\n",
       " 651: 'Glory (1989)',\n",
       " 652: 'Rosencrantz and Guildenstern Are Dead (1990)',\n",
       " 653: 'Touch of Evil (1958)',\n",
       " 654: 'Chinatown (1974)',\n",
       " 655: 'Stand by Me (1986)',\n",
       " 656: 'M (1931)',\n",
       " 657: 'Manchurian Candidate, The (1962)',\n",
       " 658: 'Pump Up the Volume (1990)',\n",
       " 659: 'Arsenic and Old Lace (1944)',\n",
       " 660: 'Fried Green Tomatoes (1991)',\n",
       " 661: 'High Noon (1952)',\n",
       " 662: 'Somewhere in Time (1980)',\n",
       " 663: 'Being There (1979)',\n",
       " 664: 'Paris, Texas (1984)',\n",
       " 665: 'Alien 3 (1992)',\n",
       " 666: \"Blood For Dracula (Andy Warhol's Dracula) (1974)\",\n",
       " 667: 'Audrey Rose (1977)',\n",
       " 668: 'Blood Beach (1981)',\n",
       " 669: 'Body Parts (1991)',\n",
       " 670: 'Body Snatchers (1993)',\n",
       " 671: 'Bride of Frankenstein (1935)',\n",
       " 672: 'Candyman (1992)',\n",
       " 673: 'Cape Fear (1962)',\n",
       " 674: 'Cat People (1982)',\n",
       " 675: 'Nosferatu (Nosferatu, eine Symphonie des Grauens) (1922)',\n",
       " 676: 'Crucible, The (1996)',\n",
       " 677: 'Fire on the Mountain (1996)',\n",
       " 678: 'Volcano (1997)',\n",
       " 679: 'Conan the Barbarian (1981)',\n",
       " 680: 'Kull the Conqueror (1997)',\n",
       " 681: 'Wishmaster (1997)',\n",
       " 682: 'I Know What You Did Last Summer (1997)',\n",
       " 683: 'Rocket Man (1997)',\n",
       " 684: 'In the Line of Fire (1993)',\n",
       " 685: 'Executive Decision (1996)',\n",
       " 686: 'Perfect World, A (1993)',\n",
       " 687: \"McHale's Navy (1997)\",\n",
       " 688: 'Leave It to Beaver (1997)',\n",
       " 689: 'Jackal, The (1997)',\n",
       " 690: 'Seven Years in Tibet (1997)',\n",
       " 691: 'Dark City (1998)',\n",
       " 692: 'American President, The (1995)',\n",
       " 693: 'Casino (1995)',\n",
       " 694: 'Persuasion (1995)',\n",
       " 695: 'Kicking and Screaming (1995)',\n",
       " 696: 'City Hall (1996)',\n",
       " 697: 'Basketball Diaries, The (1995)',\n",
       " 698: 'Browning Version, The (1994)',\n",
       " 699: 'Little Women (1994)',\n",
       " 700: 'Miami Rhapsody (1995)',\n",
       " 701: 'Wonderful, Horrible Life of Leni Riefenstahl, The (1993)',\n",
       " 702: 'Barcelona (1994)',\n",
       " 703: \"Widows' Peak (1994)\",\n",
       " 704: 'House of the Spirits, The (1993)',\n",
       " 705: \"Singin' in the Rain (1952)\",\n",
       " 706: 'Bad Moon (1996)',\n",
       " 707: 'Enchanted April (1991)',\n",
       " 708: 'Sex, Lies, and Videotape (1989)',\n",
       " 709: 'Strictly Ballroom (1992)',\n",
       " 710: 'Better Off Dead... (1985)',\n",
       " 711: 'Substance of Fire, The (1996)',\n",
       " 712: 'Tin Men (1987)',\n",
       " 713: 'Othello (1995)',\n",
       " 714: 'Carrington (1995)',\n",
       " 715: 'To Die For (1995)',\n",
       " 716: 'Home for the Holidays (1995)',\n",
       " 717: 'Juror, The (1996)',\n",
       " 718: 'In the Bleak Midwinter (1995)',\n",
       " 719: 'Canadian Bacon (1994)',\n",
       " 720: 'First Knight (1995)',\n",
       " 721: 'Mallrats (1995)',\n",
       " 722: 'Nine Months (1995)',\n",
       " 723: 'Boys on the Side (1995)',\n",
       " 724: 'Circle of Friends (1995)',\n",
       " 725: 'Exit to Eden (1994)',\n",
       " 726: 'Fluke (1995)',\n",
       " 727: 'Immortal Beloved (1994)',\n",
       " 728: 'Junior (1994)',\n",
       " 729: 'Nell (1994)',\n",
       " 730: 'Queen Margot (Reine Margot, La) (1994)',\n",
       " 731: 'Corrina, Corrina (1994)',\n",
       " 732: 'Dave (1993)',\n",
       " 733: 'Go Fish (1994)',\n",
       " 734: 'Made in America (1993)',\n",
       " 735: 'Philadelphia (1993)',\n",
       " 736: 'Shadowlands (1993)',\n",
       " 737: 'Sirens (1994)',\n",
       " 738: 'Threesome (1994)',\n",
       " 739: 'Pretty Woman (1990)',\n",
       " 740: 'Jane Eyre (1996)',\n",
       " 741: 'Last Supper, The (1995)',\n",
       " 742: 'Ransom (1996)',\n",
       " 743: 'Crow: City of Angels, The (1996)',\n",
       " 744: 'Michael Collins (1996)',\n",
       " 745: 'Ruling Class, The (1972)',\n",
       " 746: 'Real Genius (1985)',\n",
       " 747: 'Benny & Joon (1993)',\n",
       " 748: 'Saint, The (1997)',\n",
       " 749: 'MatchMaker, The (1997)',\n",
       " 750: 'Amistad (1997)',\n",
       " 751: 'Tomorrow Never Dies (1997)',\n",
       " 752: 'Replacement Killers, The (1998)',\n",
       " 753: 'Burnt By the Sun (1994)',\n",
       " 754: 'Red Corner (1997)',\n",
       " 755: 'Jumanji (1995)',\n",
       " 756: 'Father of the Bride Part II (1995)',\n",
       " 757: 'Across the Sea of Time (1995)',\n",
       " 758: 'Lawnmower Man 2: Beyond Cyberspace (1996)',\n",
       " 759: 'Fair Game (1995)',\n",
       " 760: 'Screamers (1995)',\n",
       " 761: 'Nick of Time (1995)',\n",
       " 762: 'Beautiful Girls (1996)',\n",
       " 763: 'Happy Gilmore (1996)',\n",
       " 764: 'If Lucy Fell (1996)',\n",
       " 765: 'Boomerang (1992)',\n",
       " 766: 'Man of the Year (1995)',\n",
       " 767: 'Addiction, The (1995)',\n",
       " 768: 'Casper (1995)',\n",
       " 769: 'Congo (1995)',\n",
       " 770: 'Devil in a Blue Dress (1995)',\n",
       " 771: 'Johnny Mnemonic (1995)',\n",
       " 772: 'Kids (1995)',\n",
       " 773: 'Mute Witness (1994)',\n",
       " 774: 'Prophecy, The (1995)',\n",
       " 775: 'Something to Talk About (1995)',\n",
       " 776: 'Three Wishes (1995)',\n",
       " 777: 'Castle Freak (1995)',\n",
       " 778: 'Don Juan DeMarco (1995)',\n",
       " 779: 'Drop Zone (1994)',\n",
       " 780: 'Dumb & Dumber (1994)',\n",
       " 781: 'French Kiss (1995)',\n",
       " 782: 'Little Odessa (1994)',\n",
       " 783: 'Milk Money (1994)',\n",
       " 784: 'Beyond Bedlam (1993)',\n",
       " 785: 'Only You (1994)',\n",
       " 786: 'Perez Family, The (1995)',\n",
       " 787: 'Roommates (1995)',\n",
       " 788: 'Relative Fear (1994)',\n",
       " 789: 'Swimming with Sharks (1995)',\n",
       " 790: 'Tommy Boy (1995)',\n",
       " 791: 'Baby-Sitters Club, The (1995)',\n",
       " 792: 'Bullets Over Broadway (1994)',\n",
       " 793: 'Crooklyn (1994)',\n",
       " 794: 'It Could Happen to You (1994)',\n",
       " 795: 'Richie Rich (1994)',\n",
       " 796: 'Speechless (1994)',\n",
       " 797: 'Timecop (1994)',\n",
       " 798: 'Bad Company (1995)',\n",
       " 799: 'Boys Life (1995)',\n",
       " 800: 'In the Mouth of Madness (1995)',\n",
       " 801: 'Air Up There, The (1994)',\n",
       " 802: 'Hard Target (1993)',\n",
       " 803: 'Heaven & Earth (1993)',\n",
       " 804: 'Jimmy Hollywood (1994)',\n",
       " 805: 'Manhattan Murder Mystery (1993)',\n",
       " 806: 'Menace II Society (1993)',\n",
       " 807: 'Poetic Justice (1993)',\n",
       " 808: 'Program, The (1993)',\n",
       " 809: 'Rising Sun (1993)',\n",
       " 810: 'Shadow, The (1994)',\n",
       " 811: 'Thirty-Two Short Films About Glenn Gould (1993)',\n",
       " 812: 'Andre (1994)',\n",
       " 813: 'Celluloid Closet, The (1995)',\n",
       " 814: 'Great Day in Harlem, A (1994)',\n",
       " 815: 'One Fine Day (1996)',\n",
       " 816: 'Candyman: Farewell to the Flesh (1995)',\n",
       " 817: 'Frisk (1995)',\n",
       " 818: 'Girl 6 (1996)',\n",
       " 819: 'Eddie (1996)',\n",
       " 820: 'Space Jam (1996)',\n",
       " 821: 'Mrs. Winterbourne (1996)',\n",
       " 822: 'Faces (1968)',\n",
       " 823: 'Mulholland Falls (1996)',\n",
       " 824: 'Great White Hype, The (1996)',\n",
       " 825: 'Arrival, The (1996)',\n",
       " 826: 'Phantom, The (1996)',\n",
       " 827: 'Daylight (1996)',\n",
       " 828: 'Alaska (1996)',\n",
       " 829: 'Fled (1996)',\n",
       " 830: 'Power 98 (1995)',\n",
       " 831: 'Escape from L.A. (1996)',\n",
       " 832: 'Bogus (1996)',\n",
       " 833: 'Bulletproof (1996)',\n",
       " 834: 'Halloween: The Curse of Michael Myers (1995)',\n",
       " 835: 'Gay Divorcee, The (1934)',\n",
       " 836: 'Ninotchka (1939)',\n",
       " 837: 'Meet John Doe (1941)',\n",
       " 838: 'In the Line of Duty 2 (1987)',\n",
       " 839: 'Loch Ness (1995)',\n",
       " 840: 'Last Man Standing (1996)',\n",
       " 841: 'Glimmer Man, The (1996)',\n",
       " 842: 'Pollyanna (1960)',\n",
       " 843: 'Shaggy Dog, The (1959)',\n",
       " 844: 'Freeway (1996)',\n",
       " 845: 'That Thing You Do! (1996)',\n",
       " 846: 'To Gillian on Her 37th Birthday (1996)',\n",
       " 847: 'Looking for Richard (1996)',\n",
       " 848: 'Murder, My Sweet (1944)',\n",
       " 849: 'Days of Thunder (1990)',\n",
       " 850: 'Perfect Candidate, A (1996)',\n",
       " 851: 'Two or Three Things I Know About Her (1966)',\n",
       " 852: 'Bloody Child, The (1996)',\n",
       " 853: 'Braindead (1992)',\n",
       " 854: 'Bad Taste (1987)',\n",
       " 855: 'Diva (1981)',\n",
       " 856: 'Night on Earth (1991)',\n",
       " 857: 'Paris Was a Woman (1995)',\n",
       " 858: 'Amityville: Dollhouse (1996)',\n",
       " 859: \"April Fool's Day (1986)\",\n",
       " 860: 'Believers, The (1987)',\n",
       " 861: 'Nosferatu a Venezia (1986)',\n",
       " 862: 'Jingle All the Way (1996)',\n",
       " 863: 'Garden of Finzi-Contini, The (Giardino dei Finzi-Contini, Il) (1970)',\n",
       " 864: 'My Fellow Americans (1996)',\n",
       " 865: 'Ice Storm, The (1997)',\n",
       " 866: 'Michael (1996)',\n",
       " 867: 'Whole Wide World, The (1996)',\n",
       " 868: 'Hearts and Minds (1996)',\n",
       " 869: 'Fools Rush In (1997)',\n",
       " 870: 'Touch (1997)',\n",
       " 871: 'Vegas Vacation (1997)',\n",
       " 872: 'Love Jones (1997)',\n",
       " 873: 'Picture Perfect (1997)',\n",
       " 874: 'Career Girls (1997)',\n",
       " 875: \"She's So Lovely (1997)\",\n",
       " 876: 'Money Talks (1997)',\n",
       " 877: 'Excess Baggage (1997)',\n",
       " 878: 'That Darn Cat! (1997)',\n",
       " 879: 'Peacemaker, The (1997)',\n",
       " 880: 'Soul Food (1997)',\n",
       " 881: 'Money Talks (1997)',\n",
       " 882: 'Washington Square (1997)',\n",
       " 883: 'Telling Lies in America (1997)',\n",
       " 884: 'Year of the Horse (1997)',\n",
       " 885: 'Phantoms (1998)',\n",
       " 886: 'Life Less Ordinary, A (1997)',\n",
       " 887: \"Eve's Bayou (1997)\",\n",
       " 888: 'One Night Stand (1997)',\n",
       " 889: 'Tango Lesson, The (1997)',\n",
       " 890: 'Mortal Kombat: Annihilation (1997)',\n",
       " 891: 'Bent (1997)',\n",
       " 892: 'Flubber (1997)',\n",
       " 893: 'For Richer or Poorer (1997)',\n",
       " 894: 'Home Alone 3 (1997)',\n",
       " 895: 'Scream 2 (1997)',\n",
       " 896: 'Sweet Hereafter, The (1997)',\n",
       " 897: 'Time Tracers (1995)',\n",
       " 898: 'Postman, The (1997)',\n",
       " 899: 'Winter Guest, The (1997)',\n",
       " 900: 'Kundun (1997)',\n",
       " 901: 'Mr. Magoo (1997)',\n",
       " 902: 'Big Lebowski, The (1998)',\n",
       " 903: 'Afterglow (1997)',\n",
       " 904: 'Ma vie en rose (My Life in Pink) (1997)',\n",
       " 905: 'Great Expectations (1998)',\n",
       " 906: 'Oscar & Lucinda (1997)',\n",
       " 907: 'Vermin (1998)',\n",
       " 908: 'Half Baked (1998)',\n",
       " 909: 'Dangerous Beauty (1998)',\n",
       " 910: 'Nil By Mouth (1997)',\n",
       " 911: 'Twilight (1998)',\n",
       " 912: 'U.S. Marshalls (1998)',\n",
       " 913: 'Love and Death on Long Island (1997)',\n",
       " 914: 'Wild Things (1998)',\n",
       " 915: 'Primary Colors (1998)',\n",
       " 916: 'Lost in Space (1998)',\n",
       " 917: 'Mercury Rising (1998)',\n",
       " 918: 'City of Angels (1998)',\n",
       " 919: 'City of Lost Children, The (1995)',\n",
       " 920: 'Two Bits (1995)',\n",
       " 921: 'Farewell My Concubine (1993)',\n",
       " 922: 'Dead Man (1995)',\n",
       " 923: 'Raise the Red Lantern (1991)',\n",
       " 924: 'White Squall (1996)',\n",
       " 925: 'Unforgettable (1996)',\n",
       " 926: 'Down Periscope (1996)',\n",
       " 927: 'Flower of My Secret, The (Flor de mi secreto, La) (1995)',\n",
       " 928: 'Craft, The (1996)',\n",
       " 929: 'Harriet the Spy (1996)',\n",
       " 930: 'Chain Reaction (1996)',\n",
       " 931: 'Island of Dr. Moreau, The (1996)',\n",
       " 932: 'First Kid (1996)',\n",
       " 933: 'Funeral, The (1996)',\n",
       " 934: \"Preacher's Wife, The (1996)\",\n",
       " 935: 'Paradise Road (1997)',\n",
       " 936: 'Brassed Off (1996)',\n",
       " 937: 'Thousand Acres, A (1997)',\n",
       " 938: 'Smile Like Yours, A (1997)',\n",
       " 939: 'Murder in the First (1995)',\n",
       " 940: 'Airheads (1994)',\n",
       " 941: 'With Honors (1994)',\n",
       " 942: \"What's Love Got to Do with It (1993)\",\n",
       " 943: 'Killing Zoe (1994)',\n",
       " 944: 'Renaissance Man (1994)',\n",
       " 945: 'Charade (1963)',\n",
       " 946: 'Fox and the Hound, The (1981)',\n",
       " 947: 'Big Blue, The (Grand bleu, Le) (1988)',\n",
       " 948: 'Booty Call (1997)',\n",
       " 949: 'How to Make an American Quilt (1995)',\n",
       " 950: 'Georgia (1995)',\n",
       " 951: 'Indian in the Cupboard, The (1995)',\n",
       " 952: 'Blue in the Face (1995)',\n",
       " 953: 'Unstrung Heroes (1995)',\n",
       " 954: 'Unzipped (1995)',\n",
       " 955: 'Before Sunrise (1995)',\n",
       " 956: \"Nobody's Fool (1994)\",\n",
       " 957: 'Pushing Hands (1992)',\n",
       " 958: 'To Live (Huozhe) (1994)',\n",
       " 959: 'Dazed and Confused (1993)',\n",
       " 960: 'Naked (1993)',\n",
       " 961: 'Orlando (1993)',\n",
       " 962: 'Ruby in Paradise (1993)',\n",
       " 963: 'Some Folks Call It a Sling Blade (1993)',\n",
       " 964: 'Month by the Lake, A (1995)',\n",
       " 965: 'Funny Face (1957)',\n",
       " 966: 'Affair to Remember, An (1957)',\n",
       " 967: 'Little Lord Fauntleroy (1936)',\n",
       " 968: 'Inspector General, The (1949)',\n",
       " 969: 'Winnie the Pooh and the Blustery Day (1968)',\n",
       " 970: 'Hear My Song (1991)',\n",
       " 971: 'Mediterraneo (1991)',\n",
       " 972: 'Passion Fish (1992)',\n",
       " 973: 'Grateful Dead (1995)',\n",
       " 974: 'Eye for an Eye (1996)',\n",
       " 975: 'Fear (1996)',\n",
       " 976: 'Solo (1996)',\n",
       " 977: 'Substitute, The (1996)',\n",
       " 978: \"Heaven's Prisoners (1996)\",\n",
       " 979: 'Trigger Effect, The (1996)',\n",
       " 980: 'Mother Night (1996)',\n",
       " 981: 'Dangerous Ground (1997)',\n",
       " 982: 'Maximum Risk (1996)',\n",
       " 983: \"Rich Man's Wife, The (1996)\",\n",
       " 984: 'Shadow Conspiracy (1997)',\n",
       " 985: 'Blood & Wine (1997)',\n",
       " 986: 'Turbulence (1997)',\n",
       " 987: 'Underworld (1997)',\n",
       " 988: 'Beautician and the Beast, The (1997)',\n",
       " 989: \"Cats Don't Dance (1997)\",\n",
       " 990: 'Anna Karenina (1997)',\n",
       " 991: 'Keys to Tulsa (1997)',\n",
       " 992: 'Head Above Water (1996)',\n",
       " 993: 'Hercules (1997)',\n",
       " 994: 'Last Time I Committed Suicide, The (1997)',\n",
       " 995: 'Kiss Me, Guido (1997)',\n",
       " 996: 'Big Green, The (1995)',\n",
       " 997: 'Stuart Saves His Family (1995)',\n",
       " 998: 'Cabin Boy (1994)',\n",
       " 999: 'Clean Slate (1994)',\n",
       " 1000: 'Lightning Jack (1994)',\n",
       " ...}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movieID_name_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1682"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(movieID_name_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The closest movie to Casino (1995) in the embedding space is Quiz Show (1994)\n"
     ]
    }
   ],
   "source": [
    "movie_id_to_examine = 693 # Customize the movie ID you want to examine\n",
    "\n",
    "candidate_movie_ids.remove(movie_id_to_examine)\n",
    "best_id, min_dist = get_nn_of_movie(movie_id_to_examine, candidate_movie_ids, embedding_dict)\n",
    "movieID_name_map = load_movie_id_name_map('ml-100k/u.item')\n",
    "print('The closest movie to {} in the embedding space is {}'.format(movieID_name_map[movie_id_to_examine],\n",
    "                                                                  movieID_name_map[best_id]))\n",
    "candidate_movie_ids.append(movie_id_to_examine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'object2vec-2020-05-20-03-50-22-708'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor_2.endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instances': ({'in0': [680], 'in1': [276]}, {'in0': [295], 'in1': [190]})}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"predictions\":[{\"scores\":[0.239473223686218,0.760526776313782]},{\"scores\":[0.007587122730911,0.992412865161896]}]}'\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import boto3 \n",
    "client = boto3.client('runtime.sagemaker')\n",
    "\n",
    "data = {'instances': ({'in0': [52], 'in1': [95]})} \n",
    "response = client.invoke_endpoint(EndpointName='object2vec-2020-05-20-03-50-22-708',\n",
    "                                  Body=json.dumps(dat1))\n",
    "response_body = response['Body'] \n",
    "print(response_body.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "custom_attributes = \"c000b4f9-df62-4c85-a0bf-7c525f9104a4\"  # An example of a trace ID.\n",
    "endpoint_name = \"object2vec-2020-05-20-03-50-22-708\"                                       # Your endpoint name.\n",
    "content_type = \"...\"                                        # The MIME type of the input data in the request body.\n",
    "accept = \"...\"                                              # The desired MIME type of the inference in the response.\n",
    "payload = \"...\"                                             # Payload for inference."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
